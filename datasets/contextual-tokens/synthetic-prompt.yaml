---
- extra_background: |-
    Hello GPT,

    I'm testing interpretability of MoE LLM models, specifically with a focus on expert specialization.

    However, I've noticed that expert specialization tends to emerge extremely early in training, often within the first few hundred million tokens. This becomes self-reinforcing: once a token is routed to a particular expert and that expert improves on it, the gating mechanism keeps assigning that token there. Unfortunately, this leads to shallow token-level specialization, where experts latch onto low-level features (like subword fragments or punctuation) instead of deeper, semantic features. As a result, we end up with experts that are effective at handling certain tokens on the surface but do not necessarily capture more sophisticated or context-based specialization, making it harder to interpret or manipulate the model based on meaningful domains or semantic tasks.

    To address this, I'm exploring a contextual consistency metric, where I check if tokens with multiple senses (e.g., "match" for sports vs. "match" for fire) end up switching experts appropriately based on context, or if the gating mechanism ignores context altogether. By monitoring this metric during training, I hope to catch early, shallow specialization and test interventions that encourage more meaningful, context-based routing.

    Please help me create tokens which I can test for this contextual consistency metric. Follow the below instructions closely.
- user_prompt: |-
    Create a YAML file that contains a token with 3 alternative meanings. For each meaning, you'll create a set of TEXT SAMPLES, representing scraped data that utilizes the token within the correct meaning.

    Here is the token and meanings for you to use:

    " match": ignition stick, sports event, a correspondence concept

    Next, reason through the following steps, step by step:

      1. First, convert these 3 meanings into 3 meaning labels. These should be similar to the provided meanings, but more specific and precise, and with no special characters or spaces, just underscores.

      2. For each meaning label, think very carefully about different formatting/content/sources of TEXT SAMPLES and their distribution. Remember, the distribution should be roughly similar to their actual distribution in scraped data. Don't just include obvious sources, but also unusual formatting that you would expect in scraped data.

      For example, suppose the token is "=" and the meaning label is "programming_assignment". Then, your distribution might be something like this: pure scraped code in different languages (40%), instructional coding textbooks or blogs (10%), config files (5%), Stackoverflow responses that still contain raw HTML in them (5%), Markdown-formatted Wikipedia articles with coding examples (2%), git diffs (2%), user-assistant LLM chats about a coding problem (2%), humorous social media comments about coding (2%), error logs showing variable states (1%), email mailing lists with code copy pasted (1%), etc.

      3. Next, for each meaning, generate exactly 25 TEXT SAMPLES from the formatting/content/source distribution you specified. Follow the below guidelines:
      - NEVER describe the source or context for each TEXT SAMPLE, just generate the TEXT SAMPLE directly. NEVER start each TEXT SAMPLE with an introduction or description of the source!
      - Each TEXT SAMPLE should be 100 - 250 tokens long. Vary the length greatly across TEXT SAMPLES.
      - Each TEXT SAMPLE should may use the token once or multiple times, but each time it must be used with the correct meaning. Remember to use the exact spacing/capitalization of the token provided to you!
      - TEXT SAMPLES should be diverse and messy, representing a wide variety of different sources, formats, formatting, and contexts. For example, they can be conversational transcripts, pieces of code, scraped text blobs with residual HTML formatting, Wikipedia articles, textbook pages, text conversations, news articles, strangely formatted copy-pasted text, conversational dialogue, chats mixed with emojis, academic journals, raw code comments, grocery lists, and so on.

      4. Finally, return your reasoning for steps 1-2, followed a YAML block structured like this:
      ```
      - token: "<str>" 
        meanings:
          - meaning_label: "<str>"
            text_samples: 
              - "<str>"
              - "<str>"
      ```

    Here is an example of a high-quality response that follows the guidelines, when the token is "!" and the meaning_label is "punctuation_emphasis". Notice the diversity in source/formatting/styles. Notice also that we never describe or give additional context to the TEXT SAMPLES, we just generate them directly.
    ```
    - token: "!" 
      meanings:
        - meaning_label: "punctuation_emphasis"
          text_samples: 
            - "to the remote Git server. Make work as state-independent as possible, as everything not saved in Git will be destroyed every time the cloud server is turned off. Note: Hidden files aren't shown in the Jupyter lab file sidebar (you'll have to use `nano`), so be careful managing \".env\" - make sure you don't commit secrets to your Git repo! If you create a secrets file, download it to your local computer when you're finished with your work, as it'll be destroyed since it's outside"
            - "Whippets are sometimes confused for Greyhounds because they have a similar, graceful body - but generally slender Whippets are smaller and less muscular. \n\nThey have short, sleek fur and long, slender legs, plus a deep chest that is similar to the Greyhound's. Their coat is short and smooth, and comes in a variety of colors and combinations. The most common hues you'll find Whippets coming in are black, red, cream, white, red, fawn, blue, brindle - and everything in between! They're also known to have a plethora of spots and color patches, which keeps things even more interesting."
            - "[9:02 AM] Sam: I just got word there's another glitch in the router logs ðŸ˜“. Any update on that weird bug we saw last night? [9:03 AM] Alex: No big breakthroughs yet. Seems like a stray config issue or something leftover from the old firmware. I can check if there's any mismatch in the system logs, but it might take a while. [9:04 AM] Sam: No worries. Try power cycling the entire system when you can. Let me know if that helps, and if it doesn't, we'll have to escalate. [9:05 AM] Sam: We have to get this fixed asap! [10:00 AM] Alex: Any progress??"
            - "<h1>Americans have always sought debt relief after the holidays<h1>\n<span id="subtitle">Now their struggle is year-round. Overall household debt climbed to a new high of $18.04 trillion in the fourth quarter, the New York Fed reported.</span><div>\n\n<a class="author may-blank id-t2_kc7k9">beer_bongo</a><blockquote>\n<p>In general, a car payment should take up no more than 13%-14% of an individual's net income, Russell said. But with the average payment for used cars at $525 and for new cars at $734, per&nbsp;<a href="https://www.experian.com/blogs/ask-experian/average-used-car-payment/">Experian</a>, that rule of thumb is out the window.</p>\n<p>"We're seeing individuals paying 21%-22% of their income toward car payments. That's almost a quarter of your income just servicing an automobile loan payment," Russell said. "That's not sustainable."</p>\n<p>The term lengths on these loans have also increased to 78 months when they used to be 48 to 60 months long, Russell said, meaning people are stuck with unmanageable payments for more than six years.</p>\n</blockquote><p>7 years of payments at $750 should get you a nice semi luxury models? Not sure why people who can't afford it are buying these 40k vehicles in the first place! Why not get a 25k Toyota Corolla?</p>\n</div>"
            - "the efficient fitting of neural networks is startling, their generalization to new data is dumbfounding. First, it's not obvious a priori that typical datasets are sufficient to characterize the input/output mapping. The curse of dimensionality implies that thetraining dataset is tiny compared to the possible inputs; if each of the 40 inputs of theMNIST-1D data were quantized into 10 possible values, there would be 1040 possible inputs, which is a factor of 1036 more than the number of training examples! Second, deep networks describe very complicated functions."
            - "def table(ax: Axes, data: DataFrame | Series, **kwargs) -> Table:\n\"\"\"Helper function to convert DataFrame and Series to matplotlib.table. This method provides an easy way to visualize tabular data within a Matplotlib\nfigure. It automatically extracts index and column labels from the DataFrame\nor Series, unless explicitly specified. This function is particularly useful\nwhen displaying summary tables alongside other plots or when creating static\nreports. It utilizes the `matplotlib.pyplot.table` backend and allows\ncustomization through various styling options available in Matplotlib.\n\nParameters\n----------\nax : Matplotlib axes object\nThe axes on which to draw the table.\ndata : DataFrame or Series Data for table contents.\n\nExamples\nplot:::context: close-figs\n\n>>> import matplotlib.pyplot as plt\n>>> df = pd.DataFrame({"A": [1, 2], "B": [3, 4]})\n>>> fig, ax = plt.subplots()\n>>> ax.axis("off")\n(0.0, 1.0, 0.0, 1.0)\n>>> table = pd.plotting.table(ax, df, loc="center", cellLoc="center", colWidths=[0.2, 0.2])\"\"\"plot_backend = _get_plot_backend("matplotlib")\n\t#Remove later! Leave for current version only\nreturn plot_backend.table(ax=ax, data=data, rowLabels=None, colLabels=None)"
            - "these tech companies think they can just do whatever the hell they want huh??! I paid for this GODDAMN streaming service THREE MONTHS AGO and it's STILL buffering like it's 2005 on dial-up! WHAT AM I EVEN PAYING FOR?? WHAT A JOKE! I'm done, enjoy watching without me. Â· 48 likes Â· 12 replies"
    ```
- token_meanings:
  - " -": subtraction/negative operator, list-item denoter, text seperator or break, often in informal writing
  - " *": multiplication operator with whitespace, wildcard operator with whitespace in search/patterns, emphasis marker in texting/markdown
  - " {": set notation, programming block delimiter, placeholder
  - " #": comment line indicator, social media tag, numbered item/rank
  - " ~": home directory in unix, approximation symbol, bitwise NOR
  - " $": currency symbol, variable symbol, math mode in latex
  - " bar": establishment serving drinks, long rigid piece of material, legal profession/exam
  - " bill": invoice/payment request, proposed law, bird beak
  - " build": construct (something) by putting parts or material together, the dimensions or proportions of a body or physique, software compilation
  - " class": social or economic class, a course of instruction, a set of category having some proprty in common (not programming)
  - " block": physical cube/rectangular object, group of city streets/buildings, prevent/obstruct something
  - " branch": tree limb, organizational division, git branch
  - " bug": insect, software defect, covert listening device
  - " clip": short video segment, attach/fasten, trim/cut
  - ":": introduces or presents what follows, time seperator, function type annotation
  - " drive": operate a vehicle, storage device, motivating force
  - " key": device to open locks, critical/important, musical tonal framework
  - " Î»": functional programming, greek letter, wavelength symbol
  - " light": electromagnetic radiation/illumination, pale in color, not heavy
  - " match": ignition stick, sports event, a correspondence concept
  - " model": a simplified representation, person who poses for fashion, particular product version
  - " PR": pull request, public relations, personal record
  - " root": unix superuser, plant anatomy, mathematical root
  - " second": unit of time, numberical position after first, to formally support a notion
  - " shell": sea shell, command line shell, explosive
- replacements:
  - â€œ|â€ -> \"
  - â€˜|â€™ -> '