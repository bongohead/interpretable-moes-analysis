{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib \n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.store_outputs import convert_outputs_to_df\n",
    "from utils.vocab import export_vocab_as_csv\n",
    "from utils import pretrained_models\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\n",
    "Architectures supported currently:\n",
    "- OlMoE architecture, includes OLMoE-1B-7B-0125-Instruct (1B/7B)\n",
    "- Qwen2MoE architecture, inclues Qwen1.5-MoE-A2.7B-Chat (2.7B/14.3B), Qwen2-57B-A14B (14B/57B)\n",
    "- Deepseek v2 architecture, includes Deepseek-v2-Lite (2.4B/15.7B), Deepseek-v2 (21B/236B)\n",
    "- Deepseek v3 architecture, includes Deepseek-v3 (37B/671B), Deepseek-R1 (37B/671B), Moonlight-16B-A3B (3B/16B)\n",
    "\"\"\"\n",
    "selected_model_index = 1\n",
    "\n",
    "def get_model(index):\n",
    "    model = [\n",
    "        ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 'olmoe'),\n",
    "        ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 'qwen2moe'),\n",
    "        ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 'dsv2'),\n",
    "        ('moonshotai/Moonlight-16B-A3B', 'moonlight', 'dsv3')\n",
    "    ][index]\n",
    "\n",
    "    return model[0], model[1], model[2]\n",
    "\n",
    "model_id, model_prefix, model_architecture = get_model(selected_model_index)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load reverse-engineered forward pass functions that return topk expert IDs and weights\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "    original_results = model(**inputs)\n",
    "    custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'])\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    assert len(custom_results['all_topk_experts']) == len(custom_results['all_topk_weights']), 'Length of topk IDs and weights not equal'\n",
    "    print(f\"Length of topk: {len(custom_results['all_topk_experts'])}\")\n",
    "    print(f\"Topk size: {custom_results['all_topk_experts'][0].shape}\")\n",
    "    print(f\"First token topk IDs: {custom_results['all_topk_experts'][0][1,]}\")\n",
    "    print(f\"First token topk weights: {custom_results['all_topk_weights'][0][1,]}\")\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), model.config.vocab_size).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Get MMLU data and domains to test\n",
    "\"\"\"\n",
    "raw_mmlu_ds = load_dataset(\"cais/mmlu\", 'all', split = 'test')\n",
    "[print(f\"{domain}: {len([q for q in raw_mmlu_ds if q['subject'] == domain])}\") for domain in set(raw_mmlu_ds['subject'])]\n",
    "\n",
    "# Dict containing {final_domains: [source1, source2], ...}\n",
    "mmlu_domain_mappings = {\n",
    "    'math': ['elementary_mathematics'], \n",
    "    'statistics': ['high_school_statistics'],\n",
    "    'cs': ['high_school_computer_science', 'college_computer_science'], # 100 each\n",
    "    'chemistry': ['high_school_chemistry'],\n",
    "    'biology': ['high_school_biology'],\n",
    "    'nutrition': ['nutrition'],\n",
    "    'psych': ['high_school_psychology']\n",
    "}\n",
    "\n",
    "# Now let's put the MMLU questions into a list grouped by domain\n",
    "def group_mmlu_ds(raw_ds, domain_map, max_questions_per_domain):\n",
    "    source_to_domain_map = {source: domain for domain, sources in mmlu_domain_mappings.items() for source in sources} # Map each source => domain\n",
    "    final_ds = {domain: {'sources': sources, 'questions': []} for domain, sources in domain_map.items()} # Create empty dict to fill\n",
    "    for q in raw_ds: \n",
    "        if q['subject'] in source_to_domain_map.keys():\n",
    "            if (len(final_ds[source_to_domain_map[q['subject']]]['questions']) >= max_questions_per_domain):\n",
    "                continue\n",
    "            final_ds[source_to_domain_map[q['subject']]]['questions'].append({\n",
    "                'question': q['question'],\n",
    "                'choices': q['choices'],\n",
    "                'n_choices': len(q['choices']),\n",
    "                'domain': source_to_domain_map[q['subject']],\n",
    "                'source': q['subject'],\n",
    "                'answer_index': q['answer'],\n",
    "                'answer_char': chr(65 + q['answer'])\n",
    "            })\n",
    "    return [{'domain': domain, **values} for domain, values in final_ds.items()] # Convert back to list of dicts\n",
    "\n",
    "mmlu_ds_grouped = group_mmlu_ds(raw_mmlu_ds, mmlu_domain_mappings, 200)\n",
    "print([len(domain['questions']) for domain in mmlu_ds_grouped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create function to map MMLU data into an instruct-formatted string\n",
    "\"\"\"\n",
    "def prep_question(question, choices):\n",
    "    prompt = f\"Question: {question}\\nChoices:\\n\"\n",
    "    for i, option in enumerate(choices):\n",
    "        letter = chr(65 + i)\n",
    "        prompt += f\"({letter}) {option}\\n\"\n",
    "    return prompt\n",
    "\n",
    "fs_ex = [\n",
    "    [q for q in raw_mmlu_ds if q['subject'] == 'anatomy'][0],\n",
    "    [q for q in raw_mmlu_ds if q['subject'] == 'machine_learning'][0],\n",
    "    [q for q in raw_mmlu_ds if q['subject'] == 'astronomy'][0]\n",
    "]\n",
    "\n",
    "base_prompt = [\n",
    "    {'role': 'system', 'content': 'You will be provided with a multiple-choice question, as well as a list of possible answer choices. Respond exactly with: \"The correct answer is {X}\", substituting in X with the code for the correct choice.'},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[0]['question'], fs_ex[0]['choices'])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + fs_ex[0]['answer'])},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[1]['question'], fs_ex[1]['choices'])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + fs_ex[1]['answer'])},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[2]['question'], fs_ex[2]['choices'])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + fs_ex[2]['answer'])}\n",
    "]\n",
    "\n",
    "mmlu_df = pd.DataFrame([\n",
    "    {\n",
    "        **q,\n",
    "        'q_ix': q_ix,\n",
    "        'input_prompt': tokenizer.apply_chat_template(\n",
    "            base_prompt + [{'role': 'user', 'content': prep_question(q['question'], q['choices'])}, {'role': 'assistant', 'content': 'The correct answer is'}],\n",
    "            tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    "        )\n",
    "    }\n",
    "    for q_ix, q in enumerate(sum([domain['questions'] for domain in mmlu_ds_grouped], []))\n",
    "])\n",
    "\n",
    "display(mmlu_df)\n",
    "print(mmlu_df['input_prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, q_indices, questions, choices, n_choices, domains, answer_chars, tokenized_prompts):\n",
    "        self.q_indices = q_indices\n",
    "        self.questions = questions\n",
    "        self.choices = choices\n",
    "        self.n_choices = n_choices\n",
    "        self.domains = domains\n",
    "        self.answer_chars = answer_chars\n",
    "        self.input_ids = tokenized_prompts['input_ids']\n",
    "        self.attention_mask = tokenized_prompts['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'q_indices': self.q_indices[idx],\n",
    "            'questions': self.questions[idx],\n",
    "            'choices': self.choices[idx],\n",
    "            'n_choices': self.n_choices[idx],\n",
    "            'domains': self.domains[idx],\n",
    "            'answer_chars': self.answer_chars[idx],\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx]\n",
    "        }\n",
    "\n",
    "tokenized_prompts = tokenizer(mmlu_df['input_prompt'].tolist(), add_special_tokens = False, max_length = 1200, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "print(tokenized_prompts['attention_mask'].sum(dim = 1).max()) # Must be under max length to confirm nothing was truncated\n",
    "\n",
    "mmlu_dl = DataLoader(TextDataset(\n",
    "    mmlu_df['q_ix'].tolist(),\n",
    "    mmlu_df['question'].tolist(),\n",
    "    mmlu_df['choices'].tolist(),\n",
    "    mmlu_df['n_choices'].tolist(),\n",
    "    mmlu_df['domain'].tolist(),\n",
    "    mmlu_df['answer_char'].tolist(),\n",
    "    tokenized_prompts\n",
    "), batch_size = 8, shuffle = False)\n",
    "\n",
    "print(next(iter(mmlu_dl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model on MMLU & store topks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes & evaluate correctness\n",
    "\"\"\"\n",
    "output_dfs = []\n",
    "topk_dfs = []\n",
    "\n",
    "for batch_ix, batch in tqdm(enumerate(mmlu_dl), total = len(mmlu_dl)):\n",
    "\n",
    "    input_ids = batch['input_ids'].to(main_device)\n",
    "    attention_mask = batch['attention_mask'].to(main_device)\n",
    "\n",
    "    output = run_model_return_topk(model, input_ids, attention_mask.to(main_device))\n",
    "\n",
    "    # Check no bugs by validating output/perplexity\n",
    "    if batch_ix == 0:\n",
    "        loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), model.config.vocab_size).detach().cpu().item()\n",
    "        for i in range(min(2, input_ids.size(0))):\n",
    "            decoded_input = tokenizer.decode(input_ids[i, :attention_mask[i].sum()], skip_special_tokens = True)\n",
    "            next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "            print(decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = True), 'green'))\n",
    "        print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "        \n",
    "    # Decode answers and correctness\n",
    "    predicted_texts = tokenizer.batch_decode(torch.argmax(output['logits'][:, -1, :], dim = 1).tolist())\n",
    "    predicted_choices = []\n",
    "\n",
    "    for c_ix, c in enumerate(predicted_texts):\n",
    "        predicted_choice = None\n",
    "        if c.upper() in [' ' + chr(65 + i) for i in range(batch['n_choices'][c_ix])]:\n",
    "            predicted_choice = c.upper().strip()\n",
    "        predicted_choices.append(predicted_choice)\n",
    "\n",
    "    q_df = pd.DataFrame({\n",
    "        'q_ix': batch['q_indices'],\n",
    "        'predicted_texts': predicted_texts,\n",
    "        'predicted_choice': predicted_choices,\n",
    "        'correct_choice': batch['answer_chars'],\n",
    "        'is_correct': [1 if predicted_choices[c_ix] == batch['answer_chars'][c_ix] else 0 for c_ix in range(0, len(predicted_texts))],\n",
    "        'is_valid': [1 if predicted_choices[c_ix] is not None else 0 for c_ix in range(0, len(predicted_texts))]\n",
    "    })\n",
    "\n",
    "    # Convert to df and map sequence indices batch to question indices\n",
    "    seq_to_q_map = pd.DataFrame({'sequence_ix': list(range(0, len(batch['questions']))), 'q_ix': batch['q_indices']})\n",
    "\n",
    "    output_df =\\\n",
    "        convert_outputs_to_df(input_ids, attention_mask, output['logits'])\\\n",
    "        .pipe(lambda df: df[df['token_ix'] == df.groupby('sequence_ix')['token_ix'].transform('max')])\\\n",
    "        .merge(seq_to_q_map, how = 'inner').drop(columns = 'sequence_ix')\\\n",
    "        .merge(q_df, how = 'inner', on = 'q_ix')\\\n",
    "        .drop(columns = ['token_ix', 'token_id'])\n",
    "\n",
    "    topk_df =\\\n",
    "        convert_topk_to_df(input_ids, attention_mask, output['all_topk_experts'], output['all_topk_weights'])\\\n",
    "        .pipe(lambda df: df[df['token_ix'] == df.groupby('sequence_ix')['token_ix'].transform('max')])\\\n",
    "        .merge(seq_to_q_map, how = 'inner').drop(columns = 'sequence_ix')\\\n",
    "        .assign(weight = lambda df: df['weight'].round(3))\\\n",
    "        .drop(columns = ['token_ix', 'token_id'])\n",
    "\n",
    "    output_dfs.append(output_df)\n",
    "    topk_dfs.append(topk_df)\n",
    "\n",
    "full_output_df = pd.concat(output_dfs).merge(mmlu_df[['q_ix', 'question', 'domain', 'choices', 'source']], how = 'inner', on = 'q_ix')\n",
    "display(full_output_df)\n",
    "full_topk_df = pd.concat(topk_dfs)\n",
    "display(full_topk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check model accuracy\n",
    "\"\"\"\n",
    "full_output_df = pd.concat(output_dfs).merge(mmlu_df[['q_ix', 'question', 'domain', 'choices', 'source']], how = 'inner', on = 'q_ix')\n",
    "\n",
    "print(f'Overall accuracy: {sum(full_output_df['is_correct'].tolist())/len(full_output_df) * 100:.1f}%')\n",
    "print(f'Overall validity: {sum(full_output_df['is_valid'].tolist())/len(full_output_df) * 100:.1f}%')\n",
    "\n",
    "display(full_output_df\\\n",
    "    .groupby('domain')\\\n",
    "    .agg(\n",
    "        n_accurate = ('is_correct', 'sum'),\n",
    "        n_valid = ('is_valid', 'sum'), \n",
    "        n_total = ('q_ix', 'count')\n",
    "    ).reset_index(drop = False)\\\n",
    "    .assign(\n",
    "        accuracy = lambda df: df['n_accurate']/df['n_total'],\n",
    "        validity = lambda df: df['n_valid']/df['n_total']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "export_vocab_as_csv(tokenizer, f'{model_prefix}-vocab.csv')\n",
    "full_output_df.to_csv(f'{model_prefix}-mmlu-questions.csv', mode = 'w', index = False)\n",
    "full_topk_df.to_csv(f'{model_prefix}-mmlu-topk.csv', mode = 'w', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
