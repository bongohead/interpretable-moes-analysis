{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.vocab import export_vocab_as_csv\n",
    "\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\n",
    "Architectures supported currently:\n",
    "- OlMoE 1B-7B\n",
    "- Qwen 1.5 MoE (same architecture as Qwen2-57B-A14B-Instruct, but still untested)\n",
    "- Deepseek v2 Lite\n",
    "- Moonlight A16 (same architecture as Deepseek v3/R1, but but still untested)\n",
    "\"\"\"\n",
    "selected_model_index = 3\n",
    "\n",
    "def get_model(index):\n",
    "    model = [\n",
    "        ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', '1B/7B'),\n",
    "        ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwenv15', '2.7B/14.3B'),\n",
    "        ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', '2.4B/15.7B'),\n",
    "        ('moonshotai/Moonlight-16B-A3B', 'moonlight', '3B/16B')\n",
    "    ][index]\n",
    "\n",
    "    return model[0], model[1]\n",
    "\n",
    "model_id, model_prefix = get_model(selected_model_index)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset (c4)\n",
    "\"\"\"\n",
    "ds = load_dataset(\"allenai/c4\", 'en', split = 'validation', streaming = True).shuffle(seed = 42, buffer_size = 5_000_000)\n",
    "ds_iter = iter(ds)\n",
    "\n",
    "c4_raw = []\n",
    "for _ in range(0, 5_000_000):\n",
    "    sample = next(ds_iter, None)\n",
    "    if sample is None:\n",
    "        break\n",
    "    c4_raw.append(sample['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer_output):\n",
    "        self.input_ids = tokenizer_output['input_ids']\n",
    "        self.attention_mask = tokenizer_output['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx]\n",
    "        }\n",
    "\n",
    "res = tokenizer(c4_raw, add_special_tokens = False, max_length = 512, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "c4_dl = DataLoader(TextDataset(res), batch_size = 16, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get expert selections + export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define model-specific forward pass functions\n",
    "\n",
    "These functions must return a dict with keys:\n",
    "- `logits`: The standard B x N x V LM output\n",
    "- `all_topk_experts`: A list of length equal to the number of MoE layers, with each element a BN x topk tensor of expert IDs\n",
    "- `all_topkweights`: A list of length equal to the number of MoE layers, with each element a BN x topk tensor of expert weights\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_olmoe_return_topk(input_ids, attention_mask):\n",
    "    input_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    cache_position = torch.arange(0, input_embeds.shape[1], device = input_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = model.model._update_causal_mask(attention_mask, input_embeds, cache_position, None, None)\n",
    "\n",
    "    hidden_state = input_embeds\n",
    "    position_embeddings = model.model.rotary_emb(hidden_state, position_ids)\n",
    "\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer in model.model.layers:\n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        hidden_state, _, _ = layer.self_attn(hidden_states = hidden_state, attention_mask = causal_mask, position_ids = position_ids, position_embeddings = position_embeddings)\n",
    "        hidden_state = residual + hidden_state\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "        ####### OlMoESparseMoeBlock - below code replaces hidden_state = layer.mlp(hidden_state)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_state.shape\n",
    "        moe_hidden_state = hidden_state.view(-1, hidden_dim)\n",
    "        router_logits = layer.mlp.gate(moe_hidden_state)\n",
    "\n",
    "        routing_weights = torch.nn.functional.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, layer.mlp.top_k, dim=-1, sorted = True)\n",
    "        routing_weights = routing_weights.to(moe_hidden_state.dtype)\n",
    "        final_hidden_states = torch.zeros((batch_size * sequence_length, hidden_dim), dtype = hidden_state.dtype, device = hidden_state.device)\n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes = layer.mlp.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(layer.mlp.num_experts):\n",
    "            expert_layer = layer.mlp.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            current_state = moe_hidden_state[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(moe_hidden_state.dtype))\n",
    "\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        #######\n",
    "\n",
    "        hidden_state = final_hidden_states\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "        all_topk_experts.append(selected_experts.detach().cpu())\n",
    "        all_topk_weights.append(routing_weights.detach().cpu().to(torch.float32))\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_qwenv15_return_topk(input_ids, attention_mask):\n",
    "    input_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    cache_position = torch.arange(0, input_embeds.shape[1], device = input_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = model.model._update_causal_mask(attention_mask, input_embeds, cache_position, None, None)\n",
    "\n",
    "    hidden_state = input_embeds\n",
    "    position_embeddings = model.model.rotary_emb(hidden_state, position_ids)\n",
    "\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer in model.model.layers:\n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        hidden_state, _, _ = layer.self_attn(hidden_states = hidden_state, attention_mask = causal_mask, position_ids = position_ids, position_embeddings = position_embeddings)\n",
    "        hidden_state = residual + hidden_state\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "        ####### Qwen2MoeSparseMoeBlock - below code replaces hidden_state = layer.mlp(hidden_state)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_state.shape\n",
    "        moe_hidden_state = hidden_state.view(-1, hidden_dim)\n",
    "        router_logits = layer.mlp.gate(moe_hidden_state) # Size (BN, n_experts)\n",
    "\n",
    "        routing_weights = torch.nn.functional.softmax(router_logits, dim = 1, dtype = torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, layer.mlp.top_k, dim = -1, sorted = True)\n",
    "        routing_weights = routing_weights.to(moe_hidden_state.dtype)\n",
    "\n",
    "        final_hidden_states = torch.zeros((batch_size * sequence_length, hidden_dim), dtype = moe_hidden_state.dtype, device = moe_hidden_state.device)\n",
    "\n",
    "        # One hot encode the selected experts to create an expert mask \n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes = layer.mlp.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(layer.mlp.num_experts):\n",
    "            expert_layer = layer.mlp.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "            # Index the correct hidden states and compute the expert hidden state for the current expert.\n",
    "            current_state = moe_hidden_state[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(moe_hidden_state.dtype))\n",
    "\n",
    "        shared_expert_output = layer.mlp.shared_expert(moe_hidden_state)\n",
    "        shared_expert_output = torch.nn.functional.sigmoid(layer.mlp.shared_expert_gate(moe_hidden_state)) * shared_expert_output\n",
    "\n",
    "        final_hidden_states = (final_hidden_states + shared_expert_output).reshape(batch_size, sequence_length, hidden_dim)\n",
    "        #######\n",
    "        hidden_state = final_hidden_states\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "        all_topk_experts.append(selected_experts.detach().cpu())\n",
    "        all_topk_weights.append(routing_weights.detach().cpu().to(torch.float32))\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "\n",
    "\n",
    "from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n",
    "@torch.no_grad()\n",
    "def run_dsv2_return_topk(input_ids, attention_mask):\n",
    "    B, N = input_ids.shape[:2]\n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "\n",
    "    inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "    hidden_state = inputs_embeds\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer_ix, layer in enumerate(model.model.layers):\n",
    "        # layer_outputs = layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        # Self Attention\n",
    "        hidden_state, self_attn_weights, present_key_value = layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "        hidden_state = residual + hidden_state\n",
    "        # Fully Connected\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "        ## MLP\n",
    "        if 'DeepseekV2MLP' in str(type(layer.mlp)):\n",
    "            hidden_state = layer.mlp(hidden_state)\n",
    "        else:\n",
    "            identity = hidden_state\n",
    "            orig_shape = hidden_state.shape\n",
    "            ### Start MoeGate - originally topk_idx, topk_weight, aux_loss = layer.mlp.gate(hidden_state)\n",
    "            bsz, seq_len, h = hidden_state.shape\n",
    "            moe_hidden_state = hidden_state.view(-1, h)\n",
    "            logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), layer.mlp.gate.weight.type(torch.float32), None)\n",
    "            scores = logits.softmax(dim=-1, dtype=torch.float32)\n",
    "            topk_weight, topk_idx = torch.topk(scores, k=layer.mlp.gate.top_k, dim=-1, sorted=False)\n",
    "            if layer.mlp.gate.top_k > 1 and layer.mlp.gate.norm_topk_prob:\n",
    "                denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20\n",
    "                topk_weight = topk_weight / denominator\n",
    "            else:\n",
    "                topk_weight = topk_weight * layer.mlp.gate.routed_scaling_factor\n",
    "            #### End MoeGate\n",
    "            hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "            ### Start moe_infer\n",
    "            x = hidden_state\n",
    "            topk_ids = topk_idx\n",
    "            cnts = topk_ids.new_zeros((topk_ids.shape[0], len(layer.mlp.experts)))\n",
    "            cnts.scatter_(1, topk_ids, 1)\n",
    "            tokens_per_expert = cnts.sum(dim=0)\n",
    "            idxs = topk_ids.view(-1).argsort()\n",
    "            sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "            tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "            outputs = []\n",
    "            start_idx = 0\n",
    "            for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                end_idx = start_idx + num_tokens\n",
    "                if num_tokens == 0:\n",
    "                    continue\n",
    "                expert = layer.mlp.experts[i + layer.mlp.ep_rank * layer.mlp.experts_per_rank]\n",
    "                tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                expert_out = expert(tokens_for_this_expert)\n",
    "                outputs.append(expert_out)\n",
    "                start_idx = end_idx\n",
    "            outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "            new_x = torch.empty_like(outs)\n",
    "            new_x[idxs] = outs\n",
    "            final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "            ### End moe_infer\n",
    "            y = final_out.view(*orig_shape)\n",
    "            if layer.mlp.config.n_shared_experts is not None:\n",
    "                y = y + layer.mlp.shared_experts(identity)\n",
    "            hidden_state = y\n",
    "\n",
    "            all_topk_experts.append(topk_ids)\n",
    "            all_topk_weights.append(topk_weight)\n",
    "\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_dsv3_return_topk(input_ids, attention_mask):\n",
    "    B, N = input_ids.shape[:2]\n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "    inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    if model.model._use_flash_attention_2:\n",
    "        attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
    "    else:\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "    hidden_state = inputs_embeds\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer_ix, layer in enumerate(model.model.layers):\n",
    "        # layer_outputs = layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        # Self Attention\n",
    "        hidden_state, self_attn_weights, present_key_value = layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "        hidden_state = residual + hidden_state\n",
    "        # Fully Connected\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "        ## MLP\n",
    "        if 'DeepseekV3MLP' in str(type(layer.mlp)):\n",
    "            hidden_state = layer.mlp(hidden_state)\n",
    "        else:\n",
    "            identity = hidden_state\n",
    "            orig_shape = hidden_state.shape\n",
    "            ### Start MoeGate - originally topk_idx, topk_weight = layer.mlp.gate(hidden_state)\n",
    "            bsz, seq_len, h = hidden_state.shape\n",
    "            moe_hidden_state = hidden_state.view(-1, h)\n",
    "            logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), layer.mlp.gate.weight.type(torch.float32), None)\n",
    "            scores = logits.sigmoid()\n",
    "            \n",
    "            scores_for_choice = scores.view(bsz * seq_len, -1) + layer.mlp.gate.e_score_correction_bias.unsqueeze(0)\n",
    "            group_scores = (scores_for_choice.view(bsz * seq_len, layer.mlp.gate.n_group, -1).topk(2, dim=-1)[0].sum(dim = -1))  # [n, n_group]\n",
    "            group_idx = torch.topk(group_scores, k = layer.mlp.gate.topk_group, dim=-1, sorted = False)[1]  # [n, top_k_group]\n",
    "            group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n",
    "            group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n",
    "            score_mask = (group_mask.unsqueeze(-1).expand(bsz * seq_len, layer.mlp.gate.n_group, layer.mlp.gate.n_routed_experts // layer.mlp.gate.n_group).reshape(bsz * seq_len, -1))  # [n, e]\n",
    "            tmp_scores = scores_for_choice.masked_fill(~score_mask.bool(), 0.0)  # [n, e]\n",
    "            _, topk_idx = torch.topk(tmp_scores, k=layer.mlp.gate.top_k, dim=-1, sorted=True)\n",
    "            topk_weight = scores.gather(1, topk_idx)\n",
    "            if layer.mlp.gate.top_k > 1 and layer.mlp.gate.norm_topk_prob:\n",
    "                denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20\n",
    "                topk_weight = topk_weight / denominator\n",
    "            else:\n",
    "                topk_weight = topk_weight * layer.mlp.gate.routed_scaling_factor\n",
    "            ### End MoeGate \n",
    "            hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "            ### Start moe_infer - replaces layer.mlp.moe_infer(hidden_state, topk_idx, topk_weight).view(*orig_shape)\n",
    "            x = hidden_state\n",
    "            topk_ids = topk_idx\n",
    "            cnts = topk_ids.new_zeros((topk_ids.shape[0], len(layer.mlp.experts)))\n",
    "            cnts.scatter_(1, topk_ids, 1)\n",
    "            tokens_per_expert = cnts.sum(dim=0)\n",
    "            idxs = topk_ids.view(-1).argsort()\n",
    "            sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "            tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "            outputs = []\n",
    "            start_idx = 0\n",
    "            for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                end_idx = start_idx + num_tokens\n",
    "                if num_tokens == 0:\n",
    "                    continue\n",
    "                expert = layer.mlp.experts[i + layer.mlp.ep_rank * layer.mlp.experts_per_rank]\n",
    "                tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                expert_out = expert(tokens_for_this_expert)\n",
    "                outputs.append(expert_out)\n",
    "                start_idx = end_idx\n",
    "            outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "            new_x = torch.empty_like(outs)\n",
    "            new_x[idxs] = outs\n",
    "            final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "            ### End moe_infer\n",
    "            if layer.mlp.config.n_shared_experts is not None:\n",
    "                y = y + layer.mlp.shared_experts(identity)\n",
    "            hidden_state = y\n",
    "\n",
    "            all_topk_experts.append(topk_ids)\n",
    "            all_topk_weights.append(topk_weight)\n",
    "\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state).float()\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Export expert selections\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def run_and_export_topk(model_id, model_prefix, c4_dl, batches_to_test = 250):\n",
    "    \"\"\"\n",
    "    Run forward passes on a given model ID, return topk df\n",
    "    \"\"\"\n",
    "    b_count = 0\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(c4_dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        \n",
    "        if model_prefix == 'olmoe':\n",
    "            fn = run_olmoe_return_topk\n",
    "        elif model_prefix == 'qwenv15':\n",
    "            fn = run_qwenv15_return_topk\n",
    "        elif model_prefix == 'dsv2':\n",
    "            fn = run_dsv2_return_topk\n",
    "        elif model_prefix == 'moonlight':\n",
    "            fn = run_dsv3_return_topk\n",
    "        else:\n",
    "            raise Exception('Unsupported model!')\n",
    "\n",
    "        output = fn(input_ids, attention_mask)\n",
    "        label_ids = torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "        base_loss = ForCausalLMLoss(output_logits, label_ids, model.config.vocab_size)\n",
    "        print(torch.exp(base_loss)) # Verify no bugs by checking that the perplexity is normal\n",
    "\n",
    "        topk_df = convert_topk_to_df(input_ids, output['all_topk_experts'], output['all_topk_weights'])\n",
    "        topk_df =\\\n",
    "            topk_df[topk_df['token_id'] != tokenizer.pad_token_id]\\\n",
    "            .assign(\n",
    "                batch_ix = batch_ix,\n",
    "                weight = lambda df: df['weight'].round(3)\n",
    "            )\n",
    "\n",
    "        topk_df\\\n",
    "            .to_csv(f'{model_prefix}-c4-routes.csv', mode = 'w' if batch_ix == 0 else 'a', index = False, header = (batch_ix == 0))\n",
    "        \n",
    "        topk_df[topk_df['topk_ix'] == 1]\\\n",
    "            .to_csv(f'{model_prefix}-c4-routes-top1.csv', mode = 'w' if batch_ix == 0 else 'a', index = False, header = (batch_ix == 0))\n",
    "\n",
    "        b_count += 1\n",
    "        if b_count >= batches_to_test:\n",
    "            break\n",
    "\n",
    "    return True\n",
    "\n",
    "export_vocab_as_csv(tokenizer, f'{model_prefix}-vocab.csv')\n",
    "run_and_export_topk(model_id, model_prefix, c4_dl, batches_to_test = 250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
