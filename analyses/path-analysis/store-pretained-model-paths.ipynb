{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CUDA memory cleared on all devices.\n",
      "Device 0: NVIDIA RTX 6000 Ada Generation\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 47.50 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib \n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.store_outputs import convert_outputs_to_df\n",
    "from utils.vocab import export_vocab_as_csv\n",
    "from utils import pretrained_models\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442ff1d935b0482e8b5aa2d659d832d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.50 GiB of which 16.56 MiB is free. Process 2563311 has 45.89 GiB memory in use. Process 2828758 has 1.58 GiB memory in use. Of the allocated memory 1.06 GiB is allocated by PyTorch, and 107.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m model_id, model_prefix, model_architecture = get_model(selected_model_index)\n\u001b[32m     23\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = \u001b[38;5;28;01mFalse\u001b[39;00m, add_bos_token = \u001b[38;5;28;01mFalse\u001b[39;00m, padding_side = \u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m, trust_remote_code = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3668\u001b[39m, in \u001b[36mPreTrainedModel.cuda\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3663\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3664\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCalling `cuda()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3665\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3666\u001b[39m         )\n\u001b[32m   3667\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3668\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1053\u001b[39m, in \u001b[36mModule.cuda\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> T:\n\u001b[32m   1037\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[32m   1038\u001b[39m \n\u001b[32m   1039\u001b[39m \u001b[33;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1051\u001b[39m \u001b[33;03m        Module: self\u001b[39;00m\n\u001b[32m   1052\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 903 (4 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1053\u001b[39m, in \u001b[36mModule.cuda.<locals>.<lambda>\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> T:\n\u001b[32m   1037\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[32m   1038\u001b[39m \n\u001b[32m   1039\u001b[39m \u001b[33;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1051\u001b[39m \u001b[33;03m        Module: self\u001b[39;00m\n\u001b[32m   1052\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 47.50 GiB of which 16.56 MiB is free. Process 2563311 has 45.89 GiB memory in use. Process 2828758 has 1.58 GiB memory in use. Of the allocated memory 1.06 GiB is allocated by PyTorch, and 107.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\n",
    "Architectures supported currently:\n",
    "- OlMoE architecture, includes OLMoE-1B-7B-0125-Instruct (1B/7B)\n",
    "- Qwen2MoE architecture, inclues Qwen1.5-MoE-A2.7B-Chat (2.7B/14.3B), Qwen2-57B-A14B (14B/57B)\n",
    "- Deepseek v2 architecture, includes Deepseek-v2-Lite (2.4B/15.7B), Deepseek-v2 (21B/236B)\n",
    "- Deepseek v3 architecture, includes Deepseek-v3 (37B/671B), Deepseek-R1 (37B/671B), Moonlight-16B-A3B (3B/16B)\n",
    "\"\"\"\n",
    "selected_model_index = 1\n",
    "\n",
    "def get_model(index):\n",
    "    model = [\n",
    "        ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 'olmoe'),\n",
    "        ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 'qwen2moe'),\n",
    "        ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 'dsv2'),\n",
    "        ('moonshotai/Moonlight-16B-A3B', 'moonlight', 'dsv3')\n",
    "    ][index]\n",
    "\n",
    "    return model[0], model[1], model[2]\n",
    "\n",
    "model_id, model_prefix, model_architecture = get_model(selected_model_index)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load reverse-engineered forward pass functions that return topk expert IDs and weights\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "    original_results = model(**inputs)\n",
    "    custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'])\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    assert len(custom_results['all_topk_experts']) == len(custom_results['all_topk_weights']), 'Length of topk IDs and weights not equal'\n",
    "    print(f\"Length of topk: {len(custom_results['all_topk_experts'])}\")\n",
    "    print(f\"Topk size: {custom_results['all_topk_experts'][0].shape}\")\n",
    "    print(f\"First token topk IDs: {custom_results['all_topk_experts'][0][1,]}\")\n",
    "    print(f\"First token topk weights: {custom_results['all_topk_weights'][0][1,]}\")\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), model.config.vocab_size).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset (c4)\n",
    "\"\"\"\n",
    "ds = load_dataset('allenai/c4', 'en', split = 'validation', streaming = True).shuffle(seed = 123, buffer_size = 1_000_000)\n",
    "# ds = load_dataset('HuggingFaceFW/fineweb-edu', 'CC-MAIN-2024-51', split = 'train', streaming = True).shuffle(seed = 123, buffer_size = 1_000_000)\n",
    "ds_iter = iter(ds)\n",
    "\n",
    "c4_raw = []\n",
    "for _ in range(0, 25_000):\n",
    "    sample = next(ds_iter, None)\n",
    "    if sample is None:\n",
    "        break\n",
    "    c4_raw.append(sample['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer_output):\n",
    "        self.input_ids = tokenizer_output['input_ids']\n",
    "        self.attention_mask = tokenizer_output['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx]\n",
    "        }\n",
    "\n",
    "res = tokenizer(c4_raw, add_special_tokens = False, max_length = 512, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "c4_dl = DataLoader(TextDataset(res), batch_size = 8, shuffle = False) # 64 for OlMoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get expert selections + export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define model-specific forward pass functions\n",
    "\n",
    "These functions must return a dict with keys:\n",
    "- `logits`: The standard B x N x V LM output\n",
    "- `all_topk_experts`: A list of length equal to the number of MoE layers, with each element a BN x topk tensor of expert IDs\n",
    "- `all_topk_weights`: A list of length equal to the number of MoE layers, with each element a BN x topk tensor of expert weights\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_olmoe_return_topk(input_ids, attention_mask):\n",
    "    input_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    cache_position = torch.arange(0, input_embeds.shape[1], device = input_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = model.model._update_causal_mask(attention_mask, input_embeds, cache_position, None, None)\n",
    "\n",
    "    hidden_state = input_embeds\n",
    "    position_embeddings = model.model.rotary_emb(hidden_state, position_ids)\n",
    "\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer in model.model.layers:\n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        hidden_state, _, _ = layer.self_attn(hidden_states = hidden_state, attention_mask = causal_mask, position_ids = position_ids, position_embeddings = position_embeddings)\n",
    "        hidden_state = residual + hidden_state\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "        ####### OlMoESparseMoeBlock - below code replaces hidden_state = layer.mlp(hidden_state)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_state.shape\n",
    "        moe_hidden_state = hidden_state.view(-1, hidden_dim)\n",
    "        router_logits = layer.mlp.gate(moe_hidden_state)\n",
    "\n",
    "        routing_weights = torch.nn.functional.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, layer.mlp.top_k, dim=-1, sorted = True)\n",
    "        routing_weights = routing_weights.to(moe_hidden_state.dtype)\n",
    "        final_hidden_states = torch.zeros((batch_size * sequence_length, hidden_dim), dtype = hidden_state.dtype, device = hidden_state.device)\n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes = layer.mlp.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(layer.mlp.num_experts):\n",
    "            expert_layer = layer.mlp.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            current_state = moe_hidden_state[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(moe_hidden_state.dtype))\n",
    "\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        #######\n",
    "\n",
    "        hidden_state = final_hidden_states\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "        all_topk_experts.append(selected_experts.detach().cpu())\n",
    "        all_topk_weights.append(routing_weights.detach().cpu().to(torch.float32))\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_qwenv15_return_topk(input_ids, attention_mask):\n",
    "    input_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    cache_position = torch.arange(0, input_embeds.shape[1], device = input_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = model.model._update_causal_mask(attention_mask, input_embeds, cache_position, None, None)\n",
    "\n",
    "    hidden_state = input_embeds\n",
    "    position_embeddings = model.model.rotary_emb(hidden_state, position_ids)\n",
    "\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer in model.model.layers:\n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        hidden_state, _, _ = layer.self_attn(hidden_states = hidden_state, attention_mask = causal_mask, position_ids = position_ids, position_embeddings = position_embeddings)\n",
    "        hidden_state = residual + hidden_state\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "        ####### Qwen2MoeSparseMoeBlock - below code replaces hidden_state = layer.mlp(hidden_state)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_state.shape\n",
    "        moe_hidden_state = hidden_state.view(-1, hidden_dim)\n",
    "        router_logits = layer.mlp.gate(moe_hidden_state) # Size (BN, n_experts)\n",
    "\n",
    "        routing_weights = torch.nn.functional.softmax(router_logits, dim = 1, dtype = torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, layer.mlp.top_k, dim = -1, sorted = True)\n",
    "        routing_weights = routing_weights.to(moe_hidden_state.dtype)\n",
    "\n",
    "        final_hidden_states = torch.zeros((batch_size * sequence_length, hidden_dim), dtype = moe_hidden_state.dtype, device = moe_hidden_state.device)\n",
    "\n",
    "        # One hot encode the selected experts to create an expert mask \n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes = layer.mlp.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(layer.mlp.num_experts):\n",
    "            expert_layer = layer.mlp.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "            # Index the correct hidden states and compute the expert hidden state for the current expert.\n",
    "            current_state = moe_hidden_state[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(moe_hidden_state.dtype))\n",
    "\n",
    "        shared_expert_output = layer.mlp.shared_expert(moe_hidden_state)\n",
    "        shared_expert_output = torch.nn.functional.sigmoid(layer.mlp.shared_expert_gate(moe_hidden_state)) * shared_expert_output\n",
    "\n",
    "        final_hidden_states = (final_hidden_states + shared_expert_output).reshape(batch_size, sequence_length, hidden_dim)\n",
    "        #######\n",
    "        hidden_state = final_hidden_states\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "        all_topk_experts.append(selected_experts.detach().cpu())\n",
    "        all_topk_weights.append(routing_weights.detach().cpu().to(torch.float32))\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "\n",
    "\n",
    "from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n",
    "@torch.no_grad()\n",
    "def run_dsv2_return_topk(input_ids, attention_mask):\n",
    "    B, N = input_ids.shape[:2]\n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "\n",
    "    inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "    hidden_state = inputs_embeds\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer_ix, layer in enumerate(model.model.layers):\n",
    "        # layer_outputs = layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        # Self Attention\n",
    "        hidden_state, self_attn_weights, present_key_value = layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "        hidden_state = residual + hidden_state\n",
    "        # Fully Connected\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "        ## MLP\n",
    "        if 'DeepseekV2MLP' in str(type(layer.mlp)):\n",
    "            hidden_state = layer.mlp(hidden_state)\n",
    "        else:\n",
    "            identity = hidden_state\n",
    "            orig_shape = hidden_state.shape\n",
    "            ### Start MoeGate - originally topk_idx, topk_weight, aux_loss = layer.mlp.gate(hidden_state)\n",
    "            bsz, seq_len, h = hidden_state.shape\n",
    "            moe_hidden_state = hidden_state.view(-1, h)\n",
    "            logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), layer.mlp.gate.weight.type(torch.float32), None)\n",
    "            scores = logits.softmax(dim=-1, dtype=torch.float32)\n",
    "            topk_weight, topk_idx = torch.topk(scores, k=layer.mlp.gate.top_k, dim=-1, sorted=False)\n",
    "            if layer.mlp.gate.top_k > 1 and layer.mlp.gate.norm_topk_prob:\n",
    "                denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20\n",
    "                topk_weight = topk_weight / denominator\n",
    "            else:\n",
    "                topk_weight = topk_weight * layer.mlp.gate.routed_scaling_factor\n",
    "            #### End MoeGate\n",
    "            hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "            ### Start moe_infer\n",
    "            x = hidden_state\n",
    "            topk_ids = topk_idx\n",
    "            cnts = topk_ids.new_zeros((topk_ids.shape[0], len(layer.mlp.experts)))\n",
    "            cnts.scatter_(1, topk_ids, 1)\n",
    "            tokens_per_expert = cnts.sum(dim=0)\n",
    "            idxs = topk_ids.view(-1).argsort()\n",
    "            sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "            tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "            outputs = []\n",
    "            start_idx = 0\n",
    "            for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                end_idx = start_idx + num_tokens\n",
    "                if num_tokens == 0:\n",
    "                    continue\n",
    "                expert = layer.mlp.experts[i + layer.mlp.ep_rank * layer.mlp.experts_per_rank]\n",
    "                tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                expert_out = expert(tokens_for_this_expert)\n",
    "                outputs.append(expert_out)\n",
    "                start_idx = end_idx\n",
    "            outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "            new_x = torch.empty_like(outs)\n",
    "            new_x[idxs] = outs\n",
    "            final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "            ### End moe_infer\n",
    "            y = final_out.view(*orig_shape)\n",
    "            if layer.mlp.config.n_shared_experts is not None:\n",
    "                y = y + layer.mlp.shared_experts(identity)\n",
    "            hidden_state = y\n",
    "\n",
    "            all_topk_experts.append(topk_ids)\n",
    "            all_topk_weights.append(topk_weight)\n",
    "\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_dsv3_return_topk(input_ids, attention_mask):\n",
    "    B, N = input_ids.shape[:2]\n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "    inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    if model.model._use_flash_attention_2:\n",
    "        attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
    "    else:\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "    hidden_state = inputs_embeds\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer_ix, layer in enumerate(model.model.layers):\n",
    "        # layer_outputs = layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        # Self Attention\n",
    "        hidden_state, self_attn_weights, present_key_value = layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "        hidden_state = residual + hidden_state\n",
    "        # Fully Connected\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "        ## MLP\n",
    "        if 'DeepseekV3MLP' in str(type(layer.mlp)):\n",
    "            hidden_state = layer.mlp(hidden_state)\n",
    "        else:\n",
    "            identity = hidden_state\n",
    "            orig_shape = hidden_state.shape\n",
    "            ### Start MoeGate - originally topk_idx, topk_weight = layer.mlp.gate(hidden_state)\n",
    "            bsz, seq_len, h = hidden_state.shape\n",
    "            moe_hidden_state = hidden_state.view(-1, h)\n",
    "            logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), layer.mlp.gate.weight.type(torch.float32), None)\n",
    "            scores = logits.sigmoid()\n",
    "            \n",
    "            scores_for_choice = scores.view(bsz * seq_len, -1) + layer.mlp.gate.e_score_correction_bias.unsqueeze(0)\n",
    "            group_scores = (scores_for_choice.view(bsz * seq_len, layer.mlp.gate.n_group, -1).topk(2, dim=-1)[0].sum(dim = -1))  # [n, n_group]\n",
    "            group_idx = torch.topk(group_scores, k = layer.mlp.gate.topk_group, dim=-1, sorted = False)[1]  # [n, top_k_group]\n",
    "            group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n",
    "            group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n",
    "            score_mask = (group_mask.unsqueeze(-1).expand(bsz * seq_len, layer.mlp.gate.n_group, layer.mlp.gate.n_routed_experts // layer.mlp.gate.n_group).reshape(bsz * seq_len, -1))  # [n, e]\n",
    "            tmp_scores = scores_for_choice.masked_fill(~score_mask.bool(), 0.0)  # [n, e]\n",
    "            _, topk_idx = torch.topk(tmp_scores, k=layer.mlp.gate.top_k, dim=-1, sorted=True)\n",
    "            topk_weight = scores.gather(1, topk_idx)\n",
    "            if layer.mlp.gate.top_k > 1 and layer.mlp.gate.norm_topk_prob:\n",
    "                denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20\n",
    "                topk_weight = topk_weight / denominator\n",
    "            else:\n",
    "                topk_weight = topk_weight * layer.mlp.gate.routed_scaling_factor\n",
    "            ### End MoeGate \n",
    "            hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "            ### Start moe_infer - replaces layer.mlp.moe_infer(hidden_state, topk_idx, topk_weight).view(*orig_shape)\n",
    "            x = hidden_state\n",
    "            topk_ids = topk_idx\n",
    "            cnts = topk_ids.new_zeros((topk_ids.shape[0], len(layer.mlp.experts)))\n",
    "            cnts.scatter_(1, topk_ids, 1)\n",
    "            tokens_per_expert = cnts.sum(dim=0)\n",
    "            idxs = topk_ids.view(-1).argsort()\n",
    "            sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "            tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "            outputs = []\n",
    "            start_idx = 0\n",
    "            for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                end_idx = start_idx + num_tokens\n",
    "                if num_tokens == 0:\n",
    "                    continue\n",
    "                expert = layer.mlp.experts[i + layer.mlp.ep_rank * layer.mlp.experts_per_rank]\n",
    "                tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                expert_out = expert(tokens_for_this_expert)\n",
    "                outputs.append(expert_out)\n",
    "                start_idx = end_idx\n",
    "            outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "            new_x = torch.empty_like(outs)\n",
    "            new_x[idxs] = outs\n",
    "            final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "            ### End moe_infer\n",
    "            y = final_out.view(*orig_shape)\n",
    "            if layer.mlp.config.n_shared_experts is not None:\n",
    "                y = y + layer.mlp.shared_experts(identity)\n",
    "            hidden_state = y\n",
    "\n",
    "            all_topk_experts.append(topk_ids)\n",
    "            all_topk_weights.append(topk_weight)\n",
    "\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state).float()\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Export expert selections\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_and_export_topk(model_prefix, c4_dl, max_batches = None):\n",
    "    \"\"\"\n",
    "    Run forward passes on a given model ID, return topk df\n",
    "    \"\"\"\n",
    "    b_count = 0\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(c4_dl), total = len(c4_dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        \n",
    "        if model_prefix == 'olmoe':\n",
    "            fn = run_olmoe_return_topk\n",
    "        elif model_prefix == 'qwenv15':\n",
    "            fn = run_qwenv15_return_topk\n",
    "        elif model_prefix == 'dsv2':\n",
    "            fn = run_dsv2_return_topk\n",
    "        elif model_prefix == 'moonlight':\n",
    "            fn = run_dsv3_return_topk\n",
    "        else:\n",
    "            raise Exception('Unsupported model!')\n",
    "\n",
    "        output = fn(input_ids, attention_mask)\n",
    "\n",
    "        # Check no bugs by validating output/perplexity\n",
    "        if batch_ix == 0:\n",
    "            label_ids = torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "            base_loss = ForCausalLMLoss(output['logits'], label_ids, model.config.vocab_size).detach().cpu().item()\n",
    "            for i in range(min(2, input_ids.size(0))):\n",
    "                decoded_input = tokenizer.decode(input_ids[i, :attention_mask[i].sum()], skip_special_tokens = True)\n",
    "                next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                print(decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = True), 'green'))\n",
    "            print(f\"PPL:\", torch.exp(torch.tensor(base_loss)).item())\n",
    "        \n",
    "        output_df =\\\n",
    "            convert_outputs_to_df(input_ids, attention_mask, output['logits'])\\\n",
    "            .assign(\n",
    "                batch_ix = batch_ix\n",
    "            )\n",
    "\n",
    "        topk_df =\\\n",
    "            convert_topk_to_df(input_ids, attention_mask, output['all_topk_experts'], output['all_topk_weights'])\\\n",
    "            .assign(\n",
    "                batch_ix = batch_ix,\n",
    "                weight = lambda df: df['weight'].round(3)\n",
    "            )\n",
    "        \n",
    "        output_df\\\n",
    "            .to_csv(f'{model_prefix}-c4-outputs.csv', mode = 'w' if batch_ix == 0 else 'a', index = False, header = (batch_ix == 0))\n",
    "\n",
    "        topk_df\\\n",
    "            .to_csv(f'{model_prefix}-c4-routes.csv', mode = 'w' if batch_ix == 0 else 'a', index = False, header = (batch_ix == 0))\n",
    "        \n",
    "        topk_df[topk_df['topk_ix'] == 1]\\\n",
    "            .to_csv(f'{model_prefix}-c4-routes-top1.csv', mode = 'w' if batch_ix == 0 else 'a', index = False, header = (batch_ix == 0))\n",
    "\n",
    "        b_count += 1\n",
    "        if max_batches is not None and b_count >= max_batches:\n",
    "            break\n",
    "\n",
    "    return True\n",
    "\n",
    "export_vocab_as_csv(tokenizer, f'{model_prefix}-vocab.csv')\n",
    "run_and_export_topk(model_prefix, c4_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
