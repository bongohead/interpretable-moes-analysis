{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.vocab import export_vocab_as_csv\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_id = [\n",
    "    'allenai/OLMoE-1B-7B-0125-Instruct',\n",
    "    'Qwen/Qwen1.5-MoE-A2.7B-Chat',\n",
    "    'deepseek-ai/DeepSeek-V2-Lite'\n",
    "][0]\n",
    "\n",
    "model_prefix = {\n",
    "    'allenai/OLMoE-1B-7B-0125-Instruct': 'olmoe',\n",
    "    'Qwen/Qwen1.5-MoE-A2.7B-Chat': 'qwenv15',\n",
    "    'deepseek-ai/DeepSeek-V2-Lite': 'dsv2',\n",
    "\n",
    "}[model_id]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"allenai/c4\", 'en', split = 'validation', streaming = True).shuffle(seed = 42, buffer_size = 5_000_000)\n",
    "ds_iter = iter(ds)\n",
    "\n",
    "c4_raw = []\n",
    "for _ in range(0, 5_000_000):\n",
    "    sample = next(ds_iter, None)\n",
    "    if sample is None:\n",
    "        break\n",
    "    c4_raw.append(sample['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer_output):\n",
    "        self.input_ids = tokenizer_output['input_ids']\n",
    "        self.attention_mask = tokenizer_output['attention_mask']\n",
    "        self.tokens = tokenizer.batch_decode(self.input_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'tokens': self.tokens[idx],\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx]\n",
    "        }\n",
    "\n",
    "res = tokenizer(c4_raw, add_special_tokens = False, max_length = 512, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "c4_dl = DataLoader(TextDataset(res), batch_size = 16, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get expert selections + export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define model-specific functions\n",
    "\n",
    "These functions must return a dict with keys:\n",
    "- `logits`: The standard B x N x V LM output\n",
    "- `all_topk_experts`: A list of length equal to the number of MoE layers, with each element a BN x topk tensor of expert IDs\n",
    "- `all_topkweights`: A list of length equal to the number of MoE layers, with each element a BN x topk tensor of expert weights\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_olmoe_return_topk(input_ids, attention_mask):\n",
    "    input_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    cache_position = torch.arange(0, input_embeds.shape[1], device = input_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = model.model._update_causal_mask(attention_mask, input_embeds, cache_position, None, None)\n",
    "\n",
    "    hidden_state = input_embeds\n",
    "    position_embeddings = model.model.rotary_emb(hidden_state, position_ids)\n",
    "\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer in model.model.layers:\n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        hidden_state, _, _ = layer.self_attn(hidden_states = hidden_state, attention_mask = causal_mask, position_ids = position_ids, position_embeddings = position_embeddings)\n",
    "        hidden_state = residual + hidden_state\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "        ####### OlMoESparseMoeBlock - below code replaces hidden_state = layer.mlp(hidden_state)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_state.shape\n",
    "        moe_hidden_state = hidden_state.view(-1, hidden_dim)\n",
    "        router_logits = layer.mlp.gate(moe_hidden_state)\n",
    "\n",
    "        routing_weights = torch.nn.functional.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, layer.mlp.top_k, dim=-1, sorted = True)\n",
    "        routing_weights = routing_weights.to(moe_hidden_state.dtype)\n",
    "        final_hidden_states = torch.zeros((batch_size * sequence_length, hidden_dim), dtype = hidden_state.dtype, device = hidden_state.device)\n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes = layer.mlp.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(layer.mlp.num_experts):\n",
    "            expert_layer = layer.mlp.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            current_state = moe_hidden_state[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(moe_hidden_state.dtype))\n",
    "\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        #######\n",
    "\n",
    "        hidden_state = final_hidden_states\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "        all_topk_experts.append(selected_experts.detach().cpu())\n",
    "        all_topk_weights.append(routing_weights.detach().cpu().to(torch.float32))\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_qwenv15_return_topk(input_ids, attention_mask):\n",
    "    input_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    cache_position = torch.arange(0, input_embeds.shape[1], device = input_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = model.model._update_causal_mask(attention_mask, input_embeds, cache_position, None, None)\n",
    "\n",
    "    hidden_state = input_embeds\n",
    "    position_embeddings = model.model.rotary_emb(hidden_state, position_ids)\n",
    "\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer in model.model.layers:\n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        hidden_state, _, _ = layer.self_attn(hidden_states = hidden_state, attention_mask = causal_mask, position_ids = position_ids, position_embeddings = position_embeddings)\n",
    "        hidden_state = residual + hidden_state\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "        ####### Qwen2MoeSparseMoeBlock - below code replaces hidden_state = layer.mlp(hidden_state)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_state.shape\n",
    "        moe_hidden_state = hidden_state.view(-1, hidden_dim)\n",
    "        router_logits = layer.mlp.gate(moe_hidden_state) # Size (BN, n_experts)\n",
    "\n",
    "        routing_weights = torch.nn.functional.softmax(router_logits, dim = 1, dtype = torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, layer.mlp.top_k, dim = -1, sorted = True)\n",
    "        routing_weights = routing_weights.to(moe_hidden_state.dtype)\n",
    "\n",
    "        final_hidden_states = torch.zeros((batch_size * sequence_length, hidden_dim), dtype = moe_hidden_state.dtype, device = moe_hidden_state.device)\n",
    "\n",
    "        # One hot encode the selected experts to create an expert mask \n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes = layer.mlp.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(layer.mlp.num_experts):\n",
    "            expert_layer = layer.mlp.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "            # Index the correct hidden states and compute the expert hidden state for the current expert.\n",
    "            current_state = moe_hidden_state[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(moe_hidden_state.dtype))\n",
    "\n",
    "        shared_expert_output = layer.mlp.shared_expert(moe_hidden_state)\n",
    "        shared_expert_output = torch.nn.functional.sigmoid(layer.mlp.shared_expert_gate(moe_hidden_state)) * shared_expert_output\n",
    "\n",
    "        final_hidden_states = (final_hidden_states + shared_expert_output).reshape(batch_size, sequence_length, hidden_dim)\n",
    "        #######\n",
    "        hidden_state = final_hidden_states\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "        all_topk_experts.append(selected_experts.detach().cpu())\n",
    "        all_topk_weights.append(routing_weights.detach().cpu().to(torch.float32))\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "\n",
    "\n",
    "from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n",
    "@torch.no_grad()\n",
    "def run_dsv2_return_topk(input_ids, attention_mask):\n",
    "    B, N = input_ids.shape[:2]\n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "\n",
    "    inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "    hidden_state = inputs_embeds\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer_ix, decoder_layer in enumerate(model.model.layers):\n",
    "        # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "        residual = hidden_state\n",
    "        hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "        # Self Attention\n",
    "        hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "        hidden_state = residual + hidden_state\n",
    "        # Fully Connected\n",
    "        residual = hidden_state\n",
    "        hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "        ## MLP\n",
    "        if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "            hidden_state = decoder_layer.mlp(hidden_state)\n",
    "        else:\n",
    "            identity = hidden_state\n",
    "            orig_shape = hidden_state.shape\n",
    "            ### moegate - originally topk_idx, topk_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "            bsz, seq_len, h = hidden_state.shape\n",
    "            moe_hidden_state = hidden_state.view(-1, h)\n",
    "            logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), decoder_layer.mlp.gate.weight.type(torch.float32), None)\n",
    "            scores = logits.softmax(dim=-1, dtype=torch.float32)\n",
    "            topk_weight, topk_idx = torch.topk(scores, k=decoder_layer.mlp.gate.top_k, dim=-1, sorted=False)\n",
    "            topk_weight = topk_weight * decoder_layer.mlp.gate.routed_scaling_factor\n",
    "            ####\n",
    "            hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "            ### moe infer\n",
    "            x = hidden_state\n",
    "            topk_ids = topk_idx\n",
    "            cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "            cnts.scatter_(1, topk_ids, 1)\n",
    "            tokens_per_expert = cnts.sum(dim=0)\n",
    "            idxs = topk_ids.view(-1).argsort()\n",
    "            sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "            tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "            outputs = []\n",
    "            start_idx = 0\n",
    "            for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                end_idx = start_idx + num_tokens\n",
    "                if num_tokens == 0:\n",
    "                    continue\n",
    "                expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                expert_out = expert(tokens_for_this_expert)\n",
    "                outputs.append(expert_out)\n",
    "                start_idx = end_idx\n",
    "            outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "            new_x = torch.empty_like(outs)\n",
    "            new_x[idxs] = outs\n",
    "            final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "            ###\n",
    "            y = final_out.view(*orig_shape)\n",
    "            if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "            hidden_state = y\n",
    "\n",
    "            all_topk_experts.append(topk_ids)\n",
    "            all_topk_weights.append(topk_weight)\n",
    "\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Export expert selections\n",
    "\"\"\"\n",
    "def run_and_export_topk(model_id, model_prefix, c4_dl, batches_to_test = 250):\n",
    "    \"\"\"\n",
    "    Run forward passes on a given model ID, return topk df\n",
    "    \"\"\"\n",
    "    b_count = 0\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(c4_dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        \n",
    "        if model_prefix == 'olmoe':\n",
    "            fn = run_olmoe_return_topk\n",
    "        elif model_prefix == 'qwenv15':\n",
    "            fn = run_qwenv15_return_topk\n",
    "        elif model_prefix == 'dsv2':\n",
    "            fn = run_dsv2_return_topk\n",
    "        else:\n",
    "            raise Exception('Unsupported model!')\n",
    "\n",
    "        output = fn(input_ids, attention_mask)\n",
    "\n",
    "        topk_df = convert_topk_to_df(input_ids, output['all_topk_experts'], output['all_topk_weights'])\n",
    "        topk_df =\\\n",
    "            topk_df[topk_df['token_id'] != tokenizer.pad_token_id]\\\n",
    "            .assign(\n",
    "                batch_ix = batch_ix,\n",
    "                weight = lambda df: df['weight'].round(3)\n",
    "            )\n",
    "\n",
    "        topk_df.to_csv(\n",
    "            f'{model_prefix}-c4-routes.csv',\n",
    "            mode = 'w' if batch_ix == 0 else 'a',\n",
    "            index = False,\n",
    "            header = (batch_ix == 0)\n",
    "        )\n",
    "\n",
    "        topk_df[topk_df['topk_ix'] == 1].to_csv(\n",
    "            f'{model_prefix}-c4-routes-top1.csv',\n",
    "            mode = 'w' if batch_ix == 0 else 'a',\n",
    "            index = False,\n",
    "            header = (batch_ix == 0)\n",
    "        )\n",
    "\n",
    "        b_count += 1\n",
    "        if b_count >= batches_to_test:\n",
    "            break\n",
    "\n",
    "    return True\n",
    "\n",
    "export_vocab_as_csv(tokenizer, f'{model_prefix}-vocab.csv')\n",
    "run_and_export_topk(model_id, model_prefix, c4_dl, batches_to_test = 250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
