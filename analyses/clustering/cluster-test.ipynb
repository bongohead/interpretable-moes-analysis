{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from utils.memory import check_memory, profile_memory, clear_all_cuda_memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import plotly\n",
    "import plotly.express as px \n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base model\n",
    "\"\"\"\n",
    "hf_model_id = 'allenai/OLMoE-1B-7B-0125-Instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"allenai/c4\", 'en', split = 'validation', streaming = True).shuffle(seed = 42, buffer_size = 10_000_000)\n",
    "ds_iter = iter(ds)\n",
    "\n",
    "c4_raw = []\n",
    "for _ in range(0, 1_000_000):\n",
    "    sample = next(ds_iter, None)\n",
    "    if sample is None:\n",
    "        break\n",
    "    c4_raw.append(sample['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c4_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer_output):\n",
    "        self.input_ids = tokenizer_output['input_ids']\n",
    "        self.attention_mask = tokenizer_output['attention_mask']\n",
    "        self.tokens = tokenizer.batch_decode(self.input_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'tokens': self.tokens[idx],\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx]\n",
    "        }\n",
    "\n",
    "res = tokenizer(c4_raw, add_special_tokens = False, max_length = 512, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "c4_dl = DataLoader(TextDataset(res), batch_size = 8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(c4_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_model_return_topk(input_ids, attention_mask):\n",
    "    input_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    cache_position = torch.arange(0, input_embeds.shape[1], device = input_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = model.model._update_causal_mask(attention_mask, input_embeds, cache_position, None, None)\n",
    "\n",
    "    hidden_state = input_embeds\n",
    "    position_embeddings = model.model.rotary_emb(hidden_state, position_ids)\n",
    "\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer in model.model.layers:\n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        hidden_state, _, _ = layer.self_attn(hidden_states = hidden_state, attention_mask = causal_mask, position_ids = position_ids, position_embeddings = position_embeddings)\n",
    "        hidden_state = residual + hidden_state\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "        # MoE\n",
    "        ####### OlMoESparseMoeBlock - below code replaces hidden_state = layer.mlp(hidden_state)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_state.shape\n",
    "        moe_hidden_state = hidden_state.view(-1, hidden_dim)\n",
    "        # router_logits: (batch * sequence_length, n_experts)\n",
    "        router_logits = layer.mlp.gate(moe_hidden_state)\n",
    "\n",
    "        routing_weights = torch.nn.functional.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, layer.mlp.top_k, dim=-1, sorted = True)\n",
    "        routing_weights = routing_weights.to(moe_hidden_state.dtype)\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * sequence_length, hidden_dim), dtype=hidden_state.dtype, device=hidden_state.device\n",
    "        )\n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes = layer.mlp.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(layer.mlp.num_experts):\n",
    "            expert_layer = layer.mlp.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            current_state = moe_hidden_state[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(moe_hidden_state.dtype))\n",
    "\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        #######\n",
    "\n",
    "        hidden_state = final_hidden_states\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "        all_topk_experts.append(selected_experts.detach().cpu())\n",
    "        all_topk_weights.append(routing_weights.detach().cpu().to(torch.float32))\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "\n",
    "\n",
    "topk_dfs = []\n",
    "\n",
    "b_count = 0\n",
    "\n",
    "for batch_ix, batch in tqdm(enumerate(c4_dl)):\n",
    "\n",
    "    input_ids = batch['input_ids'].to(main_device)\n",
    "    attention_mask = batch['attention_mask'].to(main_device)\n",
    "    \n",
    "    output = run_model_return_topk(input_ids, attention_mask)\n",
    "\n",
    "    topk_df = convert_topk_to_df(input_ids, output['all_topk_experts'], output['all_topk_weights'])\n",
    "    topk_df =\\\n",
    "        topk_df[topk_df['token_id'] != tokenizer.pad_token_id]\\\n",
    "        .assign(weight = lambda df: df['weight'].round(3))\n",
    "\n",
    "    topk_dfs.append(topk_df)\n",
    "\n",
    "    b_count += 1\n",
    "    if b_count >= 200:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(topk_dfs).to_csv('olmoe_clustering.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vocab import export_vocab_as_csv\n",
    "\n",
    "export_vocab_as_csv(tokenizer, 'olmoe_vocab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = []\n",
    "for _ in tqdm(range(0, 1)):\n",
    "    sample = next(ds_iter)\n",
    "    chunk.append(sample['text'])\n",
    "\n",
    "chunk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
