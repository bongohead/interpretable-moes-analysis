{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from utils.memory import check_memory, profile_memory, clear_all_cuda_memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id = 'deepseek-ai/DeepSeek-V2-Lite'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda()\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "\n",
    "# Hooks needed: https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite/blob/main/modeling_deepseek.py\n",
    "inputs = tokenizer(['Hi this is dog', 'Where is the beef'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "\n",
    "def attach_moe_gate_hooks(model):\n",
    "    \"\"\"\n",
    "    Registers forward-hooks on each MoE gating module in 'model' so that after a forward pass,\n",
    "    we can retrieve the BN x topk 'topk_idx' from each layer.\n",
    "    \n",
    "    Returns:\n",
    "        all_expert_ids: A list that will be filled at runtime withtuples of (layer_index, topk_idx_tensor).\n",
    "        handles: A dictionary of {layer_index: hook_handle}, so you can remove them if desired.\n",
    "    \"\"\"\n",
    "    all_expert_ids = []\n",
    "    handles = {}\n",
    "\n",
    "    def gate_forward_hook(module, input, output):\n",
    "        \"\"\"\n",
    "        This hook is triggered after MoEGate.forward(...).\n",
    "        'output' should be the tuple: (topk_idx, topk_weight, aux_loss).\n",
    "        We only need topk_idx here.\n",
    "        \"\"\"\n",
    "        topk_idx, _, _ = output\n",
    "        all_expert_ids.append(topk_idx.detach())\n",
    "\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        # Layer 0 is not moe\n",
    "        if layer_idx > 0:\n",
    "            # attach an attribute so we know which layer this gating belongs to\n",
    "            layer.mlp.gate._layer_id = layer_idx\n",
    "            hook_handle = layer.mlp.gate.register_forward_hook(gate_forward_hook)\n",
    "            handles[layer_idx] = hook_handle\n",
    "\n",
    "    return all_expert_ids, handles\n",
    "\n",
    "\n",
    "all_expert_ids, hook_handles = attach_moe_gate_hooks(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "for topk_idx_tensor in all_expert_ids:\n",
    "    print(f\"topk_idx shape = {topk_idx_tensor.shape}\")\n",
    "    # e.g. shape is [B*N, top_k]\n",
    "\n",
    "for layer_idx, h in hook_handles.items():\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "\n",
    "# No need for hooks! https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py\n",
    "inputs = tokenizer(['Hi this is dog', 'Where is the beef'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_topk_experts, hook_handles = attach_moe_gate_hooks(model)\n",
    "    outputs = model(**inputs)\n",
    "    for layer_idx, h in hook_handles.items():\n",
    "        h.remove()\n",
    "\n",
    "all_topk_experts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Get MMLU data and domains to test\n",
    "\"\"\"\n",
    "from datasets import load_dataset\n",
    "\n",
    "mmlu_ds = load_dataset(\"cais/mmlu\", 'all', split = 'test')\n",
    "print(mmlu_ds[0])\n",
    "\n",
    "# Only retain domains for high school subjects\n",
    "# domains_to_keep = ['college_biology', 'college_medicine', 'college_computer_science', 'college_mathematics']\n",
    "domains_to_keep = ['elementary_mathematics', 'high_school_computer_science', 'high_school_world_history', 'high_school_psychology', 'high_school_chemistry', 'high_school_biology']\n",
    "domains = [x for x in list(set([x['subject'] for x in mmlu_ds])) if x in domains_to_keep]\n",
    "print(domains)\n",
    "\n",
    "# Now let's put the MMLU questions into a list gruoped by domain\n",
    "all_domain_questions = [\n",
    "    {\n",
    "        'domain': domain,\n",
    "        'questions': \n",
    "            [\n",
    "                {'question': q['question'], 'choices': q['choices'], 'answer_index': q['answer'], 'answer_char': chr(65 + q['answer'])}\n",
    "                for q in mmlu_ds\n",
    "                if q['subject'] == domain \n",
    "            ]\n",
    "    }\n",
    "    for domain in tqdm(domains)\n",
    "]\n",
    "\n",
    "all_domain_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_domains = list(set(mmlu_ds['subject']))\n",
    "# for domain in all_domains:\n",
    "#     dom_length = len([q for q in mmlu_ds if q['subject'] == domain])\n",
    "#     print(f\"{domain}: {dom_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create function to map MMLU data into questions\n",
    "\"\"\"\n",
    "def prep_question(question, choices):\n",
    "    \n",
    "    prompt = f\"Question: {question}\\nChoices:\\n\"\n",
    "    \n",
    "    for i, option in enumerate(choices):\n",
    "        letter = chr(65 + i)\n",
    "        prompt += f\"({letter}) {option}\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# print(prep_question(mmlu_ds[0]['question'], mmlu_ds[0]['choices']))\n",
    "# print(set(mmlu_ds['subject']))\n",
    "\n",
    "mmlu_system_prompt = 'You will be provided with a multiple-choice question, as well as a list of possible answer choices. Respond exactly with: \"The correct answer is {X}\", substituting in X with the code for the correct choice.'\n",
    "fs_ex = [\n",
    "    [q for q in mmlu_ds if q['subject'] == 'anatomy'][0],\n",
    "    [q for q in mmlu_ds if q['subject'] == 'machine_learning'][0],\n",
    "    [q for q in mmlu_ds if q['subject'] == 'astronomy'][0]\n",
    "]\n",
    "\n",
    "base_prompt = [\n",
    "    {'role': 'system', 'content': mmlu_system_prompt},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[0]['question'], fs_ex[0]['choices'])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + fs_ex[0]['answer'])},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[1]['question'], fs_ex[1]['choices'])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + fs_ex[1]['answer'])},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[2]['question'], fs_ex[2]['choices'])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + fs_ex[2]['answer'])}\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(base_prompt, tokenize = False, add_generation_prompt = False, continue_final_message = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save topk for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "\n",
    "cat_results = []\n",
    "\n",
    "for this_domain in all_domain_questions:\n",
    "\n",
    "    domain_questions = this_domain['questions']\n",
    "\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    topk_dfs = []\n",
    "    results = []\n",
    "    for question_ix, q in tqdm(enumerate(domain_questions[0:200])):\n",
    "\n",
    "        input_prompt = tokenizer.apply_chat_template(\n",
    "            base_prompt + [\n",
    "                {'role': 'user', 'content': prep_question(q['question'], q['choices'])},\n",
    "                {'role': 'assistant', 'content': 'The correct answer is'},\n",
    "            ],\n",
    "            tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    "        )\n",
    "        inputs = tokenizer(input_prompt, return_tensors = 'pt').to(main_device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            all_topk_experts, hook_handles = attach_moe_gate_hooks(model)\n",
    "            outputs = model(input_ids = inputs[\"input_ids\"], attention_mask = inputs[\"attention_mask\"])\n",
    "            for layer_idx, h in hook_handles.items():\n",
    "                h.remove()\n",
    "\n",
    "            topk_df = convert_topk_to_df(all_topk_experts, inputs[\"input_ids\"]).assign(domain = this_domain['domain'], question_ix = question_ix).drop(columns = 'sequence_ix')\n",
    "            topk_df = topk_df[topk_df['token_id'] != tokenizer.pad_token_id] # Filter out rows with attention_mask\n",
    "            topk_dfs.append(topk_df)\n",
    "\n",
    "            next_token_logits = outputs['logits'][0, -1, :]\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "            predicted_text = tokenizer.decode([next_token_id]).strip()\n",
    "\n",
    "        predicted_letter = None\n",
    "        for c in predicted_text:\n",
    "            if c.upper() in [chr(65 + i) for i in range(len(q['choices']))]:\n",
    "                predicted_letter = c.upper()\n",
    "                break\n",
    "\n",
    "        result = {\n",
    "            'domain': this_domain['domain'],\n",
    "            'question_ix': question_ix, \n",
    "            'model_output': predicted_text,\n",
    "            'model_choice': predicted_letter,\n",
    "            'correct_choice': q['answer_char'],\n",
    "            'is_correct': 1 if predicted_letter == q['answer_char'] else 0\n",
    "        }\n",
    "\n",
    "        if result['is_correct'] == 1: count_correct += 1\n",
    "        else: count_incorrect += 1\n",
    "\n",
    "        results.append(result)\n",
    "    \n",
    "\n",
    "    cat_results.append({\n",
    "        'answer_df': pd.DataFrame(results),\n",
    "        'topks_df': pd.concat(topk_dfs)\n",
    "    })\n",
    "\n",
    "    print(f'{this_domain[\"domain\"]} | Correct: {str(count_correct)} | Incorrect: {str(count_incorrect)} | Accuracy: {(count_correct / (count_correct + count_incorrect)) * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map =\\\n",
    "    pd.DataFrame([{\"token\": token.replace('Ä ', ' '), \"token_id\": token_id} for token, token_id in tokenizer.get_vocab().items()])\\\n",
    "    .sort_values(by = 'token_id')\\\n",
    "    .reset_index()\n",
    "\n",
    "display(vocab_map)\n",
    "\n",
    "all_answers = pd.concat([cat['answer_df'] for cat in cat_results])\n",
    "display(all_answers)\n",
    "\n",
    "all_topks = pd.concat([cat['topks_df'] for cat in cat_results]).merge(vocab_map, how = 'left', on = 'token_id')\n",
    "display(all_topks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answers.to_csv('all_answers.csv', index = False)\n",
    "all_topks.to_csv('all_topks.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids = inputs[\"input_ids\"], attention_mask = inputs['attention_mask'])\n",
    "    print(outputs['logits'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.model(input_ids = inputs[\"input_ids\"], attention_mask = inputs['attention_mask'])\n",
    "    hidden_state = outputs[0]\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    print(logits)\n",
    "\n",
    "from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n",
    "\n",
    "with torch.no_grad():\n",
    "    B, N = inputs[\"input_ids\"].shape[:2]\n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "\n",
    "    inputs_embeds = model.model.embed_tokens(inputs[\"input_ids\"])\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(inputs['attention_mask'], (B, N), inputs_embeds, 0,)\n",
    "\n",
    "    hidden_state = inputs_embeds\n",
    "    for decoder_layer in model.model.layers:\n",
    "        # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "        residual = hidden_state\n",
    "        hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "        # Self Attention\n",
    "        hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "        hidden_state = residual + hidden_state\n",
    "        # Fully Connected\n",
    "        residual = hidden_state\n",
    "        hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "        ## MLP\n",
    "        if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "            hidden_state = decoder_layer.mlp(hidden_state)\n",
    "        else:\n",
    "            identity = hidden_state\n",
    "            orig_shape = hidden_state.shape\n",
    "            ### moegate - originally topk_idx, topk_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "            bsz, seq_len, h = hidden_state.shape\n",
    "            moe_hidden_state = hidden_state.view(-1, h)\n",
    "            logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), decoder_layer.mlp.gate.weight.type(torch.float32), None)\n",
    "            scores = logits.softmax(dim=-1, dtype=torch.float32)\n",
    "            topk_weight, topk_idx = torch.topk(scores, k=decoder_layer.mlp.gate.top_k, dim=-1, sorted=False)\n",
    "            topk_weight = topk_weight * decoder_layer.mlp.gate.routed_scaling_factor\n",
    "            ####\n",
    "            hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "            flat_topk_idx = topk_idx.view(-1)\n",
    "            ### moe infer\n",
    "            x = hidden_state\n",
    "            topk_ids = topk_idx\n",
    "            cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "            cnts.scatter_(1, topk_ids, 1)\n",
    "            tokens_per_expert = cnts.sum(dim=0)\n",
    "            idxs = topk_ids.view(-1).argsort()\n",
    "            sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "            tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "            outputs = []\n",
    "            start_idx = 0\n",
    "            for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                end_idx = start_idx + num_tokens\n",
    "                if num_tokens == 0:\n",
    "                    continue\n",
    "                expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                expert_out = expert(tokens_for_this_expert)\n",
    "                outputs.append(expert_out)\n",
    "                start_idx = end_idx\n",
    "            outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "            new_x = torch.empty_like(outs)\n",
    "            new_x[idxs] = outs\n",
    "            final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "            ###\n",
    "            y = final_out.view(*orig_shape)\n",
    "            if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "            hidden_state = y\n",
    "\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_no_ablation(input_ids, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        return model(input_ids, attention_mask)['logits']\n",
    "\n",
    "def evaluate_with_ablation(run_forward_fn, *args, **kwargs):\n",
    "    \n",
    "    cat_results = []\n",
    "\n",
    "    for this_domain in all_domain_questions:\n",
    "\n",
    "        domain_questions = this_domain['questions']\n",
    "\n",
    "        count_correct = 0\n",
    "        count_incorrect = 0\n",
    "        results = []\n",
    "        for question_ix, q in tqdm(enumerate(domain_questions[0:200])):\n",
    "\n",
    "            input_prompt = tokenizer.apply_chat_template(\n",
    "                base_prompt + [\n",
    "                    {'role': 'user', 'content': prep_question(q['question'], q['choices'])},\n",
    "                    {'role': 'assistant', 'content': 'The correct answer is'},\n",
    "                ],\n",
    "                tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    "            )\n",
    "            inputs = tokenizer(input_prompt, return_tensors = 'pt').to(main_device)\n",
    "\n",
    "            outputs = run_forward_fn(inputs['input_ids'], inputs['attention_mask'], *args, **kwargs)\n",
    "            next_token_logits = outputs[0, -1, :]\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "            predicted_text = tokenizer.decode([next_token_id]).strip()\n",
    "\n",
    "            predicted_letter = None\n",
    "            for c in predicted_text:\n",
    "                if c.upper() in [chr(65 + i) for i in range(len(q['choices']))]:\n",
    "                    predicted_letter = c.upper()\n",
    "                    break\n",
    "\n",
    "            result = {\n",
    "                'domain': this_domain['domain'],\n",
    "                'question_ix': question_ix, \n",
    "                'model_output': predicted_text,\n",
    "                'model_choice': predicted_letter,\n",
    "                'correct_choice': q['answer_char'],\n",
    "                'is_correct': 1 if predicted_letter == q['answer_char'] else 0\n",
    "            }\n",
    "\n",
    "            if result['is_correct'] == 1: count_correct += 1\n",
    "            else: count_incorrect += 1\n",
    "\n",
    "            results.append(result)\n",
    "        \n",
    "\n",
    "        cat_results.append({\n",
    "            'answer_df': pd.DataFrame(results)\n",
    "        })\n",
    "\n",
    "        print(f'{this_domain[\"domain\"]} | Correct: {str(count_correct)} | Incorrect: {str(count_incorrect)} | Accuracy: {(count_correct / (count_correct + count_incorrect)) * 100:.1f}%')\n",
    "\n",
    "evaluate_with_ablation(run_model_no_ablation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation 1 - ablate top1 expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_ablate_top1(input_ids, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        B, N = input_ids.shape[:2]\n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "\n",
    "        inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "        hidden_state = inputs_embeds\n",
    "        for layer_ix, decoder_layer in enumerate(model.model.layers):\n",
    "            # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "            # Self Attention\n",
    "            hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "            hidden_state = residual + hidden_state\n",
    "            # Fully Connected\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "            ## MLP\n",
    "            if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "                hidden_state = decoder_layer.mlp(hidden_state)\n",
    "            else:\n",
    "                identity = hidden_state\n",
    "                orig_shape = hidden_state.shape\n",
    "                ### moegate - originally topk_idx, topk_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "                bsz, seq_len, h = hidden_state.shape\n",
    "                moe_hidden_state = hidden_state.view(-1, h)\n",
    "                logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), decoder_layer.mlp.gate.weight.type(torch.float32), None)\n",
    "                scores = logits.softmax(dim=-1, dtype=torch.float32)\n",
    "                topk_weight, topk_idx = torch.topk(scores, k=decoder_layer.mlp.gate.top_k, dim=-1, sorted=False)\n",
    "                topk_weight = topk_weight * decoder_layer.mlp.gate.routed_scaling_factor\n",
    "                ####\n",
    "                ######################## ABLATE TOP-1\n",
    "                # For each token, find the single highest weight in topk_weight, set it to zero.\n",
    "                # shape: topk_weight is [B*N, top_k]\n",
    "                # if layer_ix in list(range(0, 27)):\n",
    "                if layer_ix in list(range(1, 4)):\n",
    "                    row_max_indices = topk_weight.argmax(dim=-1)  # which of the top_k is max\n",
    "                    for row_i in range(topk_weight.shape[0]):\n",
    "                        topk_weight[row_i, row_max_indices[row_i]] = 0.0\n",
    "                ######################## The rest is unchanged\n",
    "                hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "                flat_topk_idx = topk_idx.view(-1)\n",
    "                ### moe infer\n",
    "                x = hidden_state\n",
    "                topk_ids = topk_idx\n",
    "                cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "                cnts.scatter_(1, topk_ids, 1)\n",
    "                tokens_per_expert = cnts.sum(dim=0)\n",
    "                idxs = topk_ids.view(-1).argsort()\n",
    "                sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "                tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "                outputs = []\n",
    "                start_idx = 0\n",
    "                for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                    end_idx = start_idx + num_tokens\n",
    "                    if num_tokens == 0:\n",
    "                        continue\n",
    "                    expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                    tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                    expert_out = expert(tokens_for_this_expert)\n",
    "                    outputs.append(expert_out)\n",
    "                    start_idx = end_idx\n",
    "                outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "                new_x = torch.empty_like(outs)\n",
    "                new_x[idxs] = outs\n",
    "                final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "                ###\n",
    "                y = final_out.view(*orig_shape)\n",
    "                if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                    y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "                hidden_state = y\n",
    "\n",
    "            hidden_state = residual + hidden_state\n",
    "\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "        logits = model.lm_head(hidden_state)\n",
    "        return logits\n",
    "    \n",
    "evaluate_with_ablation(run_model_ablate_top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation 2 - ablate top1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_ablate_top1_renorm(input_ids, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        B, N = input_ids.shape[:2]\n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "\n",
    "        inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "        hidden_state = inputs_embeds\n",
    "        for decoder_layer in model.model.layers:\n",
    "            # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "            # Self Attention\n",
    "            hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "            hidden_state = residual + hidden_state\n",
    "            # Fully Connected\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "            ## MLP\n",
    "            if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "                hidden_state = decoder_layer.mlp(hidden_state)\n",
    "            else:\n",
    "                identity = hidden_state\n",
    "                orig_shape = hidden_state.shape\n",
    "                ### moegate - originally topk_idx, topk_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "                bsz, seq_len, h = hidden_state.shape\n",
    "                moe_hidden_state = hidden_state.view(-1, h)\n",
    "                logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), decoder_layer.mlp.gate.weight.type(torch.float32), None)\n",
    "                scores = logits.softmax(dim=-1, dtype=torch.float32)\n",
    "                topk_weight, topk_idx = torch.topk(scores, k=decoder_layer.mlp.gate.top_k, dim=-1, sorted=False)\n",
    "                topk_weight = topk_weight * decoder_layer.mlp.gate.routed_scaling_factor\n",
    "                ####\n",
    "                ######################## ABLATE TOP-1\n",
    "                # For each token, find the single highest weight in topk_weight, set it to zero.\n",
    "                # shape: topk_weight is [B*N, top_k]\n",
    "                row_sum_before = topk_weight.sum(dim=-1, keepdim=True)  # shape [B*N, 1]\n",
    "                row_max_indices = topk_weight.argmax(dim=-1)\n",
    "                for row_i in range(topk_weight.shape[0]):\n",
    "                    topk_weight[row_i, row_max_indices[row_i]] = 0.0\n",
    "                row_sum_after = topk_weight.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                # Avoid dividing by zero\n",
    "                scale_factor = row_sum_before / (row_sum_after + 1e-9)\n",
    "                topk_weight = topk_weight * scale_factor\n",
    "                ######################## The rest is unchanged\n",
    "                hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "                flat_topk_idx = topk_idx.view(-1)\n",
    "                ### moe infer\n",
    "                x = hidden_state\n",
    "                topk_ids = topk_idx\n",
    "                cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "                cnts.scatter_(1, topk_ids, 1)\n",
    "                tokens_per_expert = cnts.sum(dim=0)\n",
    "                idxs = topk_ids.view(-1).argsort()\n",
    "                sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "                tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "                outputs = []\n",
    "                start_idx = 0\n",
    "                for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                    end_idx = start_idx + num_tokens\n",
    "                    if num_tokens == 0:\n",
    "                        continue\n",
    "                    expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                    tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                    expert_out = expert(tokens_for_this_expert)\n",
    "                    outputs.append(expert_out)\n",
    "                    start_idx = end_idx\n",
    "                outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "                new_x = torch.empty_like(outs)\n",
    "                new_x[idxs] = outs\n",
    "                final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "                ###\n",
    "                y = final_out.view(*orig_shape)\n",
    "                if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                    y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "                hidden_state = y\n",
    "\n",
    "            hidden_state = residual + hidden_state\n",
    "\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "        logits = model.lm_head(hidden_state)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "evaluate_with_ablation(run_model_ablate_top1_renorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation 3 - ablate everything other than topk=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_ablate_only_top1_renorm(input_ids, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        B, N = input_ids.shape[:2]\n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "\n",
    "        inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "        hidden_state = inputs_embeds\n",
    "        for decoder_layer in model.model.layers:\n",
    "            # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "            # Self Attention\n",
    "            hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "            hidden_state = residual + hidden_state\n",
    "            # Fully Connected\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "            ## MLP\n",
    "            if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "                hidden_state = decoder_layer.mlp(hidden_state)\n",
    "            else:\n",
    "                identity = hidden_state\n",
    "                orig_shape = hidden_state.shape\n",
    "                ### moegate - originally topk_idx, topk_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "                bsz, seq_len, h = hidden_state.shape\n",
    "                moe_hidden_state = hidden_state.view(-1, h)\n",
    "                logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), decoder_layer.mlp.gate.weight.type(torch.float32), None)\n",
    "                scores = logits.softmax(dim=-1, dtype=torch.float32)\n",
    "                topk_weight, topk_idx = torch.topk(scores, k=decoder_layer.mlp.gate.top_k, dim=-1, sorted=False)\n",
    "                topk_weight = topk_weight * decoder_layer.mlp.gate.routed_scaling_factor\n",
    "                ####\n",
    "                ######################## ABLATE ALL EXCEPT TOP-1\n",
    "                row_sum_before = topk_weight.sum(dim=-1, keepdim=True)  # sum of all k\n",
    "                row_max_indices = topk_weight.argmax(dim=-1)            # which column is top1\n",
    "\n",
    "                # We'll keep only that top1 column, zero out others\n",
    "                # shape: topk_weight is [B*N, top_k]\n",
    "                temp = topk_weight.clone()\n",
    "                topk_weight.zero_()\n",
    "                # For each row, restore only the largest weight\n",
    "                for row_i in range(topk_weight.shape[0]):\n",
    "                    col = row_max_indices[row_i]\n",
    "                    topk_weight[row_i, col] = temp[row_i, col]\n",
    "\n",
    "                # Now renormalize so the sum is unchanged\n",
    "                # row_sum_after = topk_weight.sum(dim=-1, keepdim=True)\n",
    "                # scale_factor = row_sum_before / (row_sum_after + 1e-9)\n",
    "                # topk_weight = topk_weight * scale_factor\n",
    "                ######################## The rest is unchanged\n",
    "                hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "                flat_topk_idx = topk_idx.view(-1)\n",
    "                ### moe infer\n",
    "                x = hidden_state\n",
    "                topk_ids = topk_idx\n",
    "                cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "                cnts.scatter_(1, topk_ids, 1)\n",
    "                tokens_per_expert = cnts.sum(dim=0)\n",
    "                idxs = topk_ids.view(-1).argsort()\n",
    "                sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "                tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "                outputs = []\n",
    "                start_idx = 0\n",
    "                for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                    end_idx = start_idx + num_tokens\n",
    "                    if num_tokens == 0:\n",
    "                        continue\n",
    "                    expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                    tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                    expert_out = expert(tokens_for_this_expert)\n",
    "                    outputs.append(expert_out)\n",
    "                    start_idx = end_idx\n",
    "                outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "                new_x = torch.empty_like(outs)\n",
    "                new_x[idxs] = outs\n",
    "                final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "                ###\n",
    "                y = final_out.view(*orig_shape)\n",
    "                if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                    y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "                hidden_state = y\n",
    "\n",
    "            hidden_state = residual + hidden_state\n",
    "\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "        logits = model.lm_head(hidden_state)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "evaluate_with_ablation(run_model_ablate_only_top1_renorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablate top1, last expert only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check topk expert distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First, let's check the distributions of the topks so we can see why topk=1 matters so much\n",
    "\"\"\"\n",
    "def run_model_return_topk_weights(input_ids, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        B, N = input_ids.shape[:2]\n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "\n",
    "        inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "        hidden_state = inputs_embeds\n",
    "        topk_weights = []\n",
    "        for decoder_layer in model.model.layers:\n",
    "            # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "            # Self Attention\n",
    "            hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "            hidden_state = residual + hidden_state\n",
    "            # Fully Connected\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "            ## MLP\n",
    "            if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "                hidden_state = decoder_layer.mlp(hidden_state)\n",
    "            else:\n",
    "                identity = hidden_state\n",
    "                orig_shape = hidden_state.shape\n",
    "                ### moegate - originally topk_idx, topk_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "                bsz, seq_len, h = hidden_state.shape\n",
    "                moe_hidden_state = hidden_state.view(-1, h)\n",
    "                logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), decoder_layer.mlp.gate.weight.type(torch.float32), None)\n",
    "                scores = logits.softmax(dim=-1, dtype=torch.float32)\n",
    "                topk_weight, topk_idx = torch.topk(scores, k=decoder_layer.mlp.gate.top_k, dim=-1, sorted=True)\n",
    "                topk_weight = topk_weight * decoder_layer.mlp.gate.routed_scaling_factor\n",
    "                hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "                flat_topk_idx = topk_idx.view(-1)\n",
    "                ### moe infer\n",
    "                x = hidden_state\n",
    "                topk_ids = topk_idx\n",
    "                cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "                cnts.scatter_(1, topk_ids, 1)\n",
    "                tokens_per_expert = cnts.sum(dim=0)\n",
    "                idxs = topk_ids.view(-1).argsort()\n",
    "                sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "                tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "                outputs = []\n",
    "                start_idx = 0\n",
    "                for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                    end_idx = start_idx + num_tokens\n",
    "                    if num_tokens == 0:\n",
    "                        continue\n",
    "                    expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                    tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                    expert_out = expert(tokens_for_this_expert)\n",
    "                    outputs.append(expert_out)\n",
    "                    start_idx = end_idx\n",
    "                outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "                new_x = torch.empty_like(outs)\n",
    "                new_x[idxs] = outs\n",
    "                final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "                ###\n",
    "                y = final_out.view(*orig_shape)\n",
    "                if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                    y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "                hidden_state = y\n",
    "\n",
    "                topk_weights.append(topk_weight)\n",
    "\n",
    "            hidden_state = residual + hidden_state\n",
    "\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "        logits = model.lm_head(hidden_state)\n",
    "        return logits, topk_weights # topk_weights is BN x k\n",
    "\n",
    "def get_topk_weights():\n",
    "    \n",
    "    cat_results = []\n",
    "\n",
    "    for this_domain in all_domain_questions:\n",
    "\n",
    "        domain_questions = this_domain['questions']\n",
    "\n",
    "        count_correct = 0\n",
    "        count_incorrect = 0\n",
    "        results = []\n",
    "        topk_weight_dfs = []\n",
    "        for question_ix, q in tqdm(enumerate(domain_questions[0:200])):\n",
    "\n",
    "            input_prompt = tokenizer.apply_chat_template(\n",
    "                base_prompt + [\n",
    "                    {'role': 'user', 'content': prep_question(q['question'], q['choices'])},\n",
    "                    {'role': 'assistant', 'content': 'The correct answer is'},\n",
    "                ],\n",
    "                tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    "            )\n",
    "            inputs = tokenizer(input_prompt, return_tensors = 'pt').to(main_device)\n",
    "\n",
    "            logits, topk_weights = run_model_return_topk_weights(inputs['input_ids'], inputs['attention_mask'])\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "            predicted_text = tokenizer.decode([next_token_id]).strip()\n",
    "\n",
    "            predicted_letter = None\n",
    "            for c in predicted_text:\n",
    "                if c.upper() in [chr(65 + i) for i in range(len(q['choices']))]:\n",
    "                    predicted_letter = c.upper()\n",
    "                    break\n",
    "\n",
    "            result = {\n",
    "                'domain': this_domain['domain'],\n",
    "                'question_ix': question_ix, \n",
    "                'model_output': predicted_text,\n",
    "                'model_choice': predicted_letter,\n",
    "                'correct_choice': q['answer_char'],\n",
    "                'is_correct': 1 if predicted_letter == q['answer_char'] else 0\n",
    "            }\n",
    "\n",
    "            if result['is_correct'] == 1: count_correct += 1\n",
    "            else: count_incorrect += 1\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            ### Cleanup topk weights and return df ###\n",
    "            np_probs = (torch.stack(topk_weights)/torch.stack(topk_weights).sum(dim = -1, keepdim = True)).cpu().numpy()\n",
    "            flat_data = []\n",
    "            dim_names = ['layer_ix', 'token_ix', 'topk_ix']\n",
    "            dim_values = [list(range(s)) for s in np_probs.shape]\n",
    "            for i in range(np_probs.shape[0]):\n",
    "                for j in range(np_probs.shape[1]):\n",
    "                    for k in range(np_probs.shape[2]):\n",
    "                        flat_data.append({\n",
    "                            dim_names[0]: dim_values[0][i],\n",
    "                            dim_names[1]: dim_values[1][j],\n",
    "                            dim_names[2]: dim_values[2][k],\n",
    "                            'prob': np_probs[i, j, k]\n",
    "                        })\n",
    "\n",
    "            layer_x_token_x_topk = pd.DataFrame(flat_data).assign(domain = this_domain['domain'], question_ix = question_ix)\n",
    "            topk_weight_dfs.append(layer_x_token_x_topk)\n",
    "\n",
    "        cat_results.append({\n",
    "            'answer_df': pd.DataFrame(results),\n",
    "            'topk_weights': pd.concat(topk_weight_dfs)\n",
    "        })\n",
    "\n",
    "        print(f'{this_domain[\"domain\"]} | Correct: {str(count_correct)} | Incorrect: {str(count_incorrect)} | Accuracy: {(count_correct / (count_correct + count_incorrect)) * 100:.1f}%')\n",
    "    return cat_results\n",
    "\n",
    "topk_res = get_topk_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_x_token_x_topk = pd.concat([x['topk_weights'] for x in topk_res])\n",
    "layer_x_token_x_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_x_topk = layer_x_token_x_topk[layer_x_token_x_topk['layer_ix'] == 0].sample(n = 100)\n",
    "token_x_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "token_x_topk = layer_x_token_x_topk[layer_x_token_x_topk['layer_ix'] == 0].sample(n = 10000)\n",
    "display(token_x_topk)\n",
    "\n",
    "fig_violin = px.violin(\n",
    "    token_x_topk,\n",
    "    x=\"topk_ix\",\n",
    "    y=\"prob\",\n",
    "    color=\"topk_ix\",\n",
    "    box=True,  # Include box plot inside violin\n",
    "    points=\"all\",  # Show all points\n",
    "    title=\"Probability Distribution by Top-K Index\"\n",
    ")\n",
    "\n",
    "fig_violin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Get unique topk_ix values\n",
    "topk_indices = sorted(token_x_topk[\"topk_ix\"].unique())\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Generate a nice color palette\n",
    "colors = px.colors.qualitative.Plotly[:len(topk_indices)]\n",
    "\n",
    "# Add a KDE plot for each topk_ix\n",
    "for i, topk_ix in enumerate(topk_indices):\n",
    "    # Filter data for this topk_ix\n",
    "    subset = token_x_topk[token_x_topk[\"topk_ix\"] == topk_ix]\n",
    "    \n",
    "    # Get probability values\n",
    "    values = subset[\"prob\"].values\n",
    "    \n",
    "    # Calculate KDE using scipy\n",
    "    if len(values) > 1:  # Need at least 2 points for KDE\n",
    "        bandwidth = 0.1 * stats.gaussian_kde(values).factor  # Lower factor = less smooth\n",
    "        kde = stats.gaussian_kde(values, bw_method=bandwidth)\n",
    "        \n",
    "        kde_x = np.linspace(min(values), max(values), 1000)\n",
    "        kde_y = kde(kde_x)\n",
    "                \n",
    "        # Add line\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=kde_x,\n",
    "            y=kde_y,\n",
    "            mode='lines',\n",
    "            name=f'Top-K {topk_ix}',\n",
    "            line=dict(width=2, color=colors[i]),\n",
    "            fill='tozeroy',  # Fill to the x-axis\n",
    "            fillcolor=f'rgba({int(int(colors[i][1:3], 16))}, {int(int(colors[i][3:5], 16))}, {int(int(colors[i][5:7], 16))}, 0.3)'  # Translucent fill\n",
    "        ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Probability Density Distribution by Top-K Index\",\n",
    "    xaxis_title=\"Probability\",\n",
    "    yaxis_title=\"Density\",\n",
    "    legend_title=\"Top-K Index\",\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation 4 - ablate lower topks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation 5 -- ablate lower topks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_weight[0, :].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_probs = (torch.stack(topk_weights)/torch.stack(topk_weights).sum(dim = -1, keepdim = True)).cpu().numpy()\n",
    "\n",
    "flat_data = []\n",
    "dim_names = ['layer_ix', 'token_ix', 'topk_ix']\n",
    "dim_values = [list(range(s)) for s in np_probs.shape]\n",
    "for i in range(np_probs.shape[0]):\n",
    "    for j in range(np_probs.shape[1]):\n",
    "        for k in range(np_probs.shape[2]):\n",
    "            flat_data.append({\n",
    "                dim_names[0]: dim_values[0][i],\n",
    "                dim_names[1]: dim_values[1][j],\n",
    "                dim_names[2]: dim_values[2][k],\n",
    "                'prob': np_probs[i, j, k]\n",
    "            })\n",
    "\n",
    "layer_x_token_x_topk = pd.DataFrame(flat_data)\n",
    "token_x_topk = layer_x_token_x_topk[layer_x_token_x_topk['layer_ix'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: Violin plot\n",
    "fig_violin = px.violin(\n",
    "    token_x_topk,\n",
    "    x=\"topk_ix\",\n",
    "    y=\"prob\",\n",
    "    color=\"topk_ix\",\n",
    "    box=True,  # Include box plot inside violin\n",
    "    points=\"all\",  # Show all points\n",
    "    title=\"Probability Distribution by Top-K Index\"\n",
    ")\n",
    "\n",
    "fig_violin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_weights[-1]/topk_weights[-1].sum(dim = -1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_weight/topk_weight.sum(dim = -1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n",
    "\n",
    "def run_model_with_ablation(input_ids, attention_mask, experts_to_ablate: list[int]):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        @input_ids\n",
    "        @attention_mask\n",
    "        @experts_to_ablate: A list equal to the length of MoE layers, which each element containing a single expert ID to ablate per layer\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        B, N = input_ids.shape[:2]\n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device = main_device)\n",
    "        position_ids = position_ids.unsqueeze(0)\n",
    "\n",
    "        inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "        hidden_state = inputs_embeds\n",
    "        moe_layer_ix = 0\n",
    "        for decoder_layer in model.model.layers:\n",
    "            # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "            # Self Attention\n",
    "            hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "            hidden_state = residual + hidden_state\n",
    "            # Fully Connected\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "            ## MLP\n",
    "            if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "                hidden_state = decoder_layer.mlp(hidden_state)\n",
    "            else:\n",
    "                expert_to_ablate = experts_to_ablate[moe_layer_ix]\n",
    "\n",
    "                identity = hidden_state\n",
    "                orig_shape = hidden_state.shape\n",
    "                ### moegate - originally topk_idx, topk_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "                bsz, seq_len, h = hidden_state.shape\n",
    "                moe_hidden_state = hidden_state.view(-1, h)\n",
    "                logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), decoder_layer.mlp.gate.weight.type(torch.float32), None)\n",
    "                scores = logits.softmax(dim=-1, dtype=torch.float32)\n",
    "                topk_weight, topk_idx = torch.topk(scores, k=decoder_layer.mlp.gate.top_k, dim=-1, sorted=False)\n",
    "                topk_weight = topk_weight * decoder_layer.mlp.gate.routed_scaling_factor\n",
    "                ####\n",
    "                hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "                flat_topk_idx = topk_idx.view(-1)\n",
    "                ### moe infer\n",
    "                x = hidden_state\n",
    "                topk_ids = topk_idx\n",
    "                cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "                cnts.scatter_(1, topk_ids, 1)\n",
    "                tokens_per_expert = cnts.sum(dim=0)\n",
    "                idxs = topk_ids.view(-1).argsort()\n",
    "                sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "                tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "                outputs = []\n",
    "                start_idx = 0\n",
    "                for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                    end_idx = start_idx + num_tokens\n",
    "                    if num_tokens == 0:\n",
    "                        continue\n",
    "                    expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                    tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                    expert_out = expert(tokens_for_this_expert)\n",
    "                    \n",
    "                    # Figure out which \"global\" expert index this corresponds to\n",
    "                    # (some code uses 'ep_rank' & 'experts_per_rank' to place experts across ranks)\n",
    "                    expert_idx = i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank\n",
    "                    tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "\n",
    "                    if expert_idx == expert_to_ablate:\n",
    "                        # -------------------------\n",
    "                        # ABLATE THIS EXPERT\n",
    "                        # -------------------------\n",
    "                        expert_out = torch.zeros_like(tokens_for_this_expert)\n",
    "                    else:\n",
    "                        # Run the normal expert forward\n",
    "                        expert = decoder_layer.mlp.experts[expert_idx]\n",
    "                        expert_out = expert(tokens_for_this_expert)\n",
    "                    \n",
    "                    outputs.append(expert_out)\n",
    "                    start_idx = end_idx\n",
    "                outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "                new_x = torch.empty_like(outs)\n",
    "                new_x[idxs] = outs\n",
    "                final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "                ###\n",
    "                y = final_out.view(*orig_shape)\n",
    "                if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                    y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "                hidden_state = y\n",
    "\n",
    "                moe_layer_ix += 1\n",
    "\n",
    "            hidden_state = residual + hidden_state\n",
    "\n",
    "\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "        logits = model.lm_head(hidden_state)\n",
    "        return logits\n",
    "    \n",
    "run_model_with_ablation(inputs[\"input_ids\"], inputs['attention_mask'], experts_to_ablate = [7] * 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.config.norm_topk_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_ablation(run_forward_fn, *args, **kwargs):\n",
    "    \n",
    "    cat_results = []\n",
    "\n",
    "    for this_domain in all_domain_questions:\n",
    "\n",
    "        domain_questions = this_domain['questions']\n",
    "\n",
    "        count_correct = 0\n",
    "        count_incorrect = 0\n",
    "        results = []\n",
    "        for question_ix, q in tqdm(enumerate(domain_questions[3:203])):\n",
    "\n",
    "            input_prompt = tokenizer.apply_chat_template(\n",
    "                base_prompt + [\n",
    "                    {'role': 'user', 'content': prep_question(q['question'], q['choices'])},\n",
    "                    {'role': 'assistant', 'content': 'The correct answer is'},\n",
    "                ],\n",
    "                tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    "            )\n",
    "            inputs = tokenizer(input_prompt, return_tensors = 'pt').to(main_device)\n",
    "\n",
    "            outputs = run_forward_fn(*args, **kwargs)\n",
    "            next_token_logits = outputs[0, -1, :]\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "            predicted_text = tokenizer.decode([next_token_id]).strip()\n",
    "\n",
    "            predicted_letter = None\n",
    "            for c in predicted_text:\n",
    "                if c.upper() in [chr(65 + i) for i in range(len(q['choices']))]:\n",
    "                    predicted_letter = c.upper()\n",
    "                    break\n",
    "\n",
    "            result = {\n",
    "                'domain': this_domain['domain'],\n",
    "                'question_ix': question_ix, \n",
    "                'model_output': predicted_text,\n",
    "                'model_choice': predicted_letter,\n",
    "                'correct_choice': q['answer_char'],\n",
    "                'is_correct': 1 if predicted_letter == q['answer_char'] else 0\n",
    "            }\n",
    "\n",
    "            if result['is_correct'] == 1: count_correct += 1\n",
    "            else: count_incorrect += 1\n",
    "\n",
    "            results.append(result)\n",
    "        \n",
    "\n",
    "        cat_results.append({\n",
    "            'answer_df': pd.DataFrame(results)\n",
    "        })\n",
    "\n",
    "        print(f'{this_domain[\"domain\"]} | Correct: {str(count_correct)} | Incorrect: {str(count_incorrect)} | Accuracy: {(count_correct / (count_correct + count_incorrect)) * 100:.1f}%')\n",
    "\n",
    "evaluate_with_ablation(run_model_with_ablation, inputs[\"input_ids\"], inputs['attention_mask'], experts_to_ablate = [7] * 26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation (method 2 - ablate top1 expert by zeroing it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with_top1_zero(input_ids, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        B, N = input_ids.shape[:2]\n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device = main_device)\n",
    "        position_ids = position_ids.unsqueeze(0)\n",
    "\n",
    "        inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "        hidden_state = inputs_embeds\n",
    "        for decoder_layer in model.model.layers:\n",
    "            # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "            # Self Attention\n",
    "            hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "            hidden_state = residual + hidden_state\n",
    "            # Fully Connected\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "            ## MLP\n",
    "            if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "                hidden_state = decoder_layer.mlp(hidden_state)\n",
    "            else:\n",
    "                identity = hidden_state\n",
    "                orig_shape = hidden_state.shape\n",
    "                topk_idx, topk_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "                #####################################\n",
    "                # 1) Zero out the top-1 slot for each token/shape of topk_weight: (BN, k)/set topk_weight[:, 0] = 0 for all tokens\n",
    "                topk_weight[:, 0] = 0\n",
    "                #####################################\n",
    "                hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "                flat_topk_idx = topk_idx.view(-1)\n",
    "                ### moe infer\n",
    "                x = hidden_state\n",
    "                topk_ids = topk_idx\n",
    "                cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "                cnts.scatter_(1, topk_ids, 1)\n",
    "                tokens_per_expert = cnts.sum(dim=0)\n",
    "                idxs = topk_ids.view(-1).argsort()\n",
    "                sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "                sorted_tokens_shape = sorted_tokens.shape\n",
    "                tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "                outputs = []\n",
    "                start_idx = 0\n",
    "                for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                    end_idx = start_idx + num_tokens\n",
    "                    if num_tokens == 0:\n",
    "                        continue\n",
    "                    expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                    tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                    expert_out = expert(tokens_for_this_expert)\n",
    "                    outputs.append(expert_out)\n",
    "                    start_idx = end_idx\n",
    "                outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "                new_x = torch.empty_like(outs)\n",
    "                new_x[idxs] = outs\n",
    "                final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "                ###\n",
    "                y = final_out.view(*orig_shape)\n",
    "                if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                    y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "                hidden_state = y\n",
    "\n",
    "            hidden_state = residual + hidden_state\n",
    "\n",
    "\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "        logits = model.lm_head(hidden_state)\n",
    "        return logits\n",
    "\n",
    "evaluate_with_ablation(run_model_with_top1_zero, inputs[\"input_ids\"], inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation 3: ablate top1, move in topk+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_with_top1_shift(input_ids, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        B, N = input_ids.shape[:2]\n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device = main_device)\n",
    "        position_ids = position_ids.unsqueeze(0)\n",
    "\n",
    "        inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "        hidden_state = inputs_embeds\n",
    "        for decoder_layer in model.model.layers:\n",
    "            # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "            # Self Attention\n",
    "            hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "            hidden_state = residual + hidden_state\n",
    "            # Fully Connected\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "            ## MLP\n",
    "            if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "                hidden_state = decoder_layer.mlp(hidden_state)\n",
    "            else:\n",
    "                identity = hidden_state\n",
    "                orig_shape = hidden_state.shape\n",
    "                #####################################\n",
    "                k = decoder_layer.mlp.num_experts_per_tok \n",
    "                decoder_layer.mlp.gate.top_k = k + 1 # set new topk\n",
    "                topkplus1_idx, topkplus1_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "                new_topk_idx = topkplus1_idx[:, 1:]    # shape (BN, k)\n",
    "                new_topk_weight = topkplus1_weight[:, 1:]  # shape (BN, k) \n",
    "                #####################################\n",
    "                hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "                ### moe infer\n",
    "                x = hidden_state\n",
    "                topk_ids = new_topk_idx\n",
    "                print(topk_ids.shape)\n",
    "\n",
    "                cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "                cnts.scatter_(1, topk_ids, 1)\n",
    "                tokens_per_expert = cnts.sum(dim=0)\n",
    "                idxs = topk_ids.view(-1).argsort()\n",
    "                sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "                sorted_tokens_shape = sorted_tokens.shape\n",
    "                tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "                outputs = []\n",
    "                start_idx = 0\n",
    "                for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                    end_idx = start_idx + num_tokens\n",
    "                    if num_tokens == 0:\n",
    "                        continue\n",
    "                    expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                    tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                    expert_out = expert(tokens_for_this_expert)\n",
    "                    outputs.append(expert_out)\n",
    "                    start_idx = end_idx\n",
    "                outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "                new_x = torch.empty_like(outs)\n",
    "                new_x[idxs] = outs\n",
    "                final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(new_topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "                ###\n",
    "                y = final_out.view(*orig_shape)\n",
    "                if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                    y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "                hidden_state = y\n",
    "\n",
    "            hidden_state = residual + hidden_state\n",
    "\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "        logits = model.lm_head(hidden_state)\n",
    "        return logits\n",
    "\n",
    "evaluate_with_ablation(run_model_with_top1_shift, inputs[\"input_ids\"], inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer.mlp.num_experts_per_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation study (naive method, all layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "res =\\\n",
    "    all_topks\\\n",
    "    .assign(is_test_domain = lambda df: np.where(df['domain'] == 'high_school_biology', 1, 0))\\\n",
    "    .groupby(['is_test_domain', 'layer_ix', 'expert_1'])\\\n",
    "    .agg(count = ('token_ix', 'count'))\\\n",
    "    .reset_index()\\\n",
    "    .pivot(index = ['layer_ix', 'expert_1'], columns='is_test_domain', values = 'count')\\\n",
    "    .rename(columns = {0: 'other_domain_count', 1: 'test_domain_count'})\\\n",
    "    .fillna(0)\\\n",
    "    .pipe(lambda df: df.set_axis(df.columns, axis = 1))\\\n",
    "    .reset_index()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids = inputs[\"input_ids\"], attention_mask = inputs['attention_mask'])\n",
    "    print(outputs['logits'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.model(input_ids = inputs[\"input_ids\"], attention_mask = inputs['attention_mask'])\n",
    "    hidden_state = outputs[0]\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n",
    "\n",
    "def run_model_with_ablation(input_ids, attention_mask, experts_to_ablate: list[int]):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        @input_ids\n",
    "        @attention_mask\n",
    "        @experts_to_ablate: A list equal to the length of MoE layers, which each element containing a single expert ID to ablate per layer\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        B, N = input_ids.shape[:2]\n",
    "        position_ids = torch.arange(0, N, dtype=torch.long, device = main_device)\n",
    "        position_ids = position_ids.unsqueeze(0)\n",
    "\n",
    "        inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "        hidden_state = inputs_embeds\n",
    "        moe_layer_ix = 0\n",
    "        for decoder_layer in model.model.layers:\n",
    "            # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "            residual = hidden_state\n",
    "\n",
    "            hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "\n",
    "            # Self Attention\n",
    "            hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(\n",
    "                hidden_states = hidden_state,\n",
    "                attention_mask = attention_mask,\n",
    "                position_ids = position_ids\n",
    "            )\n",
    "            hidden_state = residual + hidden_state\n",
    "\n",
    "            # Fully Connected\n",
    "            residual = hidden_state\n",
    "            hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "            ## MLP\n",
    "            if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "                hidden_state = decoder_layer.mlp(hidden_state)\n",
    "            else:\n",
    "                expert_to_ablate = experts_to_ablate[moe_layer_ix]\n",
    "\n",
    "                identity = hidden_state\n",
    "                orig_shape = hidden_state.shape\n",
    "                topk_idx, topk_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "                hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "                flat_topk_idx = topk_idx.view(-1)\n",
    "                ### moe infer\n",
    "                x = hidden_state\n",
    "                topk_ids = topk_idx\n",
    "                cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "                cnts.scatter_(1, topk_ids, 1)\n",
    "                tokens_per_expert = cnts.sum(dim=0)\n",
    "                idxs = topk_ids.view(-1).argsort()\n",
    "                sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "                sorted_tokens_shape = sorted_tokens.shape\n",
    "                tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "                outputs = []\n",
    "                start_idx = 0\n",
    "                for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                    end_idx = start_idx + num_tokens\n",
    "                    if num_tokens == 0:\n",
    "                        continue\n",
    "                    expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                    tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                    expert_out = expert(tokens_for_this_expert)\n",
    "                    \n",
    "                    # Figure out which \"global\" expert index this corresponds to\n",
    "                    # (some code uses 'ep_rank' & 'experts_per_rank' to place experts across ranks)\n",
    "                    expert_idx = i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank\n",
    "                    tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "\n",
    "                    if expert_idx == expert_to_ablate:\n",
    "                        # -------------------------\n",
    "                        # ABLATE THIS EXPERT\n",
    "                        # -------------------------\n",
    "                        expert_out = torch.zeros_like(tokens_for_this_expert)\n",
    "                    else:\n",
    "                        # Run the normal expert forward\n",
    "                        expert = decoder_layer.mlp.experts[expert_idx]\n",
    "                        expert_out = expert(tokens_for_this_expert)\n",
    "                    \n",
    "                    outputs.append(expert_out)\n",
    "                    start_idx = end_idx\n",
    "                outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "                new_x = torch.empty_like(outs)\n",
    "                new_x[idxs] = outs\n",
    "                final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "                ###\n",
    "                y = final_out.view(*orig_shape)\n",
    "                if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                    y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "                hidden_state = y\n",
    "\n",
    "                moe_layer_ix += 1\n",
    "\n",
    "            hidden_state = residual + hidden_state\n",
    "\n",
    "\n",
    "        hidden_state = model.model.norm(hidden_state)\n",
    "        logits = model.lm_head(hidden_state)\n",
    "        return logits\n",
    "    \n",
    "run_model_with_ablation(inputs[\"input_ids\"], inputs['attention_mask'], experts_to_ablate = [7] * 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_results = []\n",
    "\n",
    "for this_domain in all_domain_questions:\n",
    "\n",
    "    domain_questions = this_domain['questions']\n",
    "\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    results = []\n",
    "    for question_ix, q in tqdm(enumerate(domain_questions[3:203])):\n",
    "\n",
    "        input_prompt = tokenizer.apply_chat_template(\n",
    "            base_prompt + [\n",
    "                {'role': 'user', 'content': prep_question(q['question'], q['choices'])},\n",
    "                {'role': 'assistant', 'content': 'The correct answer is'},\n",
    "            ],\n",
    "            tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    "        )\n",
    "        inputs = tokenizer(input_prompt, return_tensors = 'pt').to(main_device)\n",
    "\n",
    "        outputs = run_model_with_ablation(inputs[\"input_ids\"], inputs[\"attention_mask\"], experts_to_ablate = [7] * 26)\n",
    "        next_token_logits = outputs[0, -1, :]\n",
    "        next_token_id = torch.argmax(next_token_logits).item()\n",
    "        predicted_text = tokenizer.decode([next_token_id]).strip()\n",
    "\n",
    "        predicted_letter = None\n",
    "        for c in predicted_text:\n",
    "            if c.upper() in [chr(65 + i) for i in range(len(q['choices']))]:\n",
    "                predicted_letter = c.upper()\n",
    "                break\n",
    "\n",
    "        result = {\n",
    "            'domain': this_domain['domain'],\n",
    "            'question_ix': question_ix, \n",
    "            'model_output': predicted_text,\n",
    "            'model_choice': predicted_letter,\n",
    "            'correct_choice': q['answer_char'],\n",
    "            'is_correct': 1 if predicted_letter == q['answer_char'] else 0\n",
    "        }\n",
    "\n",
    "        if result['is_correct'] == 1: count_correct += 1\n",
    "        else: count_incorrect += 1\n",
    "\n",
    "        results.append(result)\n",
    "    \n",
    "\n",
    "    cat_results.append({\n",
    "        'answer_df': pd.DataFrame(results)\n",
    "    })\n",
    "\n",
    "    print(f'{this_domain[\"domain\"]} | Correct: {str(count_correct)} | Incorrect: {str(count_incorrect)} | Accuracy: {(count_correct / (count_correct + count_incorrect)) * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'DeepseekV2MLP' in str(type(model.model.layers[0].mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# mmlu_ds = load_dataset(\"TIGER-Lab/MMLU-Pro\", split = 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
