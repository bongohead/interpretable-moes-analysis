{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from utils.memory import check_memory, profile_memory, clear_all_cuda_memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm \n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_id = 'deepseek-ai/DeepSeek-V2-Lite'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda()\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "\n",
    "# Hooks needed: https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite/blob/main/modeling_deepseek.py\n",
    "inputs = tokenizer(['Hi this is dog', 'Where is the beef'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "\n",
    "def attach_moe_gate_hooks(model):\n",
    "    \"\"\"\n",
    "    Registers forward-hooks on each MoE gating module in 'model' so that after a forward pass,\n",
    "    we can retrieve the BN x topk 'topk_idx' from each layer.\n",
    "    \n",
    "    Returns:\n",
    "        all_expert_ids: A list that will be filled at runtime withtuples of (layer_index, topk_idx_tensor).\n",
    "        handles: A dictionary of {layer_index: hook_handle}, so you can remove them if desired.\n",
    "    \"\"\"\n",
    "    all_expert_ids = []\n",
    "    handles = {}\n",
    "\n",
    "    def gate_forward_hook(module, input, output):\n",
    "        \"\"\"\n",
    "        This hook is triggered after MoEGate.forward(...).\n",
    "        'output' should be the tuple: (topk_idx, topk_weight, aux_loss).\n",
    "        We only need topk_idx here.\n",
    "        \"\"\"\n",
    "        topk_idx, _, _ = output\n",
    "        all_expert_ids.append(topk_idx.detach())\n",
    "\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        # Layer 0 is not moe\n",
    "        if layer_idx > 0:\n",
    "            # attach an attribute so we know which layer this gating belongs to\n",
    "            layer.mlp.gate._layer_id = layer_idx\n",
    "            hook_handle = layer.mlp.gate.register_forward_hook(gate_forward_hook)\n",
    "            handles[layer_idx] = hook_handle\n",
    "\n",
    "    return all_expert_ids, handles\n",
    "\n",
    "\n",
    "all_expert_ids, hook_handles = attach_moe_gate_hooks(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "for topk_idx_tensor in all_expert_ids:\n",
    "    print(f\"topk_idx shape = {topk_idx_tensor.shape}\")\n",
    "    # e.g. shape is [B*N, top_k]\n",
    "\n",
    "for layer_idx, h in hook_handles.items():\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "\n",
    "# No need for hooks! https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py\n",
    "inputs = tokenizer(['Hi this is dog', 'Where is the beef'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_topk_experts, hook_handles = attach_moe_gate_hooks(model)\n",
    "    outputs = model(**inputs)\n",
    "    for layer_idx, h in hook_handles.items():\n",
    "        h.remove()\n",
    "\n",
    "all_topk_experts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Get MMLU data and domains to test\n",
    "\"\"\"\n",
    "from datasets import load_dataset\n",
    "\n",
    "mmlu_ds = load_dataset(\"cais/mmlu\", 'all', split = 'test')\n",
    "print(mmlu_ds[0])\n",
    "\n",
    "# Only retain domains for high school subjects\n",
    "domains = [x for x in list(set([x['subject'] for x in mmlu_ds])) if 'high_school_' in x]\n",
    "print(domains)\n",
    "\n",
    "# Now let's put the MMLU questions into a list gruoped by domain\n",
    "all_domain_questions = [\n",
    "    {\n",
    "        'domain': domain,\n",
    "        'questions': \n",
    "            [\n",
    "                {'question': q['question'], 'choices': q['choices'], 'answer_index': q['answer'], 'answer_char': chr(65 + q['answer'])}\n",
    "                for q in mmlu_ds\n",
    "                if q['subject'] == domain \n",
    "            ]\n",
    "    }\n",
    "    for domain in tqdm(domains)\n",
    "]\n",
    "\n",
    "all_domain_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create function to map MMLU data into questions\n",
    "\"\"\"\n",
    "def prep_question(question, choices):\n",
    "    \n",
    "    prompt = f\"Question: {question}\\nChoices:\\n\"\n",
    "    \n",
    "    for i, option in enumerate(choices):\n",
    "        letter = chr(65 + i)\n",
    "        prompt += f\"({letter}) {option}\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# print(prep_question(mmlu_ds[0]['question'], mmlu_ds[0]['choices']))\n",
    "\n",
    "print(tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'system', 'content': 'You will be provided with a multiple-choice question, as well as a list of possible answer choices. Respond exactly with: \"The correct answer is {X}\", substituting in X with the code for the correct choice.'},\n",
    "        {'role': 'user', 'content': prep_question(mmlu_ds[0]['question'], mmlu_ds[0]['choices'])},\n",
    "        {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + mmlu_ds[0]['answer'])}\n",
    "    ],\n",
    "    tokenize = False, add_generation_prompt = False, continue_final_message = True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "\n",
    "cat_results = []\n",
    "\n",
    "for this_domain in all_domain_questions[0:3]:\n",
    "\n",
    "    domain_questions = this_domain['questions']\n",
    "\n",
    "    count_correct = 0\n",
    "    count_incorrect = 0\n",
    "    topk_dfs = []\n",
    "    results = []\n",
    "    for question_ix, q in tqdm(enumerate(domain_questions[3:103])):\n",
    "\n",
    "        input_prompt = tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {'role': 'system', 'content': 'You will be provided with a multiple-choice question, as well as a list of possible answer choices. Respond exactly with: \"The correct answer is {X}\", substituting in X with the code for the correct choice.'},\n",
    "                {'role': 'user', 'content': prep_question(domain_questions[0]['question'], domain_questions[0]['choices'])},\n",
    "                {'role': 'assistant', 'content': 'The correct answer is ' + domain_questions[0]['answer_char']},\n",
    "                {'role': 'user', 'content': prep_question(domain_questions[1]['question'], domain_questions[1]['choices'])},\n",
    "                {'role': 'assistant', 'content': 'The correct answer is ' + domain_questions[1]['answer_char']},\n",
    "                {'role': 'user', 'content': prep_question(domain_questions[2]['question'], domain_questions[2]['choices'])},\n",
    "                {'role': 'assistant', 'content': 'The correct answer is ' + domain_questions[2]['answer_char']},\n",
    "                {'role': 'user', 'content': prep_question(q['question'], q['choices'])},\n",
    "                {'role': 'assistant', 'content': 'The correct answer is'},\n",
    "            ],\n",
    "            tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    "        )\n",
    "        inputs = tokenizer(input_prompt, return_tensors = 'pt').to(main_device)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            all_topk_experts, hook_handles = attach_moe_gate_hooks(model)\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "            for layer_idx, h in hook_handles.items():\n",
    "                h.remove()\n",
    "\n",
    "            topk_df = convert_topk_to_df(all_topk_experts, input_ids).assign(domain = this_domain['domain'], question_ix = question_ix).drop(columns = 'sequence_ix')\n",
    "            topk_df = topk_df[topk_df['token_id'] != tokenizer.pad_token_id] # Filter out rows with attention_mask\n",
    "            topk_dfs.append(topk_df)\n",
    "\n",
    "            next_token_logits = outputs['logits'][0, -1, :]\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "            predicted_text = tokenizer.decode([next_token_id]).strip()\n",
    "\n",
    "        predicted_letter = None\n",
    "        for c in predicted_text:\n",
    "            if c.upper() in [chr(65 + i) for i in range(len(q['choices']))]:\n",
    "                predicted_letter = c.upper()\n",
    "                break\n",
    "\n",
    "        result = {\n",
    "            'domain': this_domain['domain'],\n",
    "            'question_ix': question_ix, \n",
    "            'model_output': predicted_text,\n",
    "            'model_choice': predicted_letter,\n",
    "            'correct_choice': q['answer_char'],\n",
    "            'is_correct': 1 if predicted_letter == q['answer_char'] else 0\n",
    "        }\n",
    "\n",
    "        if result['is_correct'] == 1:\n",
    "            count_correct += 1\n",
    "        else:\n",
    "            count_incorrect += 1\n",
    "\n",
    "        results.append(result)\n",
    "    \n",
    "\n",
    "    cat_results.append({\n",
    "        'answer_df': pd.DataFrame(results),\n",
    "        'topks_df': pd.concat(topk_dfs)\n",
    "    })\n",
    "\n",
    "    print(f'{this_domain[\"domain\"]} | Correct: {str(count_correct)} | Incorrect: {str(count_incorrect)} | Accuracy: {(count_correct / (count_correct + count_incorrect)) * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map =\\\n",
    "    pd.DataFrame([{\"token\": token.replace('Ä ', ' '), \"token_id\": token_id} for token, token_id in tokenizer.get_vocab().items()])\\\n",
    "    .sort_values(by = 'token_id')\\\n",
    "    .reset_index()\n",
    "\n",
    "display(vocab_map)\n",
    "\n",
    "all_answers = pd.concat([cat['answer_df'] for cat in cat_results])\n",
    "display(all_answers)\n",
    "\n",
    "all_topks = pd.concat([cat['topks_df'] for cat in cat_results]).merge(vocab_map, how = 'left', on = 'token_id')\n",
    "display(all_topks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answers.to_csv('all_answers.csv', index = False)\n",
    "all_topks.to_csv('all_topks.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# mmlu_ds = load_dataset(\"TIGER-Lab/MMLU-Pro\", split = 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
