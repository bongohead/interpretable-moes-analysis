{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from utils.memory import check_memory, profile_memory, clear_all_cuda_memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import plotly\n",
    "import plotly.express as px \n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base model\n",
    "\"\"\"\n",
    "hf_model_id = 'Qwen/Qwen1.5-MoE-A2.7B-Chat'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test a forward pass with topk experts and weights extracted\n",
    "\"\"\"\n",
    "# No need for hooks! https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_moe/modeling_qwen2_moe.py\n",
    "\n",
    "def test_forward_pass():\n",
    "\n",
    "    inputs = tokenizer(['Hi this is dog', 'Where is the beef'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_router_logits = True)\n",
    "\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for l, layer_router_logits in enumerate(outputs.router_logits):\n",
    "        # layer_router_logits is shape [B*N, num_experts]\n",
    "        gating_probs = torch.softmax(layer_router_logits, dim = -1)\n",
    "        routing_weights, topk_experts = torch.topk(gating_probs, k = model.config.num_experts_per_tok, dim = -1, sorted = True)\n",
    "        \n",
    "        all_topk_experts.append(topk_experts.detach().cpu()) \n",
    "        all_topk_weights.append(routing_weights.detach().cpu().to(torch.float32))\n",
    "\n",
    "    print(f\"all_expert_weights is of length {len(all_topk_experts)}\")\n",
    "    print(f\"all_topk_weights is of length {len(all_topk_weights)}\")\n",
    "\n",
    "    print(f\"all_topk_experts[0] shape = {all_topk_experts[0].shape}\") # should be BN x topk\n",
    "    print(f\"all_topk_weights[0] shape = {all_topk_weights[0].shape}\") # should be BN x topk\n",
    "    \n",
    "    print(f\"all_topk_experts[0]: {all_topk_experts[0]}\")\n",
    "    print(f\"all_topk_weights[0]: {all_topk_weights[0]}\")\n",
    "\n",
    "test_forward_pass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Get MMLU data and domains to test\n",
    "\"\"\"\n",
    "raw_mmlu_ds = load_dataset(\"cais/mmlu\", 'all', split = 'test')\n",
    "print(raw_mmlu_ds[0])\n",
    "\n",
    "# all_domains = list(set(mmlu_ds['subject']))\n",
    "# for domain in all_domains:\n",
    "#     dom_length = len([q for q in mmlu_ds if q['subject'] == domain])\n",
    "#     print(f\"{domain}: {dom_length}\")\n",
    "\n",
    "# Dict containing {final_domains: [source1, source2], ...}\n",
    "mmlu_domain_mappings = {\n",
    "    'math': ['elementary_mathematics'], \n",
    "    'cs': ['high_school_computer_science', 'college_computer_science'], # 100 each\n",
    "    'history': ['high_school_world_history'],\n",
    "    # 'psych': ['high_school_psychology'],\n",
    "    'chemistry': ['high_school_chemistry'],\n",
    "    'biology': ['high_school_biology']\n",
    "}\n",
    "\n",
    "# Now let's put the MMLU questions into a list grouped by domain\n",
    "def group_mmlu_ds(raw_ds, domain_map, max_questions_per_domain):\n",
    "    source_to_domain_map = {source: domain for domain, sources in mmlu_domain_mappings.items() for source in sources} # Map each source => domain\n",
    "    final_ds = {domain: {'sources': sources, 'questions': []} for domain, sources in domain_map.items()} # Create empty dict to fill\n",
    "    for q in raw_ds: \n",
    "        if q['subject'] in source_to_domain_map.keys():\n",
    "            if (len(final_ds[source_to_domain_map[q['subject']]]['questions']) >= max_questions_per_domain):\n",
    "                continue\n",
    "            final_ds[source_to_domain_map[q['subject']]]['questions'].append({\n",
    "                'question': q['question'],\n",
    "                'choices': q['choices'],\n",
    "                'answer_index': q['answer'],\n",
    "                'answer_char': chr(65 + q['answer'])\n",
    "            })\n",
    "    return [{'domain': domain, **values} for domain, values in final_ds.items()] # Convert back to list of dicts\n",
    "\n",
    "mmlu_ds = group_mmlu_ds(raw_mmlu_ds, mmlu_domain_mappings, 200)\n",
    "mmlu_ds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create function to map MMLU data into string\n",
    "\"\"\"\n",
    "def prep_question(question, choices):\n",
    "    prompt = f\"Question: {question}\\nChoices:\\n\"\n",
    "    for i, option in enumerate(choices):\n",
    "        letter = chr(65 + i)\n",
    "        prompt += f\"({letter}) {option}\\n\"\n",
    "    return prompt\n",
    "\n",
    "fs_ex = [\n",
    "    [q for q in raw_mmlu_ds if q['subject'] == 'anatomy'][0],\n",
    "    [q for q in raw_mmlu_ds if q['subject'] == 'machine_learning'][0],\n",
    "    [q for q in raw_mmlu_ds if q['subject'] == 'astronomy'][0]\n",
    "]\n",
    "\n",
    "base_prompt = [\n",
    "    {'role': 'system', 'content': 'You will be provided with a multiple-choice question, as well as a list of possible answer choices. Respond exactly with: \"The correct answer is {X}\", substituting in X with the code for the correct choice.'},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[0]['question'], fs_ex[0]['choices'])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + fs_ex[0]['answer'])},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[1]['question'], fs_ex[1]['choices'])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + fs_ex[1]['answer'])},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[2]['question'], fs_ex[2]['choices'])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + fs_ex[2]['answer'])}\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(base_prompt, tokenize = False, add_generation_prompt = False, continue_final_message = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run logit lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_ds[0]['questions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_prompt = tokenizer.apply_chat_template(\n",
    "    base_prompt + [{'role': 'user', 'content': prep_question(mmlu_ds[0]['questions'][0]['question'], mmlu_ds[0]['questions'][0]['choices'])}, {'role': 'assistant', 'content': 'The correct answer is'}],\n",
    "    tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    ")\n",
    "inputs = tokenizer(test_prompt, return_tensors = 'pt').to(main_device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_embeds = model.model.embed_tokens(inputs['input_ids'])\n",
    "    \n",
    "    cache_position = torch.arange(0, input_embeds.shape[1], device = input_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = model.model._update_causal_mask(inputs['attention_mask'], input_embeds, cache_position, None, None)\n",
    "\n",
    "    hidden_state = input_embeds\n",
    "    position_embeddings = model.model.rotary_emb(hidden_state, position_ids)\n",
    "\n",
    "    layer_outputs = []\n",
    "    for layer in model.model.layers:\n",
    "        hidden_state = layer(hidden_state, attention_mask = causal_mask, position_ids = position_ids, position_embeddings = position_embeddings)[0]\n",
    "        layer_outputs.append(hidden_state.detach())\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "\n",
    "top_k_probs = 5\n",
    "with torch.no_grad():\n",
    "    logit_lens_outputs = []\n",
    "\n",
    "    for layer_ix, layer_output in enumerate(layer_outputs):\n",
    "        layer_output = model.model.norm(layer_output)\n",
    "        lm_output = model.lm_head(layer_output).float()\n",
    "\n",
    "        last_token_logits = lm_output[0, -1, :]\n",
    "        probabilities = torch.nn.functional.softmax(last_token_logits, dim = 0)\n",
    "\n",
    "        top_probabilities, top_indices = torch.topk(probabilities, top_k_probs, dim = 0)  # top_k\n",
    "        for rank in range(top_k_probs):\n",
    "            token_idx = top_indices[rank].item()\n",
    "            prob = round(top_probabilities[rank].item(), 6)\n",
    "            token = tokenizer.decode([token_idx])\n",
    "            logit_lens_outputs.append({'layer_ix': layer_ix, 'prob': prob, 'token_rank': rank + 1, 'token': token})\n",
    "            \n",
    "            \n",
    "logit_lens_df = pd.DataFrame(logit_lens_outputs)\n",
    "logit_lens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logit_lens(logit_lens_df):\n",
    "\n",
    "    logit_lens_plot_df = logit_lens_df\\\n",
    "        .assign(token_str = lambda df: np.where(\n",
    "            df['token'].isin([' A', ' B', ' C', ' D']),\n",
    "            '<span style=\"font-weight:bold;font-size:13px\">' + df['token'] + '</span>',\n",
    "            '<span style=\"font-size:11px\">' + df['token'] + '</span>')\n",
    "        )\\\n",
    "        .assign(text = lambda df: df.apply(lambda row: f\"{row['token_str']} <span style=\\\"display:none;font-size:8px;color:lightgray;font-weight:bold\\\"> {(row['prob'] * 100):.1f}%</span>\", axis=1))\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] >= 12])\n",
    "\n",
    "    neg_colors = plotly.colors.sequential.YlOrRd_r\n",
    "    pos_colors = plotly.colors.sequential.YlGnBu\n",
    "\n",
    "    neg_breaks = [-1.0, -0.6, -0.4, -0.2, -0.05, -0.01, 0]  # Define breakpoints for negative values\n",
    "    pos_breaks = [0, 0.01, 0.05, 0.1, 0.4, 0.6, 1.0] # Define breakpoints for positive values\n",
    "    all_breaks = neg_breaks + pos_breaks[1:]  # Skip duplicate 0\n",
    "\n",
    "    # Calculate the normalized positions for each breakpoint\n",
    "    min_val = all_breaks[0]\n",
    "    max_val = all_breaks[-1]\n",
    "    norm_positions = [(val - min_val) / (max_val - min_val) for val in all_breaks]\n",
    "\n",
    "    # Create hybrid colorscale with Plasma for negative and Viridis for positive\n",
    "    hybrid_scale = []\n",
    "\n",
    "    # Add colors from Plasma for the negative part\n",
    "    for i in range(len(neg_breaks)):\n",
    "        norm_pos = i / (len(neg_breaks) - 1)\n",
    "        plasma_idx = min(int(norm_pos * len(neg_colors)), len(neg_colors) - 1)\n",
    "        hybrid_scale.append([norm_positions[i], neg_colors[plasma_idx]])\n",
    "\n",
    "    # Add colors from Viridis for the positive part\n",
    "    for i in range(1, len(pos_breaks)):\n",
    "        viridis_idx = int((i / (len(pos_breaks) - 1)) * (len(pos_colors) - 1))\n",
    "        hybrid_scale.append([norm_positions[len(neg_breaks) + i - 1], pos_colors[viridis_idx]])\n",
    "\n",
    "    # Create pivot tables for the heatmap and text\n",
    "    pivot_prob = logit_lens_plot_df.pivot(index='token_rank', columns='layer_ix', values='prob')\n",
    "    pivot_text = logit_lens_plot_df.pivot(index='token_rank', columns='layer_ix', values='text')\n",
    "\n",
    "    # Create the heatmap with go.Heatmap\n",
    "    fig =\\\n",
    "        go.Figure(data = go.Heatmap(\n",
    "            z = pivot_prob.values,\n",
    "            x = pivot_prob.columns,\n",
    "            y = pivot_prob.index,\n",
    "            colorscale = hybrid_scale,\n",
    "            text = pivot_text.values,\n",
    "            texttemplate = \"%{text}\",\n",
    "            zmin = min_val,\n",
    "            zmax = max_val,\n",
    "            colorbar = dict(\n",
    "                title = \"Probability\",\n",
    "                tickvals = all_breaks,\n",
    "                ticktext = [str(v) for v in all_breaks]\n",
    "            )\n",
    "        ))\\\n",
    "        .update_yaxes(autorange=\"reversed\")\\\n",
    "        .update_layout(\n",
    "            title = \"Logit Lens\",\n",
    "            xaxis_title = \"Layer Index\",\n",
    "            yaxis_title = \"Token Rank\",\n",
    "            width = 1400,\n",
    "            height = 600\n",
    "        )\n",
    "\n",
    "    return fig\n",
    "\n",
    "plot_logit_lens(logit_lens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ablated lens\n",
    "\"\"\"\n",
    "def get_logit_lens_probs(input_ids, attention_mask, layers_to_ablate = list(range(0, 24)), topk_to_ablate = [0], renorm = False):\n",
    "    \"\"\"\n",
    "    Ablates certain rank-ordered top-k columns for the specified layers.\n",
    "    \n",
    "    Params:\n",
    "        @layers_to_ablate: Which layer indices (0-based) should we ablate?\n",
    "        @renorm: Whether to rescale the remaining weights to keep the sum unchanged\n",
    "        @topk_to_ablate: A list of ranks to ablate, e.g. [0] = top-1, [2,3] = 3rd & 4th largest, etc.\n",
    "    \"\"\"\n",
    "    input_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    cache_position = torch.arange(0, input_embeds.shape[1], device = input_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = model.model._update_causal_mask(attention_mask, input_embeds, cache_position, None, None)\n",
    "\n",
    "    hidden_state = input_embeds\n",
    "    position_embeddings = model.model.rotary_emb(hidden_state, position_ids)\n",
    "\n",
    "    layer_outputs = []\n",
    "    for layer_ix, layer in enumerate(model.model.layers):\n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        hidden_state, _, _ = layer.self_attn(hidden_states = hidden_state, attention_mask = causal_mask, position_ids = position_ids, position_embeddings = position_embeddings)\n",
    "        hidden_state = residual + hidden_state\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "        # MoE\n",
    "        ####### Qwen2MoeSparseMoeBlock - below code replaces hidden_state = layer.mlp(hidden_state)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_state.shape\n",
    "        moe_hidden_state = hidden_state.view(-1, hidden_dim)\n",
    "        router_logits = layer.mlp.gate(moe_hidden_state) # Size (BN, n_experts)\n",
    "\n",
    "        routing_weights = torch.nn.functional.softmax(router_logits, dim = 1, dtype = torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, layer.mlp.top_k, dim = -1, sorted = True)\n",
    "        routing_weights = routing_weights.to(moe_hidden_state.dtype)\n",
    "\n",
    "        #### ABLATION\n",
    "        if layer_ix in layers_to_ablate:\n",
    "            row_sum_before = routing_weights.sum(dim = -1, keepdim = True) # Shaype (BN, 1)            \n",
    "            # For each rank in topk_to_ablate, zero out that column\n",
    "            for rank in topk_to_ablate:\n",
    "                routing_weights[:, rank] = 0.0\n",
    "            if renorm:\n",
    "                row_sum_after = routing_weights.sum(dim = -1, keepdim = True)\n",
    "                scale_factor = row_sum_before / (row_sum_after + 1e-9)\n",
    "                routing_weights *= scale_factor\n",
    "        ####\n",
    "        final_hidden_states = torch.zeros((batch_size * sequence_length, hidden_dim), dtype = moe_hidden_state.dtype, device = moe_hidden_state.device)\n",
    "\n",
    "        # One hot encode the selected experts to create an expert mask \n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes = layer.mlp.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(layer.mlp.num_experts):\n",
    "            expert_layer = layer.mlp.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "            # Index the correct hidden states and compute the expert hidden state for the current expert.\n",
    "            current_state = moe_hidden_state[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(moe_hidden_state.dtype))\n",
    "\n",
    "        shared_expert_output = layer.mlp.shared_expert(moe_hidden_state)\n",
    "        shared_expert_output = torch.nn.functional.sigmoid(layer.mlp.shared_expert_gate(moe_hidden_state)) * shared_expert_output\n",
    "\n",
    "        final_hidden_states = (final_hidden_states + shared_expert_output).reshape(batch_size, sequence_length, hidden_dim)\n",
    "        #######\n",
    "        hidden_state = final_hidden_states\n",
    "        hidden_state = residual + hidden_state\n",
    "        \n",
    "        layer_outputs.append(hidden_state.detach())\n",
    "\n",
    "    layer_probs = []\n",
    "    for layer_ix, layer_output in enumerate(layer_outputs):\n",
    "        layer_output = model.model.norm(layer_output)\n",
    "        lm_output = model.lm_head(layer_output).float()\n",
    "        last_token_logits = lm_output[0, -1, :].detach().cpu()\n",
    "        layer_probs.append(torch.nn.functional.softmax(last_token_logits, dim = 0))\n",
    "\n",
    "    return layer_probs\n",
    "\n",
    "\n",
    "def draw_logit_lens_topk(logit_lens_probs, top_k_probs = 8):\n",
    "    logit_lens_outputs = []\n",
    "    for layer_ix, layer_probs in enumerate(logit_lens_probs):\n",
    "        _, top_indices = torch.topk(layer_probs.abs(), top_k_probs, dim = 0)  # top_k  # Get abs value\n",
    "        top_probabilities = layer_probs[top_indices]\n",
    "        for rank in range(top_k_probs):\n",
    "            token_idx = top_indices[rank].item()\n",
    "            prob = round(top_probabilities[rank].item(), 6)\n",
    "            token = tokenizer.decode([token_idx])\n",
    "            logit_lens_outputs.append({'layer_ix': layer_ix, 'prob': prob, 'token_rank': rank + 1, 'token': token})\n",
    "\n",
    "    logit_lens_df = pd.DataFrame(logit_lens_outputs)\n",
    "    return plot_logit_lens(logit_lens_df)\n",
    "\n",
    "logit_lens_probs = get_logit_lens_probs(inputs['input_ids'], inputs['attention_mask'], layers_to_ablate = list(range(0, 1)), topk_to_ablate = [0, 1, 2, 3], renorm = False)\n",
    "draw_logit_lens_topk(logit_lens_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_logit_lens_probs = get_logit_lens_probs(inputs['input_ids'], inputs['attention_mask'], layers_to_ablate = list(range(0, 1)), topk_to_ablate = [], renorm = False)\n",
    "logit_lens_probs = get_logit_lens_probs(inputs['input_ids'], inputs['attention_mask'], layers_to_ablate = list(range(0, 1)), topk_to_ablate = [0, 1, 2, 3], renorm = False)\n",
    "probdiffs = [logit_lens_probs[ix] - baseline_logit_lens_probs[ix] for ix in range(0, len(logit_lens_probs))]\n",
    "draw_logit_lens_topk(probdiffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_expert_probs = get_logit_lens_probs(inputs['input_ids'], inputs['attention_mask'], layers_to_ablate = list(range(6, 8)), topk_to_ablate = [0, 1, 2, 3], renorm = False)\n",
    "logit_lens_probs = get_logit_lens_probs(inputs['input_ids'], inputs['attention_mask'], layers_to_ablate = list(range(6, 8)), topk_to_ablate = [1, 2, 3], renorm = False)\n",
    "probdiffs = [logit_lens_probs[ix] - baseline_logit_lens_probs[ix] for ix in range(0, len(logit_lens_probs))]\n",
    "draw_logit_lens_topk(probdiffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save topk for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_model_and_return_topk(mmlu_ds):\n",
    "\n",
    "    final_results = []\n",
    "\n",
    "    for this_domain in mmlu_ds:\n",
    "\n",
    "        domain_questions = this_domain['questions']\n",
    "        domain_results = []\n",
    "\n",
    "        for question_ix, q in enumerate(domain_questions):\n",
    "\n",
    "            input_prompt = tokenizer.apply_chat_template(\n",
    "                base_prompt + [{'role': 'user', 'content': prep_question(q['question'], q['choices'])}, {'role': 'assistant', 'content': 'The correct answer is'}],\n",
    "                tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    "            )\n",
    "            inputs = tokenizer(input_prompt, return_tensors = 'pt').to(main_device)\n",
    "\n",
    "            outputs = model(input_ids = inputs[\"input_ids\"], attention_mask = inputs[\"attention_mask\"], output_router_logits = True)\n",
    "            all_topk_experts = []\n",
    "            all_topk_weights = []\n",
    "            for l, layer_router_logits in enumerate(outputs.router_logits):\n",
    "                # layer_router_logits is shape [B*N, num_experts]\n",
    "                gating_probs = torch.softmax(layer_router_logits, dim = -1)\n",
    "                routing_weights, topk_experts = torch.topk(gating_probs, k = model.config.num_experts_per_tok, dim = -1, sorted = True)\n",
    "                \n",
    "                all_topk_experts.append(topk_experts.detach().cpu()) \n",
    "                all_topk_weights.append(routing_weights.detach().cpu().to(torch.float32))\n",
    "\n",
    "            topk_df = convert_topk_to_df(inputs[\"input_ids\"], all_topk_experts, all_topk_weights).assign(question_ix = question_ix).drop(columns = 'sequence_ix')\n",
    "            topk_df = topk_df[topk_df['token_id'] != tokenizer.pad_token_id] # Filter out rows with attention_mask\n",
    "            \n",
    "            predicted_text = tokenizer.decode([torch.argmax(outputs['logits'][0, -1, :]).item()]).strip()\n",
    "            predicted_letter = None\n",
    "            for c in predicted_text:\n",
    "                if c.upper() in [chr(65 + i) for i in range(len(q['choices']))]:\n",
    "                    predicted_letter = c.upper()\n",
    "                    break\n",
    "\n",
    "            domain_results.append({\n",
    "                'question_ix': question_ix, \n",
    "                'model_output': predicted_text,\n",
    "                'model_choice': predicted_letter,\n",
    "                'correct_choice': q['answer_char'],\n",
    "                'is_correct': 1 if predicted_letter == q['answer_char'] else 0,\n",
    "                'is_valid': 1 if predicted_letter is not None else 0,\n",
    "                'topk_df': topk_df\n",
    "            })\n",
    "\n",
    "        n_total = len(domain_results)\n",
    "        n_correct = len([x for x in domain_results if x['is_correct'] == 1])\n",
    "        n_invalid = len([x for x in domain_results if x['is_valid'] == 0])\n",
    "        print(f'{this_domain[\"domain\"]} | Correct: {str(n_correct)} | Incorrect: {str(n_total - n_correct)} | Invalid: {str(n_invalid)} | Accuracy: {(n_correct / (n_total)) * 100:.1f}%')\n",
    "        \n",
    "        final_results.append({\n",
    "            'domain': this_domain['domain'],\n",
    "            'question_df': pd.DataFrame([{k: v for k, v in x.items() if k != 'topk_df'} for x in domain_results]).assign(domain = this_domain['domain']),\n",
    "            'topk_df': pd.concat([x['topk_df'] for x in domain_results]).assign(domain = this_domain['domain']),\n",
    "            'n_correct': n_correct,\n",
    "            'n_total': n_total,\n",
    "            'n_invalid': n_invalid,\n",
    "        })\n",
    "\n",
    "\n",
    "    return {\n",
    "        'question_df': pd.concat([d['question_df'] for d in final_results]),\n",
    "        'topk_df': pd.concat([d['topk_df'] for d in final_results]),\n",
    "        'n_correct': sum([d['n_correct'] for d in final_results]),\n",
    "        'n_total': sum([d['n_total'] for d in final_results]),\n",
    "        'n_invalid': sum([d['n_invalid'] for d in final_results]),\n",
    "        'accuracy': sum([d['n_correct'] for d in final_results])/sum([d['n_total'] for d in final_results])\n",
    "        }\n",
    "\n",
    "baseline = run_model_and_return_topk(mmlu_ds)\n",
    "display(baseline['topk_df'])\n",
    "print(baseline['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map =\\\n",
    "    pd.DataFrame([{\"token\": token.replace('Ġ', ' '), \"token_id\": token_id} for token, token_id in tokenizer.get_vocab().items()])\\\n",
    "    .sort_values(by = 'token_id')\\\n",
    "    .reset_index()\\\n",
    "    .drop(columns = 'index')\n",
    "\n",
    "display(vocab_map)\n",
    "\n",
    "all_answers = baseline['question_df']\n",
    "display(all_answers)\n",
    "\n",
    "all_topks = baseline['topk_df'].assign(weight = lambda df: np.around(df['weight'], 4))\n",
    "display(all_topks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map.to_csv('vocab.csv', index = False)\n",
    "all_answers.to_csv('all_answers.csv', index = False)\n",
    "all_topks.to_csv('all_topks.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = tokenizer.apply_chat_template(\n",
    "    base_prompt + [\n",
    "        {'role': 'user', 'content': prep_question(mmlu_ds[0]['questions'][0]['question'], mmlu_ds[0]['questions'][0]['choices'])},\n",
    "        {'role': 'assistant', 'content': 'The correct answer is'}\n",
    "    ],\n",
    "    tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    ")\n",
    "inputs = tokenizer(input_prompt, return_tensors = 'pt').to(main_device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids = inputs[\"input_ids\"], attention_mask = inputs['attention_mask'])\n",
    "    print(outputs['logits'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.model(input_ids = inputs[\"input_ids\"], attention_mask = inputs['attention_mask'])\n",
    "    hidden_state = outputs[0]\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    print(logits)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     hidden_state = model.model(input_ids = inputs[\"input_ids\"], attention_mask = inputs['attention_mask'])\n",
    "#     logits = model.lm_head(hidden_state[0])\n",
    "#     print(logits)\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_embeds = model.model.embed_tokens(inputs['input_ids'])\n",
    "    \n",
    "    cache_position = torch.arange(0, input_embeds.shape[1], device = input_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = model.model._update_causal_mask(inputs['attention_mask'], input_embeds, cache_position, None, None)\n",
    "\n",
    "    hidden_state = input_embeds\n",
    "    position_embeddings = model.model.rotary_emb(hidden_state, position_ids)\n",
    "\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer in model.model.layers:\n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        hidden_state, _, _ = layer.self_attn(hidden_states = hidden_state, attention_mask = causal_mask, position_ids = position_ids, position_embeddings = position_embeddings)\n",
    "        hidden_state = residual + hidden_state\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "        # MoE\n",
    "        ####### Qwen2MoeSparseMoeBlock - below code replaces hidden_state = layer.mlp(hidden_state)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_state.shape\n",
    "        moe_hidden_state = hidden_state.view(-1, hidden_dim)\n",
    "        router_logits = layer.mlp.gate(moe_hidden_state) # Size (BN, n_experts)\n",
    "\n",
    "        routing_weights = torch.nn.functional.softmax(router_logits, dim = 1, dtype = torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, layer.mlp.top_k, dim = -1, sorted = True)\n",
    "        routing_weights = routing_weights.to(moe_hidden_state.dtype)\n",
    "\n",
    "        final_hidden_states = torch.zeros((batch_size * sequence_length, hidden_dim), dtype = moe_hidden_state.dtype, device = moe_hidden_state.device)\n",
    "\n",
    "        # One hot encode the selected experts to create an expert mask \n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes = layer.mlp.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(layer.mlp.num_experts):\n",
    "            expert_layer = layer.mlp.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "            # Index the correct hidden states and compute the expert hidden state for the current expert.\n",
    "            current_state = moe_hidden_state[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(moe_hidden_state.dtype))\n",
    "\n",
    "        shared_expert_output = layer.mlp.shared_expert(moe_hidden_state)\n",
    "        shared_expert_output = torch.nn.functional.sigmoid(layer.mlp.shared_expert_gate(moe_hidden_state)) * shared_expert_output\n",
    "\n",
    "        final_hidden_states = (final_hidden_states + shared_expert_output).reshape(batch_size, sequence_length, hidden_dim)\n",
    "        #######\n",
    "        hidden_state = final_hidden_states\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "        all_topk_experts.append(selected_experts.detach().cpu())\n",
    "        all_topk_weights.append(routing_weights.detach().cpu().to(torch.float32))\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    print(logits)\n",
    "    print(len(all_topk_experts))\n",
    "    print(all_topk_experts[0].shape)\n",
    "    print(all_topk_weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_model_no_ablation(input_ids, attention_mask):\n",
    "    input_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    cache_position = torch.arange(0, input_embeds.shape[1], device = input_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = model.model._update_causal_mask(attention_mask, input_embeds, cache_position, None, None)\n",
    "\n",
    "    hidden_state = input_embeds\n",
    "    position_embeddings = model.model.rotary_emb(hidden_state, position_ids)\n",
    "\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer in model.model.layers:\n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        hidden_state, _, _ = layer.self_attn(hidden_states = hidden_state, attention_mask = causal_mask, position_ids = position_ids, position_embeddings = position_embeddings)\n",
    "        hidden_state = residual + hidden_state\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "        # MoE\n",
    "        ####### Qwen2MoeSparseMoeBlock - below code replaces hidden_state = layer.mlp(hidden_state)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_state.shape\n",
    "        moe_hidden_state = hidden_state.view(-1, hidden_dim)\n",
    "        router_logits = layer.mlp.gate(moe_hidden_state) # Size (BN, n_experts)\n",
    "\n",
    "        routing_weights = torch.nn.functional.softmax(router_logits, dim = 1, dtype = torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, layer.mlp.top_k, dim = -1, sorted = True)\n",
    "        routing_weights = routing_weights.to(moe_hidden_state.dtype)\n",
    "\n",
    "        final_hidden_states = torch.zeros((batch_size * sequence_length, hidden_dim), dtype = moe_hidden_state.dtype, device = moe_hidden_state.device)\n",
    "\n",
    "        # One hot encode the selected experts to create an expert mask \n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes = layer.mlp.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(layer.mlp.num_experts):\n",
    "            expert_layer = layer.mlp.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "            # Index the correct hidden states and compute the expert hidden state for the current expert.\n",
    "            current_state = moe_hidden_state[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(moe_hidden_state.dtype))\n",
    "\n",
    "        shared_expert_output = layer.mlp.shared_expert(moe_hidden_state)\n",
    "        shared_expert_output = torch.nn.functional.sigmoid(layer.mlp.shared_expert_gate(moe_hidden_state)) * shared_expert_output\n",
    "\n",
    "        final_hidden_states = (final_hidden_states + shared_expert_output).reshape(batch_size, sequence_length, hidden_dim)\n",
    "        #######\n",
    "        hidden_state = final_hidden_states\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "        all_topk_experts.append(selected_experts.detach().cpu())\n",
    "        all_topk_weights.append(routing_weights.detach().cpu().to(torch.float32))\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_ablation(run_forward_fn, return_topk, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluate modified model\n",
    "\n",
    "    Params:\n",
    "        @run_forward_fn: A function that returns a model forward pass with inputs input_ids, attention_mask, and optional *args/**kwargs. \n",
    "          The function must return a dict with key `logits`.\n",
    "        @return_topk: Whether to return the expert IDs and weights as well. If True, `run_forward_fn` must also return keys\n",
    "         `all_topk_experts` and `all_topk_weights`.\n",
    "        @*args/**kwargs: Additional arguments to pass to `run_forward_fn`.\n",
    "    \"\"\"\n",
    "    final_results = []\n",
    "\n",
    "    for this_domain in mmlu_ds:\n",
    "\n",
    "        domain_questions = this_domain['questions']\n",
    "        domain_results = []\n",
    "\n",
    "        for question_ix, q in enumerate(domain_questions):\n",
    "\n",
    "            input_prompt = tokenizer.apply_chat_template(\n",
    "                base_prompt + [{'role': 'user', 'content': prep_question(q['question'], q['choices'])}, {'role': 'assistant', 'content': 'The correct answer is'}],\n",
    "                tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    "            )\n",
    "            inputs = tokenizer(input_prompt, return_tensors = 'pt').to(main_device)\n",
    "\n",
    "            outputs = run_forward_fn(inputs['input_ids'], inputs['attention_mask'], *args, **kwargs)\n",
    "            \n",
    "            if return_topk == True:\n",
    "                topk_df = convert_topk_to_df(inputs[\"input_ids\"], outputs['all_topk_experts'], outputs['all_topk_weights']).assign(question_ix = question_ix).drop(columns = 'sequence_ix')\n",
    "                topk_df = topk_df[topk_df['token_id'] != tokenizer.pad_token_id] # Filter out rows with attention_mask\n",
    "\n",
    "            predicted_text = tokenizer.decode([torch.argmax(outputs['logits'][0, -1, :]).item()]).strip()\n",
    "            predicted_letter = None\n",
    "            for c in predicted_text:\n",
    "                if c.upper() in [chr(65 + i) for i in range(len(q['choices']))]:\n",
    "                    predicted_letter = c.upper()\n",
    "                    break\n",
    "\n",
    "            domain_results.append({\n",
    "                'question_ix': question_ix, \n",
    "                'model_output': predicted_text,\n",
    "                'model_choice': predicted_letter,\n",
    "                'correct_choice': q['answer_char'],\n",
    "                'is_correct': 1 if predicted_letter == q['answer_char'] else 0,\n",
    "                'is_valid': 1 if predicted_letter is not None else 0,\n",
    "                'topk_df': topk_df if return_topk == True else None\n",
    "            })\n",
    "\n",
    "        n_total = len(domain_results)\n",
    "        n_correct = len([x for x in domain_results if x['is_correct'] == 1])\n",
    "        n_invalid = len([x for x in domain_results if x['is_valid'] == 0])\n",
    "        print(f'{this_domain[\"domain\"]} | Correct: {str(n_correct)} | Incorrect: {str(n_total - n_correct)} | Invalid: {str(n_invalid)} | Accuracy: {(n_correct / (n_total)) * 100:.1f}%')\n",
    "        \n",
    "        final_results.append({\n",
    "            'domain': this_domain['domain'],\n",
    "            'question_df': pd.DataFrame([{k: v for k, v in x.items() if k != 'topk_df'} for x in domain_results]).assign(domain = this_domain['domain']),\n",
    "            'topk_df': pd.concat([x['topk_df'] for x in domain_results]).assign(domain = this_domain['domain']) if return_topk == True else None,\n",
    "            'n_correct': n_correct,\n",
    "            'n_total': n_total,\n",
    "            'n_invalid': n_invalid,\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'question_df': pd.concat([d['question_df'] for d in final_results]),\n",
    "        'topk_df': pd.concat([d['topk_df'] for d in final_results]) if return_topk == True else None,\n",
    "        'n_correct': sum([d['n_correct'] for d in final_results]),\n",
    "        'n_total': sum([d['n_total'] for d in final_results]),\n",
    "        'n_invalid': sum([d['n_invalid'] for d in final_results]),\n",
    "        'accuracy': sum([d['n_correct'] for d in final_results])/sum([d['n_total'] for d in final_results])\n",
    "        }\n",
    "\n",
    "evaluate_with_ablation(run_model_no_ablation, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_model_with_ablation(input_ids, attention_mask, layers_to_ablate = list(range(0, 24)), topk_to_ablate = [0], renorm = False):\n",
    "    \"\"\"\n",
    "    Ablates certain rank-ordered top-k columns for the specified layers.\n",
    "    \n",
    "    Params:\n",
    "        @layers_to_ablate: Which layer indices (0-based) should we ablate?\n",
    "        @renorm: Whether to rescale the remaining weights to keep the sum unchanged\n",
    "        @topk_to_ablate: A list of ranks to ablate, e.g. [0] = top-1, [2,3] = 3rd & 4th largest, etc.\n",
    "    \"\"\"\n",
    "    input_embeds = model.model.embed_tokens(input_ids)\n",
    "    \n",
    "    cache_position = torch.arange(0, input_embeds.shape[1], device = input_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = model.model._update_causal_mask(attention_mask, input_embeds, cache_position, None, None)\n",
    "\n",
    "    hidden_state = input_embeds\n",
    "    position_embeddings = model.model.rotary_emb(hidden_state, position_ids)\n",
    "\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer_ix, layer in enumerate(model.model.layers):\n",
    "        # SA\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.input_layernorm(hidden_state)\n",
    "        hidden_state, _, _ = layer.self_attn(hidden_states = hidden_state, attention_mask = causal_mask, position_ids = position_ids, position_embeddings = position_embeddings)\n",
    "        hidden_state = residual + hidden_state\n",
    "        residual = hidden_state\n",
    "        hidden_state = layer.post_attention_layernorm(hidden_state)\n",
    "\n",
    "        # MoE\n",
    "        ####### Qwen2MoeSparseMoeBlock - below code replaces hidden_state = layer.mlp(hidden_state)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_state.shape\n",
    "        moe_hidden_state = hidden_state.view(-1, hidden_dim)\n",
    "        router_logits = layer.mlp.gate(moe_hidden_state) # Size (BN, n_experts)\n",
    "\n",
    "        routing_weights = torch.nn.functional.softmax(router_logits, dim = 1, dtype = torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, layer.mlp.top_k, dim = -1, sorted = True)\n",
    "        routing_weights = routing_weights.to(moe_hidden_state.dtype)\n",
    "\n",
    "        #### ABLATION\n",
    "        if layer_ix in layers_to_ablate:\n",
    "            row_sum_before = routing_weights.sum(dim = -1, keepdim = True) # Shaype (BN, 1)            \n",
    "            # For each rank in topk_to_ablate, zero out that column\n",
    "            for rank in topk_to_ablate:\n",
    "                routing_weights[:, rank] = 0.0\n",
    "            if renorm:\n",
    "                row_sum_after = routing_weights.sum(dim = -1, keepdim = True)\n",
    "                scale_factor = row_sum_before / (row_sum_after + 1e-9)\n",
    "                routing_weights *= scale_factor\n",
    "        ####\n",
    "        final_hidden_states = torch.zeros((batch_size * sequence_length, hidden_dim), dtype = moe_hidden_state.dtype, device = moe_hidden_state.device)\n",
    "\n",
    "        # One hot encode the selected experts to create an expert mask \n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes = layer.mlp.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # Loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(layer.mlp.num_experts):\n",
    "            expert_layer = layer.mlp.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "            # Index the correct hidden states and compute the expert hidden state for the current expert.\n",
    "            current_state = moe_hidden_state[None, top_x].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(moe_hidden_state.dtype))\n",
    "\n",
    "        shared_expert_output = layer.mlp.shared_expert(moe_hidden_state)\n",
    "        shared_expert_output = torch.nn.functional.sigmoid(layer.mlp.shared_expert_gate(moe_hidden_state)) * shared_expert_output\n",
    "\n",
    "        final_hidden_states = (final_hidden_states + shared_expert_output).reshape(batch_size, sequence_length, hidden_dim)\n",
    "        #######\n",
    "        hidden_state = final_hidden_states\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "        all_topk_experts.append(selected_experts.detach().cpu())\n",
    "        all_topk_weights.append(routing_weights.detach().cpu().to(torch.float32))\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "    \n",
    "evaluate_with_ablation(run_model_with_ablation, return_topk = False, layers_to_ablate = list(range(6, 7)), topk_to_ablate = [0, 1, 2, 3], renorm = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0, 24))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
