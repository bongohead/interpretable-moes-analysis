{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This runs forward passes on samples and stores: (1) pre-MLP activations; (2) expert outputs; \n",
    " (3) top-k expert selections; (4) sample metadata.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.store_outputs import convert_outputs_to_df\n",
    "from utils import pretrained_models\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()\n",
    "\n",
    "# Below are for Mamba replicability - can remove if remove all SSMs\n",
    "# os.environ['MAMBA_DISABLE_CUDA_KERNELS'] = '1'\n",
    "# os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'\n",
    "# torch.use_deterministic_algorithms(True, warn_only = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\n",
    "Architectures supported currently:\n",
    "- 0. OlMoE architecture: OLMoE-1B-7B-* (1B/7B)\n",
    "- 1. Qwen2MoE architecture: Qwen1.5-MoE-A2.7B-* (2.7B/14.3B), Qwen2-57B-A14B-* (14B/57B)\n",
    "- 2. Deepseek v2 architecture: Deepseek-v2-Lite (2.4B/15.7B), Deepseek-v2 (21B/236B) -> use trust_remote_code = False\n",
    "- 3. Deepseek v3 architecture: Deepseek-v3 (37B/671B), Deepseek-R1 (37B/671B), Moonlight-16B-A3B (3B/16B) -> use trust_remote_code = False\n",
    "- 4. Qwen3MoE architecture: Qwen3-30B-A3B (3B/30B), Qwen3-235B-A22B (22B/235B), Qwen3-Coder (35B/480B)\n",
    "- 5. KimiVL architecture: Kimi-VL-A3B-* (3B/16B)\n",
    "- 6. Granite architecture: Granite-4.0-Tiny-* (1B/7B)\n",
    "- 7: GLM4MoE architecture: GLM-4.5 (32B/355B), GLM-4.5 Air (12B/106B) * Supports multi-GPU\n",
    "- 8: GTP-OSS architecture: GPT-OSS-120B (5B/117B), GPT-OSS-20B (4B/21B)\n",
    "\"\"\"\n",
    "selected_model_index = 8\n",
    "\n",
    "def get_model(index):\n",
    "    # HF model ID, model prefix, model architecture,  attn implementation, whether to use hf lib implementation\n",
    "    models = {\n",
    "        0: ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 'olmoe', None, True),\n",
    "        1: ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 'qwen2moe', None, True),\n",
    "        2: ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 'dsv2', None, True),\n",
    "        3: ('moonshotai/Moonlight-16B-A3B', 'moonlight', 'dsv3', None, True),\n",
    "        4: ('Qwen/Qwen3-30B-A3B-Instruct-2507', 'qwen3moe', 'qwen3moe', None, True),\n",
    "        5: ('moonshotai/Kimi-VL-A3B-Instruct', 'kimivl', 'kimivl', None, False),\n",
    "        6: ('ibm-granite/granite-4.0-tiny-preview', 'granite', 'granite', None, True),\n",
    "        7: ('zai-org/GLM-4.5-Air-FP8', 'glm4moe', 'glm4moe', None, True),\n",
    "        8: ('openai/gpt-oss-120b', 'gptoss120', 'gptoss', 'kernels-community/vllm-flash-attn3', True), # Will load experts in MXFP4 if triton kernels installed\n",
    "        9: ('openai/gpt-oss-20b', 'gptoss20', 'gptoss', 'kernels-community/vllm-flash-attn3', True)\n",
    "    }\n",
    "    return models[index]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from HF, or from file if already downloaded.\n",
    "    \"\"\"\n",
    "    cache_dir = '/workspace/hf'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir = cache_dir, torch_dtype = torch.bfloat16, trust_remote_code = not model_use_hf, device_map = 'auto', attn_implementation = model_attn).eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "model_id, model_prefix, model_architecture, model_attn, model_use_hf = get_model(selected_model_index)\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checks\n",
    "\"\"\"\n",
    "# # Quants\n",
    "# print(model.lm_head.weight)\n",
    "# print(model.model.embed_tokens.weight)\n",
    "\n",
    "# # Test model() call\n",
    "# inputs = tokenizer(['Test string'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 12).to(model.device)\n",
    "# with torch.no_grad():\n",
    "#     original_results = model(**inputs, use_cache = False)\n",
    "# print(original_results['logits'][0, :].detach().float().cpu().numpy())\n",
    "\n",
    "# # Test custom loader\n",
    "# import importlib\n",
    "# import utils.pretrained_models.dsv2 as test_mod   # the module object\n",
    "# test_mod = importlib.reload(test_mod)\n",
    "# run_model_return_topk = test_mod.run_dsv2_return_topk\n",
    "# custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = True)\n",
    "# print(custom_results['logits'][0, :].detach().float().cpu().numpy())\n",
    "\n",
    "# check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.layers[0].mlp.experts.down_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load reverse-engineered forward pass functions that return topk expert IDs and weights\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "    original_results = model(**inputs, use_cache = False)\n",
    "    custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = True)\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    assert len(custom_results['all_topk_experts']) == len(custom_results['all_topk_weights']), 'Length of topk IDs and weights not equal'\n",
    "    print(f\"Length of topk: {len(custom_results['all_topk_experts'])}\")\n",
    "    print(f\"Topk size: {custom_results['all_topk_experts'][0].shape}\")\n",
    "    print(f\"First token topk IDs: {custom_results['all_topk_experts'][0][1,]}\")\n",
    "    print(f\"First token topk weights: {custom_results['all_topk_weights'][0][1,]}\")\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), model.config.vocab_size).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "    print(f\"Hidden states layers (pre-mlp | post-layer): {len(custom_results['all_pre_mlp_hidden_states'])} | {len(custom_results['all_hidden_states'])}\")\n",
    "    print(f\"Hidden state size (pre-mlp | post-layer): {(custom_results['all_pre_mlp_hidden_states'][0].shape)} | {(custom_results['all_hidden_states'][0].shape)}\")\n",
    "    print(f\"Expert outputs : {(custom_results['all_expert_outputs'][0].shape)}\")\n",
    "    print(f\"Router logits : {(custom_results['all_router_logits'][0].shape)}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset - C4 + HLPT (en/zh/es)\n",
    "\"\"\"\n",
    "def load_raw_ds():\n",
    "    CACHE_FILE = '/workspace/data/c4_hlpt.jsonl'\n",
    "\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        print('Loading cached dataset...')\n",
    "        return load_dataset('json', data_files = CACHE_FILE, split = 'train').to_list()\n",
    "\n",
    "    rng = np.random.default_rng(seed = seed)\n",
    "\n",
    "    def get_hlpt(lang): # eng_Latn/zho_Hans/spa_Latn\n",
    "        return load_dataset('HPLT/HPLT2.0_cleaned', lang, split = 'train', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "\n",
    "    def get_c4(lang): # en/zh/esp\n",
    "        return load_dataset('allenai/c4', lang, split = 'validation', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "\n",
    "    def get_data(ds, n_samples, data_source): # en/zh/es\n",
    "        raw_data = []\n",
    "        ds_iter = iter(ds)\n",
    "        for _ in range(n_samples):\n",
    "            sample = next(ds_iter, None)\n",
    "            if sample is None:\n",
    "                break\n",
    "            raw_data.append({'text': sample['text'], 'source': data_source})\n",
    "\n",
    "        return raw_data\n",
    "    \n",
    "    combined_ds =\\\n",
    "        get_data(get_c4('en'), 750, 'en') + get_data(get_hlpt('eng_Latn'), 750, 'en') +\\\n",
    "        get_data(get_c4('zh'), 250, 'zh') + get_data(get_hlpt('zho_Hans'), 250, 'zh') +\\\n",
    "        get_data(get_c4('es'), 250, 'es') + get_data(get_hlpt('spa_Latn'), 250, 'es')\n",
    "\n",
    "    combined_ds = [combined_ds[i] for i in rng.permutation(len(combined_ds))]\n",
    "\n",
    "    print('Caching dataset to disk …')\n",
    "    Dataset.from_list(combined_ds).to_json(CACHE_FILE, orient = 'records', lines = True, force_ascii = False)\n",
    "\n",
    "    return combined_ds\n",
    "\n",
    "raw_data = load_raw_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader. The dataloader returns the original tokens - this is important for BPE tokenizers as otherwise it's difficult to reconstruct the correct string later!\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def chunk_list(input_list, max_length):\n",
    "    return [input_list[i:i + max_length] for i in range(0, len(input_list), max_length)]\n",
    "\n",
    "# Create and chunk into lists of size 250 each - these will be the export breaks\n",
    "test_dls = [\n",
    "    DataLoader(\n",
    "        ReconstructableTextDataset([x['text'] for x in data_chunk], tokenizer, max_length = 512, sources = [x['source'] for x in data_chunk]),\n",
    "        batch_size = 64,\n",
    "        shuffle = False,\n",
    "        collate_fn = stack_collate\n",
    "    )\n",
    "    for data_chunk in tqdm(chunk_list(raw_data, 250))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get expert selections + export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Run forward passes + export data\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def run_and_export_topk(model, model_prefix: str, dls: list[ReconstructableTextDataset], layers_to_keep_acts: list[int], layers_to_keep_experts = list[int], topk_to_keep: int = 0, max_batches: None | int = None):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the intermediate hidden layers as well as topks\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits`, `all_topk_experts`, `all_topk_weights`,\n",
    "          `all_pre_mlp_hidden_states`, and `all_expert_outputs`.\n",
    "        @model_prefix: The model prefix - used for file saving.\n",
    "        @dls: A list of dataloaders, each a ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layers_to_keep_acts: A list of layer indices (0-indexed) for which to filter `all_pre_mlp_hidden_states` (see returned object description).\n",
    "        @layers_to_keep_experts: A list of layer indices (0-indexed) for which to return topk-indices (see returned object description).\n",
    "        @topk_to_keep: How many of the topk-indices for which to return in `all_expert_outputs` (see returned object description).\n",
    "        @max_batches: The max number of batches to run.\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `topk_df`: A sample (token) x layer_ix x topk_ix level dataframe that gives the expert ID selected at each sample-layer-topk (removes masked_tokens)\n",
    "        - `all_router_logits`: A tensor of size n_samples x layers_to_keep_acts x n_experts with the pre-softmax router logits.\n",
    "        - `all_pre_mlp_hidden_states`: A tensor of size n_samples x layers_to_keep_acts x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "        - `all_expert_outputs` A tensor of size n_samples x layers_to_keep_experts x topk_to_keep x D to return the MLP output for all retained topk indices for each each retained layer.\n",
    "    \"\"\"\n",
    "    cross_dl_batch_ix = 0\n",
    "    output_dir = f'activations-sm/{model_prefix}'\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(f'{output_dir}/metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(\n",
    "            {'all_pre_mlp_hidden_states_layers': layers_to_keep_acts, 'all_expert_outputs_layers': layers_to_keep_experts},\n",
    "            f\n",
    "        )\n",
    "\n",
    "    # Iterate through dataloaders\n",
    "    for dl_ix, dl in enumerate(dls):\n",
    "        print(f\"Processing {str(dl_ix)} of {len(dls)}...\")   \n",
    "        dl_dir = f\"{output_dir}/{dl_ix:02d}\"\n",
    "        os.makedirs(dl_dir, exist_ok = True)\n",
    "\n",
    "        all_router_logits = []\n",
    "        all_pre_mlp_hidden_states = []\n",
    "        all_expert_outputs = []\n",
    "        sample_dfs = []\n",
    "        topk_dfs = []\n",
    "\n",
    "        for _, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "            input_ids = batch['input_ids'].to(main_device)\n",
    "            attention_mask = batch['attention_mask'].to(main_device)\n",
    "            original_tokens = batch['original_tokens']\n",
    "            sources = batch['sources']\n",
    "\n",
    "            output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "            # Check no bugs by validating output/perplexity\n",
    "            if cross_dl_batch_ix == 0:\n",
    "                loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), model.config.vocab_size).detach().cpu().item()\n",
    "                for i in range(min(10, input_ids.size(0))):\n",
    "                    decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = True)\n",
    "                    next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                    print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "                print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "            \n",
    "            original_tokens_df = pd.DataFrame(\n",
    "                [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "                columns = ['sequence_ix', 'token_ix', 'token']\n",
    "            )\n",
    "                    \n",
    "            sources_df = pd.DataFrame(\n",
    "                [(seq_i, seq_source) for seq_i, seq_source in enumerate(sources)], \n",
    "                columns = ['sequence_ix', 'source']\n",
    "            )\n",
    "\n",
    "            # Create sample (token) level dataframe\n",
    "            sample_df =\\\n",
    "                convert_outputs_to_df(input_ids, attention_mask, output['logits'])\\\n",
    "                .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "                .merge(sources_df, how = 'left', on = ['sequence_ix'])\\\n",
    "                .assign(batch_ix = cross_dl_batch_ix)\n",
    "\n",
    "            # Create topk x layer_ix x sample level dataframe\n",
    "            topk_df =\\\n",
    "                convert_topk_to_df(input_ids, attention_mask, output['all_topk_experts'], output['all_topk_weights'])\\\n",
    "                .assign(batch_ix = cross_dl_batch_ix, weight = lambda df: df['weight'])\\\n",
    "                .drop(columns = 'token_id')\n",
    "            \n",
    "            sample_dfs.append(sample_df)\n",
    "            topk_dfs.append(topk_df)\n",
    "\n",
    "            # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "            valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "            all_router_logits.append(torch.stack(output['all_router_logits'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "            all_pre_mlp_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep_acts, :])\n",
    "            all_expert_outputs.append(torch.stack(output['all_expert_outputs'], dim = 1)[valid_pos][:, layers_to_keep_experts, 0:topk_to_keep, :]) # This is BN x n_layers x topk x D - keep only top1 + top2\n",
    "\n",
    "            cross_dl_batch_ix += 1\n",
    "            if max_batches is not None and cross_dl_batch_ix >= max_batches:\n",
    "                pd.concat(sample_dfs, ignore_index = True).to_pickle(f'{dl_dir}/samples.pkl')\n",
    "                pd.concat(topk_dfs, ignore_index = True).to_pickle(f'{dl_dir}/topks.pkl')\n",
    "                torch.save(torch.cat(all_router_logits, dim = 0), f'{dl_dir}/all-router-logits.pt')\n",
    "                torch.save(torch.cat(all_pre_mlp_hidden_states, dim = 0), f'{dl_dir}/all-pre-mlp-hidden-states.pt')\n",
    "                torch.save(torch.cat(all_expert_outputs, dim = 0), f'{dl_dir}/all-expert-outputs.pt')\n",
    "                return True\n",
    "\n",
    "        pd.concat(sample_dfs, ignore_index = True).to_pickle(f'{dl_dir}/samples.pkl')\n",
    "        pd.concat(topk_dfs, ignore_index = True).to_pickle(f'{dl_dir}/topks.pkl')\n",
    "        torch.save(torch.cat(all_router_logits, dim = 0), f'{dl_dir}/all-router-logits.pt')\n",
    "        torch.save(torch.cat(all_pre_mlp_hidden_states, dim = 0), f'{dl_dir}/all-pre-mlp-hidden-states.pt')\n",
    "        torch.save(torch.cat(all_expert_outputs, dim = 0), f'{dl_dir}/all-expert-outputs.pt')\n",
    "\n",
    "    return True\n",
    "\n",
    "if model_prefix == 'olmoe':\n",
    "    layers_to_keep_acts = list(range(16))\n",
    "elif model_prefix == 'qwen1.5moe':\n",
    "    layers_to_keep_acts = list(range(24))\n",
    "elif model_prefix == 'dsv2':\n",
    "    layers_to_keep_acts = list(range(26))\n",
    "elif model_prefix == 'moonlight':\n",
    "    layers_to_keep_acts = list(range(26))\n",
    "elif model_prefix == 'qwen3moe':\n",
    "    layers_to_keep_acts = list(range(48))\n",
    "elif model_prefix == 'kimivl':\n",
    "    layers_to_keep_acts = list(range(26))\n",
    "elif model_prefix == 'granite':\n",
    "    layers_to_keep_acts = list(range(40))\n",
    "elif model_prefix == 'glm4moe':\n",
    "    layers_to_keep_acts = list(range(45))\n",
    "\n",
    "res = run_and_export_topk(\n",
    "    model,\n",
    "    model_prefix,\n",
    "    test_dls,\n",
    "    layers_to_keep_acts = layers_to_keep_acts,\n",
    "    layers_to_keep_experts = layers_to_keep_acts,\n",
    "    topk_to_keep = 1,\n",
    "    max_batches = None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
