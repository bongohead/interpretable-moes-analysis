{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Runs forward passes on samples and stores: (1) top-k expert selections; (2) sample metadata.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.store_outputs import convert_outputs_to_df\n",
    "from utils import pretrained_models\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\n",
    "Architectures supported currently:\n",
    "- OlMoE architecture, includes OLMoE-1B-7B-0125-Instruct (1B/7B)\n",
    "- Qwen2MoE architecture, inclues Qwen1.5-MoE-A2.7B-Chat (2.7B/14.3B), Qwen2-57B-A14B (14B/57B)\n",
    "- Deepseek v2 architecture, includes Deepseek-v2-Lite (2.4B/15.7B), Deepseek-v2 (21B/236B)\n",
    "- Deepseek v3 architecture, includes Deepseek-v3 (37B/671B), Deepseek-R1 (37B/671B), Moonlight-16B-A3B (3B/16B)\n",
    "- Qwen3MoE architecture, includes Qwen3-30B-A3B, Qwen3-235B-A22B\n",
    "\"\"\"\n",
    "selected_model_index = 4\n",
    "\n",
    "def get_model(index):\n",
    "    model = [\n",
    "        ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 'olmoe'),\n",
    "        ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 'qwen2moe'),\n",
    "        ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 'dsv2'),\n",
    "        ('moonshotai/Moonlight-16B-A3B', 'moonlight', 'dsv3'),\n",
    "        ('Qwen/Qwen3-30B-A3B', 'qwen3moe', 'qwen3moe')\n",
    "    ][index]\n",
    "\n",
    "    return model[0], model[1], model[2]\n",
    "\n",
    "model_id, model_prefix, model_architecture = get_model(selected_model_index)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load reverse-engineered forward pass functions that return topk expert IDs and weights\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "    original_results = model(**inputs)\n",
    "    custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'])\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    assert len(custom_results['all_topk_experts']) == len(custom_results['all_topk_weights']), 'Length of topk IDs and weights not equal'\n",
    "    print(f\"Length of topk: {len(custom_results['all_topk_experts'])}\")\n",
    "    print(f\"Topk size: {custom_results['all_topk_experts'][0].shape}\")\n",
    "    print(f\"First token topk IDs: {custom_results['all_topk_experts'][0][1,]}\")\n",
    "    print(f\"First token topk weights: {custom_results['all_topk_weights'][0][1,]}\")\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), model.config.vocab_size).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset (c4)\n",
    "\"\"\"\n",
    "def load_raw_ds():\n",
    "   \n",
    "    ds_en = load_dataset('allenai/c4', 'en', split = 'validation', streaming = True).shuffle(seed = seed, buffer_size = 100_000)\n",
    "    ds_zh = load_dataset('allenai/c4', 'zh', split = 'validation', streaming = True).shuffle(seed = seed, buffer_size = 100_000)\n",
    "    ds_es = load_dataset('allenai/c4', 'es', split = 'validation', streaming = True).shuffle(seed = seed, buffer_size = 100_000)\n",
    "    # ds = load_dataset('HuggingFaceFW/fineweb-edu', 'CC-MAIN-2024-51', split = 'train', streaming = True).shuffle(seed = 123, buffer_size = 1_000_000)\n",
    "\n",
    "    def get_data(ds, n_samples, data_source):\n",
    "        raw_data = []\n",
    "        ds_iter = iter(ds)\n",
    "        for _ in range(0, n_samples):\n",
    "            sample = next(ds_iter, None)\n",
    "            if sample is None:\n",
    "                break\n",
    "            raw_data.append({'text': sample['text'], 'source': data_source})\n",
    "        \n",
    "        return raw_data\n",
    "    \n",
    "    combined_ds = get_data(ds_en, 25_000, 'en') + get_data(ds_zh, 10_000, 'zh') + get_data(ds_es, 10_000, 'es')\n",
    "    \n",
    "    return combined_ds\n",
    "\n",
    "raw_data = load_raw_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader. The dataloader returns the original tokens - this is important for BPE tokenizers as otherwise it's difficult to reconstruct the correct string later!\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ReconstructableTextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text_dataset, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        Creates a dataset object that also returns a B x N list of the original tokens in the same position as the input ids.\n",
    "\n",
    "        Params:\n",
    "            @text_dataset: A list of B samples of text dataset. Each sample is a dict with keys `text` and `source`.\n",
    "            @tokenizer: A HF tokenizer object.\n",
    "        \"\"\"\n",
    "        texts = [x['text'] for x in text_dataset]\n",
    "        sources = [x['source'] for x in text_dataset]\n",
    "        tokenized = tokenizer(texts, add_special_tokens = False, max_length = max_length, padding = 'max_length', truncation = True, return_offsets_mapping = True, return_tensors = 'pt')\n",
    "\n",
    "        self.input_ids = tokenized['input_ids']\n",
    "        self.sources = sources\n",
    "        self.attention_mask = tokenized['attention_mask']\n",
    "        self.offset_mapping = tokenized['offset_mapping']\n",
    "        self.original_tokens = self.get_original_tokens(texts)\n",
    "\n",
    "    def get_original_tokens(self, texts):\n",
    "        \"\"\"\n",
    "        Return the original tokens associated with each B x N position. This is important for reconstructing the original text when BPE tokenizers are used.\n",
    "        \n",
    "        Params:\n",
    "            @input_ids: A B x N tensor of input ids.\n",
    "            @offset_mapping: A B x N x 2 tensor of offset mappings. Get from `tokenizer(..., return_offsets_mapping = True)`.\n",
    "\n",
    "        Returns:\n",
    "            A list of length B, each with length N, containing the corresponding original tokens corresponding to the token ID at the same position of input_ids.\n",
    "        \"\"\"\n",
    "        all_token_substrings = []\n",
    "        for i in range(0, self.input_ids.shape[0]):\n",
    "            token_substrings = []\n",
    "            for j in range(self.input_ids.shape[1]): \n",
    "                start_char, end_char = self.offset_mapping[i][j].tolist()\n",
    "                if start_char == 0 and end_char == 0: # When pads, offset_mapping might be [0, 0], so let's store an empty string for those positions.\n",
    "                    token_substrings.append(\"\")\n",
    "                else:\n",
    "                    original_substring = texts[i][start_char:end_char]\n",
    "                    token_substrings.append(original_substring)\n",
    "            \n",
    "            all_token_substrings.append(token_substrings)\n",
    "\n",
    "        return all_token_substrings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx], 'original_tokens': self.original_tokens[idx], 'sources': self.sources[idx]}\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function; necessary to return original_tokens in the correct shape \n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([b['input_ids'] for b in batch], dim = 0)\n",
    "    attention_mask = torch.stack([b['attention_mask'] for b in batch], dim = 0)        \n",
    "    original_tokens = [b['original_tokens'] for b in batch]\n",
    "    sources = [b['sources'] for b in batch]\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'original_tokens': original_tokens, 'sources': sources}\n",
    "\n",
    "\n",
    "def chunk_list(input_list, max_length):\n",
    "    return [input_list[i:i + max_length] for i in range(0, len(input_list), max_length)]\n",
    "\n",
    "# Create and chunk into lists of size 1000 each - these will be the export breaks\n",
    "test_dls = [\n",
    "    DataLoader(\n",
    "        ReconstructableTextDataset(x, tokenizer, max_length = 1024),\n",
    "        batch_size = 4,\n",
    "        shuffle = False,\n",
    "        collate_fn = collate_fn\n",
    "    )\n",
    "    for x in tqdm(chunk_list(raw_data, 5_000))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get expert selections + export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Run forward passes + export data\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_and_export_topk(model, model_prefix, dls: list[ReconstructableTextDataset], max_batches: None | int = None):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and saves the top-k expert ids and sample information after every dataloader\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits`, `all_topk_experts`, `all_topk_weights`.\n",
    "        @model_prefix: The model prefix - used for file saving.\n",
    "        @dls: A list of dataloaders, each a ReconstructableTextDataset of which returns `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @max_batches: The max number of batches to run.\n",
    "\n",
    "    Returns:\n",
    "        True on completion\n",
    "    \"\"\"\n",
    "    cross_dl_batch_ix = 0\n",
    "    output_dir = f'topks/{model_prefix}'\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "    for dl_ix, dl in enumerate(dls):\n",
    "\n",
    "        sample_dfs = []\n",
    "        topk_dfs = []\n",
    "\n",
    "        for _, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "            input_ids = batch['input_ids'].to(main_device)\n",
    "            attention_mask = batch['attention_mask'].to(main_device)\n",
    "            original_tokens = batch['original_tokens']\n",
    "            sources = batch['sources']\n",
    "\n",
    "            output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = False)\n",
    "\n",
    "            # Check no bugs by validating output/perplexity\n",
    "            if cross_dl_batch_ix == 0:\n",
    "                loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), model.config.vocab_size).detach().cpu().item()\n",
    "                for i in range(min(5, input_ids.size(0))):\n",
    "                    decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = True)\n",
    "                    next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                    print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "                print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "            \n",
    "            original_tokens_df = pd.DataFrame(\n",
    "                [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "                columns = ['sequence_ix', 'token_ix', 'token']\n",
    "            )\n",
    "                    \n",
    "            sources_df = pd.DataFrame(\n",
    "                [(seq_i, seq_source) for seq_i, seq_source in enumerate(sources)], \n",
    "                columns = ['sequence_ix', 'source']\n",
    "            )\n",
    "\n",
    "            # Create sample (token) level dataframe\n",
    "            sample_df =\\\n",
    "                convert_outputs_to_df(input_ids, attention_mask, output['logits'])\\\n",
    "                .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "                .merge(sources_df, how = 'left', on = ['sequence_ix'])\\\n",
    "                .assign(batch_ix = cross_dl_batch_ix)\n",
    "\n",
    "            # Create topk x layer_ix x sample level dataframe\n",
    "            topk_df =\\\n",
    "                convert_topk_to_df(input_ids, attention_mask, output['all_topk_experts'], output['all_topk_weights'])\\\n",
    "                .assign(batch_ix = cross_dl_batch_ix, weight = lambda df: df['weight'])\\\n",
    "                .drop(columns = 'token_id')#\n",
    "            \n",
    "            sample_dfs.append(sample_df)\n",
    "            topk_dfs.append(topk_df)\n",
    "\n",
    "            cross_dl_batch_ix += 1\n",
    "            if max_batches is not None and cross_dl_batch_ix >= max_batches:\n",
    "                combined_sample_df = pd.concat(sample_dfs, ignore_index = True)\n",
    "                combined_topk_df = pd.concat(topk_dfs, ignore_index = True)\n",
    "                combined_sample_df.to_csv(f'{output_dir}/samples.csv', mode = 'w' if dl_ix == 0 else 'a', index = False, header = (dl_ix == 0))\n",
    "                combined_topk_df.to_csv(f'{output_dir}/topks.csv', mode = 'w' if dl_ix == 0 else 'a', index = False, header = (dl_ix == 0))\n",
    "                combined_topk_df[combined_topk_df['topk_ix'] == 1].to_csv(f'{output_dir}/topk1s.csv', mode = 'w' if dl_ix == 0 else 'a', index = False, header = (dl_ix == 0))\n",
    "                return True\n",
    "\n",
    "        if sample_dfs:\n",
    "            combined_sample_df = pd.concat(sample_dfs, ignore_index = True)\n",
    "            combined_topk_df = pd.concat(topk_dfs, ignore_index = True)\n",
    "            combined_sample_df.to_csv(f'{output_dir}/samples.csv', mode = 'w' if dl_ix == 0 else 'a', index = False, header = (dl_ix == 0))\n",
    "            combined_topk_df.to_csv(f'{output_dir}/topks.csv', mode = 'w' if dl_ix == 0 else 'a', index = False, header = (dl_ix == 0))\n",
    "            combined_topk_df[combined_topk_df['topk_ix'] == 1].to_csv(f'{output_dir}/topk1s.csv', mode = 'w' if dl_ix == 0 else 'a', index = False, header = (dl_ix == 0))\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "run_and_export_topk(model, model_prefix, test_dls, max_batches = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
