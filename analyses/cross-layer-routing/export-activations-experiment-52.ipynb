{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "import math\n",
    "from helpers.memory import check_memory, profile_memory, clear_all_cuda_memory\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import json\n",
    "import dill\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a conf with configurable model settings.\n",
    "- These will be passed into the model class during model initialization, so add new confs needed for whatever architecture is used.\n",
    "- If you use the default conf values with the default model class defined later, it will exactly replicate the OlMoE-7B model,\n",
    "   with 7B total params/1B active/64 experts.\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class ModelConf:\n",
    "    \"\"\"\n",
    "    General config settings for this MoE\n",
    "    \"\"\"\n",
    "    vocab_size: int = 50304 # Base OlMoE: 50304 (vocab size)\n",
    "    D: int = 2048 # Base OlMoE: 2048 (hidden state dimension)\n",
    "    H: int = 16 # Base OlMoE: 16 (number of attention heads)\n",
    "    I: int = 1024 # Base OlMoE: 1024 (expert MLP dimension)\n",
    "    n_experts: int = 64 # Base OlMoE: 64 (non-shared experts only)\n",
    "    n_shared_experts: int = 0 # Base OlMoE: 0 (base OlMoE doesn't support shared experts, but may help with inducing expert specialization - see Deepseek paper)\n",
    "    top_k: int = 8 # Base OlMoE: 8 \n",
    "    norm_topk_prob: bool = False # Base OlMoE: false (whether to normalize so that expert weights sum to 1 after topk)\n",
    "    padding_idx: int = 1 # Base OlMoE: 1 (index where padding gets mapped to)\n",
    "    n_layers: int = 16 # Base OlMoE: 16 (transformer layers)\n",
    "    rms_norm_eps: float = 1e-05 # Base OlMoE: 1e-05\n",
    "    rope_theta: float = 10000.0 # Base OlMoe: 10000.0 (this is something needed for ROPE)\n",
    "    max_position_embeddings: int = 4096 # Base OlMoE: 4096 (this is something needed for ROPE)\n",
    "    attn_method: str = 'fa2' # In OlMoE this is chosen automatically, here we explicitly pass it - choose 'normal', 'sdpa', or 'fa2'\n",
    "    n_dense_layers: int = 0 # Base OlMoE: 0 (number of dense layers to include before the MoE layers)\n",
    "    dense_layer_I: int = 4096 # Base OlMoE: NA (intermediate MLP dimension for the dense layers)\n",
    "    Z: int = 64 # Dimension for the routing residual stream (RRS)    \n",
    "    \n",
    "conf = ModelConf(\n",
    "    D = 768,\n",
    "    H = 8,\n",
    "    I = int(640),\n",
    "    n_layers = 18,\n",
    "    n_experts = 64,\n",
    "    n_shared_experts = 2,\n",
    "    top_k = 4,\n",
    "    max_position_embeddings = 2048,\n",
    "    n_dense_layers = 1,\n",
    "    dense_layer_I = int(640 * 6),\n",
    "    Z = 32,\n",
    "    attn_method = 'sdpa'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper funs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "These is a dump of helper functions called by the model layers, needed to make forward/backward passes correctly.\n",
    "- `_prepare_4d_causal_attention_mask_with_cache_position` is used to create the upper-triangular infinity mask for attention (not used by flash attention).\n",
    "- `load_balancing_loss_func` is the usual load balancing function.\n",
    "- Add any new functions here if needed, but most experiments won't need to touch this section.\n",
    "\"\"\"\n",
    "\n",
    "# Create the upper-trangular matrix of infinities to mask future tokens in the attention softmax (needed for SDPA + normal attention)\n",
    "# Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L1099C1-L1152 \n",
    "def _prepare_4d_causal_attention_mask_with_cache_position(attention_mask: torch.Tensor, sequence_length: int, target_length: int, dtype: torch.dtype, device: torch.device, cache_position: torch.Tensor, batch_size: int):\n",
    "    if attention_mask is not None and attention_mask.dim() == 4:\n",
    "        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n",
    "        causal_mask = attention_mask\n",
    "    else:\n",
    "        min_dtype = torch.finfo(dtype).min\n",
    "        causal_mask = torch.full((sequence_length, target_length), fill_value = min_dtype, dtype=dtype, device=device)\n",
    "        if sequence_length != 1:\n",
    "            causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n",
    "        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
    "            mask_length = attention_mask.shape[-1]\n",
    "            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n",
    "            padding_mask = padding_mask == 0\n",
    "            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n",
    "                padding_mask, min_dtype\n",
    "            )\n",
    "    return causal_mask\n",
    "\n",
    "# Load balancing loss, copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py\n",
    "def load_balancing_loss_func(gate_logits, num_experts, top_k, attention_mask):\n",
    "    compute_device = gate_logits[0].device\n",
    "    concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim = 0)\n",
    "    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n",
    "    if attention_mask is None:\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "    else:\n",
    "        batch_size, sequence_length = attention_mask.shape\n",
    "        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n",
    "        expert_attention_mask = (attention_mask[None, :, :, None, None].expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts)).reshape(-1, top_k, num_experts).to(compute_device))\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(expert_attention_mask, dim=0)\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n",
    "        router_per_expert_attention_mask = (attention_mask[None, :, :, None].expand((num_hidden_layers, batch_size, sequence_length, num_experts)).reshape(-1, num_experts).to(compute_device))\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(router_per_expert_attention_mask, dim=0)\n",
    "    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n",
    "    return overall_loss * num_experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "First let's define the RMSNorm, ROPE, and self-attention layers.\n",
    "- These are basically taken straight from the OlMoE source code, but heavily simplified/cleaned up.\n",
    "- Note that RMSNorm is the ONLY norm type we define (same as OlMoE).\n",
    "- These layers generally do not need to be modified for MoE experiments.\n",
    "\"\"\"\n",
    "from transformers.modeling_flash_attention_utils import _flash_attention_forward # Flash attention forward\n",
    "\n",
    "class OlmoeRMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply RMS Norm\n",
    "    - Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L137-L154\n",
    "    - This is the only norm used in OlMoE!\n",
    "      - It's used 4 times per layer (attention key norm, attention query norm, layer residual pre-attention norm, post-attention norm)\n",
    "      - Also one additional time before the final LM head \n",
    "    \"\"\"\n",
    "    def __init__(self, D, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(D))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim = True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class OlmoeRotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Get sin/cos ROPE embeddings\n",
    "    - Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L161-L219\n",
    "    - Code has been simplified heavily since we're not using dynamic ROPE scaling\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf):\n",
    "        super().__init__()\n",
    "        dim = int(conf.D/conf.H)\n",
    "        inv_freq = 1.0 / (conf.rope_theta ** (torch.arange(0, dim, 2, dtype = torch.int64).float()/dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent = False)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        # Core RoPE block\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type = device_type, enabled = False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim = -1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        return cos.to(dtype = x.dtype), sin.to(dtype = x.dtype)\n",
    "\n",
    "class OlmoeAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention implementation\n",
    "    - Modified from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L288-L391\n",
    "    - Simplfied to handle base attention/sdpa/flash attention within this one class\n",
    "    - Also doesn't support GQA (OlMoE doesn't use anyways)\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf):\n",
    "        super().__init__()\n",
    "        self.attn_method = conf.attn_method\n",
    "        self.D = conf.D # Hidden state dim\n",
    "        self.H = conf.H # Num of attention heads\n",
    "        self.Dh = int(conf.D/conf.H) # Dimensions per head\n",
    "        \n",
    "        # Initialize attention layers - no biases following OlMoE architecture\n",
    "        self.q_proj = nn.Linear(self.D, self.H * self.Dh, bias = False)\n",
    "        self.k_proj = nn.Linear(self.D, self.H * self.Dh, bias = False)\n",
    "        self.v_proj = nn.Linear(self.D, self.H * self.Dh, bias = False)\n",
    "        self.o_proj = nn.Linear(self.D, self.D, bias = False)\n",
    "        self.q_norm = OlmoeRMSNorm(self.D, eps = conf.rms_norm_eps)\n",
    "        self.k_norm = OlmoeRMSNorm(self.D, eps = conf.rms_norm_eps)\n",
    "\n",
    "    # Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L223-L255\n",
    "    def apply_rotary_pos_emb(self, q, k, cos, sin, unsqueeze_dim = 1):\n",
    "        def rotate_half(x):\n",
    "            \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "            x1 = x[..., : x.shape[-1] // 2]\n",
    "            x2 = x[..., x.shape[-1] // 2 :]\n",
    "            return torch.cat((-x2, x1), dim=-1)\n",
    "            \n",
    "        cos = cos.unsqueeze(unsqueeze_dim)\n",
    "        sin = sin.unsqueeze(unsqueeze_dim)\n",
    "        q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "        return q_embed, k_embed\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor, position_ids: torch.LongTensor, position_embeddings: tuple[torch.Tensor, torch.Tensor]):\n",
    "        \n",
    "        B, N , D = hidden_state.shape\n",
    "\n",
    "        query_state = self.q_norm(self.q_proj(hidden_state)).view(B, N, self.H, self.Dh).transpose(1, 2) # B x N x 2048\n",
    "        key_state = self.k_norm(self.k_proj(hidden_state)).view(B, N, self.H, self.Dh).transpose(1, 2) # B x N x 2048\n",
    "        value_state = self.v_proj(hidden_state).view(B, N, self.H, self.Dh).transpose(1, 2) # B x N x 2048\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_state, key_state = self.apply_rotary_pos_emb(query_state, key_state, cos, sin)\n",
    "        \n",
    "        if self.attn_method == 'normal':\n",
    "            attn_weights = torch.matmul(query_state, key_state.transpose(2, 3))/math.sqrt(self.Dh)  # Should be shape B x H x N x N\n",
    "            attn_weights = attn_weights + attention_mask # Attention mask is upper triangular of negative infinity\n",
    "            attn_weights = F.softmax(attn_weights, dim = -1, dtype = torch.float32).to(query_state.dtype)\n",
    "            attn_output = torch.matmul(attn_weights, value_state) # B x H x N x D/H\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous() # Reorder into B x N x H x D/H\n",
    "            attn_output = attn_output.reshape(B, N, D) # Concatenate vertically back into B x N x D\n",
    "            \n",
    "        elif self.attn_method == 'sdpa':\n",
    "            attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "                query_state, key_state, value_state,\n",
    "                attention_mask, dropout_p = 0.0, is_causal = True\n",
    "            )\n",
    "            attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "            attn_output = attn_output.view(B, N, D)\n",
    "            \n",
    "        elif self.attn_method == 'fa2':\n",
    "            query_state = query_state.transpose(1, 2)\n",
    "            key_state = key_state.transpose(1, 2)\n",
    "            value_state = value_state.transpose(1, 2)\n",
    "            attn_output = _flash_attention_forward(\n",
    "                query_state, key_state, value_state,\n",
    "                attention_mask, N, dropout = 0.0, use_top_left_mask = False, is_causal = True\n",
    "            )\n",
    "            attn_output = attn_output.reshape(B, N, D).contiguous()\n",
    "            \n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Now let's define the MLP layer and the MoE layer.\n",
    "- The MLP layer is simple; modify as needed.\n",
    "- However, the MoE layer is much more complex, and this layer will probably need to be modified heavily for most experiments.\n",
    "  - By default, I've defined three forward methods here. As currently implemented, they all generate IDENTICAL outputs but become increasingly more efficient yet complex.\n",
    "    - `forward_slow` is the most straightforward implementation (similar to the original OlMoE code). It is also the fastest for single-GPU, limited experts (32 or less) operations.\n",
    "    - `forward_fast` is faster for large # experts, as it places all the relevant states for a single expert to be continguous in memory. For single GPU, it reaches parity w/forward_slow at ~64 experts.\n",
    "    - `forward_async` is faster for large GPU counts + large # of experts, as it batches all experts who belong on one device together, and also runs them all asynchronously.\n",
    "    - For initial testing, it's probably best to modify just `forward_slow`, and only modify the others once you want to run a large-scale training run.\n",
    "  - Each forward method must return a tuple where the first element is the B x N x D MoE layer output, and the second element is the router logits. \n",
    "    - To return more, you'll need to also modify the transformer layer class in the next section.\n",
    "\"\"\"\n",
    "from transformers.activations import silu\n",
    "\n",
    "class OlmoeMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Individual expert MLP\n",
    "    - Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L258-L272\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf):\n",
    "        super().__init__()\n",
    "        self.conf = conf\n",
    "        self.D = conf.D\n",
    "        self.I = conf.I\n",
    "        self.gate_proj = nn.Linear(self.D, self.I, bias = False)\n",
    "        self.up_proj = nn.Linear(self.D, self.I, bias = False)\n",
    "        self.down_proj = nn.Linear(self.I, self.D, bias = False)\n",
    "        self.act_fn = silu\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "\n",
    "class OlmoeMoe(nn.Module):\n",
    "    \"\"\"\n",
    "    Entire MLP layer including router\n",
    "    - Modified from https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmoe/modeling_olmoe.py#L604-L649\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf, shared_gate: nn.Linear):\n",
    "        super().__init__()\n",
    "        self.n_experts = conf.n_experts\n",
    "        self.top_k = conf.top_k\n",
    "        self.norm_topk_prob = conf.norm_topk_prob\n",
    "        self.n_shared_experts = conf.n_shared_experts\n",
    "\n",
    "        # The globally shared gating (created in top-level model)\n",
    "        self.shared_gate = shared_gate  # Not newly created here, an inherited reference\n",
    "        self.gate_offset = nn.Linear(conf.Z, self.n_experts, bias = False) # Per-layer offset gating (this is what will be penalized)\n",
    "\n",
    "        self.experts = nn.ModuleList([OlmoeMLP(conf) for _ in range(self.n_experts)]) # Create experts using OlmoeMLP\n",
    "        self.shared_experts = nn.ModuleList([OlmoeMLP(conf) for _ in range(self.n_shared_experts)])\n",
    "        \n",
    "        # Loss-free load balancing bias ----------------\n",
    "        # We store a bias value b_i for each expert i.  These are NOT updated by backprop.\n",
    "        self.register_buffer(\"expert_biases\", torch.zeros(self.n_experts))\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, rrs_state: torch.Tensor, moe_method: str, use_lflb: bool = False) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward method routes to one of several possible other forward methods\n",
    "        \"\"\"\n",
    "        if moe_method == 'forward_slow':\n",
    "            moe_output, router_logits, topk_expert_ids = self.forward_slow(hidden_state, rrs_state, use_lflb)\n",
    "        elif moe_method == 'forward_fast':\n",
    "            moe_output, router_logits, topk_expert_ids = self.forward_fast(hidden_state, rrs_state, use_lflb)\n",
    "        elif moe_method == 'forward_async':\n",
    "            moe_output, router_logits, topk_expert_ids = self.forward_async(hidden_state, rrs_state, use_lflb)\n",
    "        else:\n",
    "            raise ValueError(f'Method \"{moe_method}\" not implemented.')\n",
    "        \n",
    "        # Add output from shared experts\n",
    "        shared_total = torch.zeros_like(moe_output)\n",
    "        for shared_expert in self.shared_experts:\n",
    "            shared_total += shared_expert(hidden_state)\n",
    "\n",
    "        mlp_output = moe_output + shared_total\n",
    "        return mlp_output, router_logits, topk_expert_ids\n",
    "        \n",
    "    # -------------------- LOSS-FREE BIAS UPDATE FUNCTION --------------------\n",
    "    @torch.no_grad()\n",
    "    def update_expert_biases(self, usage_counts: dict[int, int], bias_update_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Given a dict mapping expert_id -> usage_count for this batch, push biases up/down to move usage closer to uniform.\n",
    "        \"\"\"\n",
    "        total_tokens = sum(usage_counts.values())\n",
    "        mean_usage = total_tokens / self.n_experts\n",
    "\n",
    "        for ex_id in range(self.n_experts):\n",
    "            ex_count = usage_counts.get(ex_id, 0) # usage_counts might not have an entry for an expert if it got 0 tokens\n",
    "            diff = mean_usage - ex_count # e_i = mean_usage - ex_count\n",
    "\n",
    "            # Sign-based udpate\n",
    "            if diff > 0:\n",
    "                self.expert_biases[ex_id] += bias_update_rate\n",
    "            elif diff < 0:\n",
    "                self.expert_biases[ex_id] -= bias_update_rate\n",
    "                \n",
    "        # Optionally: clamp the biases to avoid extreme values\n",
    "        self.expert_biases.clamp_(-5.0, 5.0)\n",
    "                \n",
    "    def forward_slow(self, hidden_state: torch.Tensor, rrs_state: torch.Tensor, use_lflb: bool) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        This is the more intuitive forward pass which loops through each expert slowly\n",
    "        \"\"\"\n",
    "        B, N, D = hidden_state.shape\n",
    "\n",
    "        # Flatten out B x N x D to BN x D (flattened token-level reps) to route all tokens seperately\n",
    "        hidden_state = hidden_state.view(B * N, D) # Flatten out B x N x D to BN x D (flattened token-level reps) to route all tokens seperately\n",
    "        rrs_state = rrs_state.view(B * N, -1) # BN x Z\n",
    "\n",
    "        # 1) ---------------- Compute router logits -> choose topk experts and weights ----------------\n",
    "        router_logits = self.shared_gate(rrs_state) + self.gate_offset(rrs_state) # (BN, n_experts) - routing probability for each token\n",
    "        routing_weights = F.softmax(router_logits, dim = 1, dtype = torch.float)  # (BN, n_experts)\n",
    "\n",
    "        # topk_weights and topk_expert_ids  represents for each token, the top k experts and the corresponding weights/expert indices\n",
    "        if use_lflb:\n",
    "            router_logits = router_logits + self.expert_biases.unsqueeze(0)\n",
    "            _, topk_expert_ids  = torch.topk(router_logits, self.top_k, dim = -1) # (BN, top_k)\n",
    "            topk_weights = torch.gather(routing_weights, 1, topk_expert_ids ) # (BN, top_k)\n",
    "        else:\n",
    "            topk_weights, topk_expert_ids  = torch.topk(routing_weights, self.top_k, dim = -1) # both (BN, top_k)\n",
    "\n",
    "        topk_weights = topk_weights.to(hidden_state.dtype)\n",
    "        # Optional renormalization if you want the top-k weights to sum to 1\n",
    "        if self.norm_topk_prob:\n",
    "            topk_weights /= (topk_weights.sum(dim = -1, keepdim = True) + 1e-9)\n",
    "\n",
    "        # 2) ---------------- One hot encode - for each expert, which topk x token is active - e.g. expert_assignment_mask[0, :] will be 0s if the first expert is never chosen ----------------\n",
    "        expert_assignment_mask = F.one_hot(topk_expert_ids , num_classes = self.n_experts).permute(2, 1, 0) # Creates (N_EXPERTS, TOP_K, BN)\n",
    "        \n",
    "        # 3) ---------------- Iterate through all the experts, apply each expert to the tokens where the expert are relevant, multiple output by the weights for the topk/token for that expert ----------------\n",
    "        # Initialize output buffer\n",
    "        mlp_output = torch.zeros((B * N, D), dtype = hidden_state.dtype, device = hidden_state.device) # Initialize MLP output - later iterate through experts and sum onto this object\n",
    "        # Iterate\n",
    "        for expert_ix, expert in enumerate(self.experts):\n",
    "            \n",
    "            # For this expert, gives the (topk, token) coordinates which uses the expert\n",
    "            topk_slot, token_indices = torch.where(expert_assignment_mask[expert_ix, :])\n",
    "            if token_indices.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # Gather input tokens for this expert\n",
    "            tokens_for_expert = hidden_state[token_indices, :] # (num_assigned_tokens, D)\n",
    "            \n",
    "            # Move to the expert's device\n",
    "            expert_device = next(self.experts[expert_ix].parameters()).device # Get the device this expert lives on\n",
    "            tokens_for_expert = tokens_for_expert.to(expert_device) # Move to expert device\n",
    "\n",
    "            # Forward through expert\n",
    "            expert_output = expert(tokens_for_expert)\n",
    "          \n",
    "            # For each num_assigned_tokens, multiples it by the corresponding weight in topk_slot fort that token_index\n",
    "            expert_output = expert_output * topk_weights[token_indices, topk_slot].unsqueeze(1).to(expert_device)\n",
    "\n",
    "            # Move back to original device and acucmulate into mlp output\n",
    "            expert_output = expert_output.to(mlp_output.device)\n",
    "            mlp_output.index_add_(0, token_indices, expert_output.to(hidden_state.dtype))\n",
    "\n",
    "        mlp_output = mlp_output.reshape(B, N, D) # Convert back from BN x D -> B x N x D\n",
    "        return mlp_output, router_logits, topk_expert_ids\n",
    "    \n",
    "\n",
    "    def forward_fast(self, hidden_state: torch.Tensor, rrs_state: torch.Tensor, use_lflb: bool) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Efficient MoE routing that batches tokens for each expert in a single pass using gather -> scatter operations. \n",
    "        - This will be much faster for a large number of experts, but possibly lower for low expert counts.\n",
    "        \"\"\"\n",
    "        B, N, D = hidden_state.shape\n",
    "\n",
    "        # Flatten out B x N x D to BN x D (flattened token-level reps) to route all tokens seperately\n",
    "        hidden_state = hidden_state.view(B * N, D) # Flatten out B x N x D to BN x D (flattened token-level reps) to route all tokens seperately\n",
    "        rrs_state = rrs_state.view(B * N, -1) # BN x Z\n",
    "\n",
    "        # 1) ---------------- Compute router logits -> choose topk experts and weights ----------------\n",
    "        router_logits = self.shared_gate(rrs_state) + self.gate_offset(rrs_state) # (BN, n_experts) - routing probability for each token\n",
    "        routing_weights = F.softmax(router_logits, dim = 1, dtype = torch.float)  # (BN, n_experts)\n",
    "\n",
    "        # topk_weights and topk_expert_ids  represents for each token, the top k experts and the corresponding weights/expert indices\n",
    "        if use_lflb:\n",
    "            router_logits = router_logits + self.expert_biases.unsqueeze(0)\n",
    "            _, topk_expert_ids = torch.topk(router_logits, self.top_k, dim = -1) # (BN, top_k)\n",
    "            topk_weights = torch.gather(routing_weights, 1, topk_expert_ids) # (BN, top_k)\n",
    "        else:\n",
    "            topk_weights, topk_expert_ids = torch.topk(routing_weights, self.top_k, dim = -1) # both (BN, top_k)\n",
    "\n",
    "        # 2) ---------------- Flatten topk results so we can later gather tokens for each expert cleanly ----------------\n",
    "        topk_expert_ids = topk_expert_ids.view(-1)\n",
    "        topk_weights = topk_weights.view(-1)\n",
    "\n",
    "        # [0..BN-1], repeated top_k times (one for each token-expert pair)\n",
    "        token_indices_flat = torch.arange(B * N, device = hidden_state.device).unsqueeze(1).expand(B * N, self.top_k).reshape(-1) # shape = (BN * top_k,)\n",
    "\n",
    "        # 3) ---------------- Sort by expert index so we can process contiguous tokens for each expert ----------------\n",
    "        sorted_expert_ids, expert_sort_indices = torch.sort(topk_expert_ids)\n",
    "        sorted_token_indices = token_indices_flat[expert_sort_indices]\n",
    "        sorted_weights = topk_weights[expert_sort_indices]\n",
    "        sorted_inputs = hidden_state[sorted_token_indices]  # shape = (BN*top_k, D)\n",
    "\n",
    "        # 4) ---------------- Walk through sorted_experts to find contiguous segments belonging to each expert. We can use torch.unique_consecutive to figure out segment boundaries ----------------\n",
    "        # Initialize output buffer\n",
    "        mlp_output = torch.zeros((B * N, D), dtype = hidden_state.dtype, device = hidden_state.device) # Initialize MLP output - later iterate through experts and sum onto this object\n",
    "        unique_expert_ids, counts = torch.unique_consecutive(sorted_expert_ids, return_counts = True)\n",
    "        # Iterate\n",
    "        start_offset = 0\n",
    "        for expert_id, count_for_this_expert in zip(unique_expert_ids, counts):\n",
    "            end_offset = start_offset + count_for_this_expert # The chunk [start_offset : end_offset] corresponds to all tokens for this expert\n",
    "\n",
    "            # Indices for this chunk\n",
    "            chunk_token_indices = sorted_token_indices[start_offset:end_offset] # (count, D)\n",
    "            chunk_weights = sorted_weights[start_offset:end_offset].unsqueeze(1) # (count, 1)\n",
    "            chunk_inputs = sorted_inputs[start_offset:end_offset] # (count, )\n",
    "\n",
    "            start_offset = end_offset\n",
    "\n",
    "            # Move to device\n",
    "            expert_device = next(self.experts[expert_id].parameters()).device\n",
    "            chunk_inputs = chunk_inputs.to(expert_device)\n",
    "            chunk_weights = chunk_weights.to(expert_device)\n",
    "\n",
    "            # Forward pass through this expert\n",
    "            expert = self.experts[expert_id]\n",
    "            expert_output = expert(chunk_inputs) # (count, D)\n",
    "\n",
    "            # Multiply by the top-k gate weight\n",
    "            expert_output = expert_output * chunk_weights\n",
    "\n",
    "            # Bring it back to the main device and scatter-add back to the correct token positions\n",
    "            expert_output = expert_output.to(mlp_output.device)\n",
    "            mlp_output.index_add_(0, chunk_token_indices, expert_output.to(hidden_state.dtype))\n",
    "\n",
    "        mlp_output = mlp_output.reshape(B, N, D) # Convert back from BN x D -> B x N x D\n",
    "        return mlp_output, router_logits, topk_expert_ids\n",
    "    \n",
    "\n",
    "    def forward_async(self, hidden_state: torch.Tensor, rrs_state: torch.Tensor, use_lflb: bool) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Async MoE forward for optimal multi-GPU speeds that:\n",
    "          1) Flattens tokens\n",
    "          2) Does gating + top-k\n",
    "          3) Sorts by (device, expert)\n",
    "          4) Groups tokens by device, then sub-groups by expert\n",
    "          5) Uses asynchronous CUDA streams to overlap transfers & expert compute\n",
    "          6) Accumulates outputs back to the main device\n",
    "        This is ~2.5x faster than forward_slow with 64 experts on 4 GPUs\n",
    "        \"\"\"\n",
    "        B, N, D = hidden_state.shape        \n",
    "        main_device = hidden_state.device\n",
    "        expert_device_map = [str(next(expert.parameters()).device) for expert in self.experts]\n",
    "\n",
    "        # Flatten out B x N x D to BN x D (flattened token-level reps) to route all tokens seperately\n",
    "        hidden_state = hidden_state.view(B * N, D) # Flatten out B x N x D to BN x D (flattened token-level reps) to route all tokens seperately\n",
    "        rrs_state = rrs_state.view(B * N, -1) # BN x Z\n",
    "        \n",
    "        # ---------------- 1) Compute router logits -> choose topk experts and weights ----------------\n",
    "        router_logits = self.shared_gate(rrs_state) + self.gate_offset(rrs_state) # (BN, n_experts) - routing probability for each token\n",
    "        routing_weights = F.softmax(router_logits, dim = 1, dtype = torch.float)  # (BN, n_experts)\n",
    "\n",
    "        # topk_weights and topk_expert_ids  represents for each token, the top k experts and the corresponding weights/expert indices\n",
    "        if use_lflb:\n",
    "            router_logits = router_logits + self.expert_biases.unsqueeze(0)\n",
    "            _, topk_expert_ids = torch.topk(router_logits, self.top_k, dim = -1) # (BN, top_k)\n",
    "            topk_weights = torch.gather(routing_weights, 1, topk_expert_ids) # (BN, top_k)\n",
    "        else:\n",
    "            topk_weights, topk_expert_ids = torch.topk(routing_weights, self.top_k, dim = -1) # both (BN, top_k)\n",
    "\n",
    "        # ---------------- 2) Flatten topk results so we can later gather tokens for each expert cleanly ----------------\n",
    "        topk_expert_ids = topk_expert_ids.view(-1)\n",
    "        topk_weights = topk_weights.view(-1)\n",
    "\n",
    "        # [0..BN-1], repeated top_k times (one for each token-expert pair)\n",
    "        token_indices_flat = torch.arange(B * N, device = hidden_state.device).unsqueeze(1).expand(B * N, self.top_k).reshape(-1) # shape = (BN * top_k,)\n",
    "\n",
    "        # ---------------- 3) Sort by expert index so we can process contiguous tokens for each expert ----------------\n",
    "        sorted_expert_ids, expert_sort_indices = torch.sort(topk_expert_ids)\n",
    "        sorted_token_indices = token_indices_flat[expert_sort_indices]\n",
    "        sorted_weights = topk_weights[expert_sort_indices]\n",
    "        sorted_inputs = hidden_state[sorted_token_indices]  # shape = (BN*top_k, D)\n",
    "\n",
    "        # ---------------- 4) Build a dict of (expert -> list of tokens) grouped by device  ---------------\n",
    "        device_to_chunk = defaultdict(list)\n",
    "\n",
    "        for i in range(sorted_expert_ids.size(0)):\n",
    "            ex_id = sorted_expert_ids[i].item() # Which expert\n",
    "            tok_id = sorted_token_indices[i].item() # The actual token index\n",
    "            w_val = sorted_weights[i].item() # Probability for that token-expert pair\n",
    "            inp_vec = sorted_inputs[i] # The input embedding\n",
    "            ex_dev = expert_device_map[ex_id]  # for example, 'cuda:1'\n",
    "            device_to_chunk[ex_dev].append((ex_id, tok_id, w_val, inp_vec)) # We store enough info to re-group by expert on the same device\n",
    "\n",
    "        # ---------------- 5) Create streams for each device used ---------------\n",
    "        device_streams = {}\n",
    "        for dev_str in set(expert_device_map):\n",
    "            device_streams[dev_str] = torch.cuda.Stream(device = dev_str)\n",
    "        # Store partial outputs in a dict of device -> list of (token_ids, output_tensors)\n",
    "        device_results = defaultdict(list)\n",
    "\n",
    "        # ---------------- 6) Dispatch to each device in its stream ---------------\n",
    "        # We'll do a loop over the devices. For each device, we do sub-grouping by expert ID\n",
    "        for dev_str, tuple_list in device_to_chunk.items():\n",
    "            if len(tuple_list) == 0:\n",
    "                continue\n",
    "\n",
    "            with torch.cuda.stream(device_streams[dev_str]):\n",
    "                tuple_list.sort(key=lambda x: x[0])  # Sort by ex_id : each tuple is (expert_id, token_idx, w_val, inp_vec)\n",
    "\n",
    "                # Sub-group by expert\n",
    "                idx_start = 0\n",
    "                while idx_start < len(tuple_list):\n",
    "                    current_ex = tuple_list[idx_start][0]\n",
    "                    idx_end = idx_start\n",
    "                    # Gather all entries with the same expert_id\n",
    "                    while idx_end < len(tuple_list) and tuple_list[idx_end][0] == current_ex:\n",
    "                        idx_end += 1\n",
    "                    sub_chunk = tuple_list[idx_start:idx_end]\n",
    "                    idx_start = idx_end\n",
    "\n",
    "                    # Unzip the sub-chunk\n",
    "                    token_ids = [sc[1] for sc in sub_chunk]\n",
    "                    w_vals = [sc[2] for sc in sub_chunk]\n",
    "                    inps = [sc[3] for sc in sub_chunk]\n",
    "\n",
    "                    # Move to main_device -> dev_str as needed\n",
    "                    token_ids_t = torch.tensor(token_ids, device = main_device, dtype = torch.long)\n",
    "                    w_vals_t = torch.tensor(w_vals, device = main_device, dtype = hidden_state.dtype)\n",
    "                    inps_t = torch.stack(inps, dim=0).to(dev_str, non_blocking = True)\n",
    "                    w_vals_t = w_vals_t.unsqueeze(1).to(dev_str, non_blocking = True)\n",
    "\n",
    "                    # Forward pass on the expert (on expert device)\n",
    "                    chunk_output = self.experts[current_ex](inps_t)\n",
    "                    # Multiply by gating weights\n",
    "                    chunk_output = chunk_output * w_vals_t  # gating\n",
    "                    # Move output back to main device\n",
    "                    chunk_output = chunk_output.to(main_device, non_blocking=True)\n",
    "                    # Store for now, and index_add_ after we sync\n",
    "                    device_results[dev_str].append((token_ids_t, chunk_output))\n",
    "\n",
    "        # ---------------- 7) Synchronize & gather on main device ----------------\n",
    "        mlp_output = torch.zeros_like(hidden_state, dtype = hidden_state.dtype)\n",
    "\n",
    "        for dev_str, stream in device_streams.items():\n",
    "            # Wait for everything launched in that stream to finish\n",
    "            with torch.cuda.device(dev_str):\n",
    "                stream.synchronize()\n",
    "\n",
    "            # Now we can safely do index_add_ on main_device\n",
    "            for (tok_ids, out_vecs) in device_results[dev_str]:\n",
    "                mlp_output.index_add_(0, tok_ids, out_vecs)\n",
    "\n",
    "        # Reshape to [B, N, D]\n",
    "        mlp_output = mlp_output.view(B, N, D)\n",
    "        return mlp_output, router_logits, topk_expert_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RRS Downcast Module\n",
    "\"\"\"\n",
    "class Downcast(nn.Module):\n",
    "    def __init__(self, D: int, Z: int):\n",
    "        super().__init__()\n",
    "        self.down_lin = nn.Linear(D, Z, bias = False)\n",
    "        self.down_gate = nn.Linear(D, Z, bias = False)\n",
    "        self.down_act = silu\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.down_lin(x) * self.down_act(self.down_gate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create dense layers before the MoE layers\n",
    "\"\"\"\n",
    "class OlmoeDenseMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    An MLP to use in the dense transformer layer(s)\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf):\n",
    "        super().__init__()\n",
    "        self.conf = conf\n",
    "        self.D = conf.D\n",
    "        self.dense_layer_I = conf.dense_layer_I\n",
    "        self.gate_proj = nn.Linear(self.D, self.dense_layer_I, bias = False)\n",
    "        self.up_proj = nn.Linear(self.D, self.dense_layer_I, bias = False)\n",
    "        self.down_proj = nn.Linear(self.dense_layer_I, self.D, bias = False)\n",
    "        self.act_fn = silu\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "\n",
    "class OlmoeDenseBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A dense transformer layer that uses a standard feed-forward MLP (no MoE). Otherwise parallels the structure of OlmoeBlock: SA, RMSNorm, MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.D = conf.D\n",
    "        self.self_attn = OlmoeAttention(conf = conf)\n",
    "        self.mlp = OlmoeDenseMLP(conf)\n",
    "        self.input_layernorm = OlmoeRMSNorm(conf.D, eps = conf.rms_norm_eps)\n",
    "        self.post_attention_layernorm = OlmoeRMSNorm(conf.D, eps = conf.rms_norm_eps)\n",
    "\n",
    "        self.downcast = Downcast(conf.D, conf.Z) # same as in OlmoeBlock\n",
    "        self.rrs_norm = OlmoeRMSNorm(conf.Z, eps = conf.rms_norm_eps)\n",
    "\n",
    "    def _block_forward(\n",
    "        self,\n",
    "        hidden_state: torch.Tensor,\n",
    "        rrs_state: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor]\n",
    "        ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        ### Pre-SA Residual Stream + Norm ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = self.input_layernorm(hidden_state)\n",
    "        \n",
    "        ### SA + Sum to Residual Stream ###\n",
    "        attn_output = self.self_attn(\n",
    "            hidden_state,\n",
    "            attention_mask = attention_mask,\n",
    "            position_ids = position_ids,\n",
    "            position_embeddings = position_embeddings\n",
    "        )\n",
    "        hidden_state = residual + attn_output\n",
    "\n",
    "        ### Pre-MLP Residual Stream + Norm ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = self.post_attention_layernorm(hidden_state)\n",
    "\n",
    "        ### RRS Downcast + Addition ###\n",
    "        downcast_proj = self.downcast(hidden_state)  # B x N x Z\n",
    "        rrs_state = rrs_state + downcast_proj # Sum to residual stream\n",
    "        rrs_state = self.rrs_norm(rrs_state)\n",
    "\n",
    "        ### MLP + Sum to Residual Stream###\n",
    "        mlp_output = self.mlp(hidden_state)\n",
    "        hidden_state = residual + mlp_output\n",
    "        \n",
    "        return hidden_state, rrs_state\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_state: torch.Tensor,\n",
    "        rrs_state: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
    "        use_checkpointing: bool\n",
    "        ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "            \n",
    "        if use_checkpointing:\n",
    "            hidden_state, rrs_state = torch.utils.checkpoint.checkpoint(\n",
    "                self._block_forward,\n",
    "                hidden_state, rrs_state, attention_mask, position_ids, position_embeddings,\n",
    "                use_reentrant = True\n",
    "            )\n",
    "        else:\n",
    "            hidden_state, rrs_state = self._block_forward(\n",
    "                hidden_state, rrs_state, attention_mask, position_ids, position_embeddings\n",
    "            )\n",
    "        \n",
    "        return hidden_state, rrs_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Now let's define the transformer block.\n",
    "- Most likely, there is nothing to change here, unless you need to change the input/outputs from the MoE layer.\n",
    "- Note that this forward pass is nested within a `custom_forward` call in order to support gradient checkpointing.\n",
    "\"\"\"\n",
    "\n",
    "class OlmoeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer layer\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf, layer_idx: int, shared_gate: nn.Linear):\n",
    "        super().__init__()\n",
    "        self.D = conf.D\n",
    "        self.self_attn = OlmoeAttention(conf = conf)\n",
    "        self.moe = OlmoeMoe(conf, shared_gate = shared_gate)\n",
    "        self.input_layernorm = OlmoeRMSNorm(conf.D, eps = conf.rms_norm_eps)\n",
    "        self.post_attention_layernorm = OlmoeRMSNorm(conf.D, eps = conf.rms_norm_eps)\n",
    "\n",
    "        # Downcast from D->Z for RRS accumulation\n",
    "        self.downcast = Downcast(conf.D, conf.Z)\n",
    "        self.rrs_norm = OlmoeRMSNorm(conf.Z, eps=conf.rms_norm_eps)\n",
    "\n",
    "    def _block_forward(\n",
    "        self,\n",
    "        hidden_state: torch.Tensor,\n",
    "        rrs_state: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
    "        moe_method: str,\n",
    "        use_lflb: bool = False\n",
    "        ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        ### Pre-SA Residual Stream + Norm ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = self.input_layernorm(hidden_state)\n",
    "        \n",
    "        ### SA + Sum to Residual Stream ###\n",
    "        attn_output = self.self_attn(\n",
    "            hidden_state,\n",
    "            attention_mask = attention_mask,\n",
    "            position_ids = position_ids,\n",
    "            position_embeddings = position_embeddings\n",
    "        )\n",
    "        hidden_state = residual + attn_output\n",
    "\n",
    "        ### Pre-MLP Residual Stream + Norm ###\n",
    "        residual = hidden_state\n",
    "        hidden_state = self.post_attention_layernorm(hidden_state)\n",
    "        \n",
    "        downcast_proj = self.downcast(hidden_state)  # B x N x Z\n",
    "        rrs_state = rrs_state + downcast_proj # Sum to residual stream\n",
    "        rrs_state = self.rrs_norm(rrs_state)\n",
    "        \n",
    "        ### MLP + Sum to Residual Stream###\n",
    "        mlp_output, router_logits, topk_experts = self.moe(hidden_state = hidden_state, rrs_state = rrs_state, moe_method = moe_method, use_lflb = use_lflb)\n",
    "        hidden_state = residual + mlp_output\n",
    "        \n",
    "        return hidden_state, rrs_state, router_logits, topk_experts\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_state: torch.Tensor,\n",
    "        rrs_state: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
    "        moe_method: str,\n",
    "        use_lflb: bool,\n",
    "        use_checkpointing: bool\n",
    "        ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "            \n",
    "        if use_checkpointing:\n",
    "            hidden_state, rrs_state, router_logits, topk_experts = torch.utils.checkpoint.checkpoint(\n",
    "                self._block_forward,\n",
    "                hidden_state, rrs_state, attention_mask, position_ids,\n",
    "                position_embeddings, moe_method, use_lflb,\n",
    "                use_reentrant = True\n",
    "            )\n",
    "        else:\n",
    "            hidden_state, rrs_state, router_logits, topk_experts =  self._block_forward(\n",
    "                hidden_state, rrs_state, attention_mask, position_ids,\n",
    "                position_embeddings, moe_method, use_lflb\n",
    "            )\n",
    "\n",
    "        return hidden_state, rrs_state, router_logits, topk_experts\n",
    "    \n",
    "    def get_rrs_offset_loss(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return the L2 norm of the gating offset for this layer's MoE, so the top-level model can sum them up.\n",
    "        \"\"\"\n",
    "        return (self.moe.gate_offset.weight ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Now define the top-level model.\n",
    "- This class is initialized with the `ModelConf` config settings as well as a list of expert-device mappings (leave blank for single-GPU tests).\n",
    "- After initialization, it creates all child layers and moves the experts to their correct devices. \n",
    "  - All other parameters will continue to exist on the default device.\n",
    "- Modify `_init_weights` to change the weight initialization scheme.\n",
    "- The forward pass calls the children layers and also calculates the loss (standard cross-entropy + aux loss). \n",
    "\"\"\"\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "\n",
    "class OlmoeModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The top level model object. Also handles weight initialization and loss calculations.\n",
    "    \"\"\"\n",
    "    def __init__(self, conf: ModelConf, primary_device: str, expert_device_map: None|list[str] = None):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            @conf: A configuration object of class ModelConf.\n",
    "            @primary_device: A device for which to store the dense layers and shared experts on.\n",
    "            @expert_device_map: A list of devices to store experts on. If `None`, stores them all on whatever the torch default device is.\n",
    "              For example, `expert_device_map = ['cuda:0', 'cuda:1', 'cuda:1', 'cuda:2']` means to store expert 0 on cuda:0, experts 1-2 on the device cuda:1, and expert 3 on cuda:2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conf = conf\n",
    "        self.main_device = primary_device\n",
    "\n",
    "        ## Initial dense layers\n",
    "        self.dense_layers = nn.ModuleList([OlmoeDenseBlock(self.conf, layer_idx) for layer_idx in range(self.conf.n_dense_layers)])\n",
    "\n",
    "        ### Layers ###\n",
    "        self.combined_embedding = nn.Embedding(self.conf.vocab_size, self.conf.D + self.conf.Z, self.conf.padding_idx)\n",
    "        self.rotary_emb = OlmoeRotaryEmbedding(conf = self.conf)\n",
    "\n",
    "        self.shared_gate = nn.Linear(conf.Z, conf.n_experts, bias=False)\n",
    "\n",
    "        self.layers = nn.ModuleList([OlmoeBlock(self.conf, layer_idx, shared_gate = self.shared_gate) for layer_idx in range(self.conf.n_layers)])\n",
    "        self.norm = OlmoeRMSNorm(self.conf.D, eps = self.conf.rms_norm_eps)\n",
    "        self.lm_head = nn.Linear(self.conf.D, self.conf.vocab_size, bias = False)\n",
    "        \n",
    "        ### Initialize weights ###\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        ### Model ###\n",
    "        self.to(primary_device)\n",
    "\n",
    "        ### Experts ###\n",
    "        if expert_device_map is not None:\n",
    "            self._move_experts_to_devices(expert_device_map)\n",
    "\n",
    "    # OlMoE weight initiation - see https://github.com/huggingface/transformers/blob/8f1509a96c96747c893051ac947795cfb0750357/src/transformers/modeling_utils.py#L2500-L2515\n",
    "    # Normal distribution for linear layers + embeddings\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean = 0.0, std = 0.02)\n",
    "            # In the vocab -> embedding layer, set all embeddings to 0 for the padding token (tokenizer.pad_token_id)\n",
    "            if module is self.combined_embedding:\n",
    "                module.weight.data[self.conf.padding_idx].zero_() \n",
    "            \n",
    "    def _move_experts_to_devices(self, expert_device_map: list[str]):\n",
    "        \"\"\"\n",
    "        Move each expert in each layer's MoE to the specified device.\n",
    "        \"\"\"\n",
    "        # Require that the length of expert_device_map equal the length of conf.n_experts.\n",
    "        n_experts = self.conf.n_experts\n",
    "        if len(expert_device_map) != n_experts:\n",
    "            raise ValueError(f\"expert_device_map has length {len(expert_device_map)} but n_experts = {n_experts}.\")\n",
    "            \n",
    "        for _, layer in enumerate(self.layers):\n",
    "            moe_block = layer.moe \n",
    "            for ex_idx, expert in enumerate(moe_block.experts):\n",
    "                target_dev = expert_device_map[ex_idx]\n",
    "                expert.to(target_dev)\n",
    "            \n",
    "    def forward(self, input_ids: torch.LongTensor, attention_mask: torch.Tensor, moe_method: str, use_lflb: bool = False, use_checkpointing : bool = False):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            @input_ids: A tensor of input IDs of size B x N, where B is the batch size and N is the sequence length.\n",
    "            @attention_mask: An attention mask tensor of size B x N.\n",
    "            @moe_method: The method to use to calculate the MoE routing. See the `OlmoeMoe` class for details.\n",
    "            @use_lflb: Whether or not to use loss-free balancing.\n",
    "            @use_checkpointing: Whether to use gradient checkpointing. Only set `True` during training.\n",
    "        \"\"\"\n",
    "        combined_state = self.combined_embedding(input_ids)  # (B, N, D+Z)\n",
    "        hidden_state = combined_state[..., :self.conf.D]\n",
    "        rrs_state = combined_state[..., self.conf.D:]\n",
    "        \n",
    "        B, N, D = hidden_state.shape\n",
    "\n",
    "        ### Prep rotary embeddings + attention masks  ###\n",
    "        cache_position = torch.arange(0, N, device = hidden_state.device)\n",
    "        position_ids = cache_position.unsqueeze(0)\n",
    "        position_embeddings = self.rotary_emb(hidden_state, position_ids) # Position embeddings to be shared across transformer layers\n",
    "\n",
    "        # This is the upper-trangular matrix of infinities to mask future tokens in the attention softmax;\n",
    "        if self.conf.attn_method in ['normal', 'sdpa']:\n",
    "            causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(attention_mask, N, N, hidden_state.dtype, hidden_state.device, cache_position, B)\n",
    "        # The flash attention mask is simpler - takes only the original attention mask or None\n",
    "        elif self.conf.attn_method == 'fa2':\n",
    "            causal_mask = attention_mask if (attention_mask is not None and torch.any(attention_mask == 0)) else None\n",
    "        \n",
    "        ### Dense layers ###\n",
    "        for dense_layer in self.dense_layers:\n",
    "            hidden_state, rrs_state = dense_layer(\n",
    "                hidden_state,\n",
    "                rrs_state,\n",
    "                attention_mask = causal_mask,\n",
    "                position_ids = position_ids,\n",
    "                position_embeddings = position_embeddings,\n",
    "                use_checkpointing = use_checkpointing\n",
    "            )\n",
    "\n",
    "        ### Transformer layers ###\n",
    "        all_router_logits = () # Save router logits from each layer into this; will be needed for load balancing loss\n",
    "        all_topk_experts = () # Return topk experts\n",
    "        all_rrs_states = () # Collect rrs states for metrics\n",
    "\n",
    "        # We'll accumulate offset_loss across layers\n",
    "        total_rrs_offset_loss = torch.zeros((), device = hidden_state.device)\n",
    "\n",
    "        for _, layer in enumerate(self.layers):\n",
    "            hidden_state, rrs_state, router_logits, topk_experts = layer(\n",
    "                hidden_state,\n",
    "                rrs_state = rrs_state,\n",
    "                attention_mask = causal_mask,\n",
    "                position_ids = position_ids,\n",
    "                position_embeddings = position_embeddings,\n",
    "                moe_method = moe_method,\n",
    "                use_lflb = use_lflb,\n",
    "                use_checkpointing = use_checkpointing\n",
    "            )\n",
    "            all_router_logits += (router_logits, )\n",
    "            all_topk_experts += (topk_experts,)  # Store the topk_experts for each layer\n",
    "\n",
    "            all_rrs_states += (rrs_state.clone(),)\n",
    "            total_rrs_offset_loss += layer.get_rrs_offset_loss() # Accumulate RRS offset penalty from this layer\n",
    "\n",
    "        hidden_state = self.norm(hidden_state)\n",
    "        output_logits = self.lm_head(hidden_state)\n",
    "\n",
    "        ##### Calculate Loss #####\n",
    "        # The labels object should be a tensor of token IDs or -100 (for attention mask, since don't want to calculate loss for those)\n",
    "        label_ids = torch.where(input_ids == self.conf.padding_idx, torch.tensor(-100), input_ids)\n",
    "        base_loss = ForCausalLMLoss(output_logits, label_ids, self.conf.vocab_size)\n",
    "        aux_loss = load_balancing_loss_func(gate_logits = all_router_logits, num_experts = self.conf.n_experts, top_k = self.conf.top_k, attention_mask = attention_mask)\n",
    "        \n",
    "        return {\n",
    "            'all_router_logits': all_router_logits,\n",
    "            'all_topk_experts': all_topk_experts,\n",
    "            'all_rrs_states': all_rrs_states,\n",
    "            'logits': output_logits,\n",
    "            'aux_loss': aux_loss,\n",
    "            'base_loss': base_loss,\n",
    "            'rrs_offset_loss': total_rrs_offset_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Let's load the model\n",
    "- Set the default_device to specify where all the non-expert layers live (the experts are moved on model init)\n",
    "- Set the default_dtype to specify the model dtype, all params will be in this dtype except for this explicitly specified differently in class definition\n",
    "  - In the default OlMoE, RMSNorm is required to be f32 whereas all other params are bf16. \n",
    "\"\"\"\n",
    "# torch.set_default_device(main_device) # This is buggy, don't use\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "torch.set_float32_matmul_precision('medium') # See https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html \n",
    "torch.manual_seed(seed)\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "\n",
    "model = OlmoeModel(\n",
    "    conf,\n",
    "    primary_device = main_device, # Where to store dense layers and shared experts\n",
    "    expert_device_map = ['cuda:0'] * 64 # We have 4 experts, here let's test them with all of them on cuda:0\n",
    ")\n",
    "model = torch.compile(model)\n",
    "total_params = sum(p.numel() for p in model.parameters()) \n",
    "active_params = total_params - sum(p.numel() * (1 - conf.top_k/conf.n_experts) for p in model.layers[0].moe.experts.parameters()) * conf.n_layers\n",
    "print(f\"Active/total parameters: {active_params:,}/{total_params:,}\")\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint_data = torch.load('./../../datasets/saved-models/exp-52-00025000.pt', pickle_module = dill)\n",
    "model.load_state_dict(checkpoint_data['model_state_dict'])\n",
    "model = model._orig_mod # Load the uncompiled model \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/OLMoE-1B-7B-0924', add_eos_token = False, add_bos_token = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qualitative test\n",
    "\"\"\"\n",
    "from termcolor import colored\n",
    "\n",
    "prompts = [\n",
    "    'My dog Caramel',\n",
    "    'Hello! What\\'s up today? I',\n",
    "    'Ugh,',\n",
    "    'Large language models are'\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors = 'pt').to(main_device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    print('\\n----------\\n')\n",
    "    print(colored(prompt, 'green'), end = '')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(20):\n",
    "            if i > 0:\n",
    "                input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim = 1)\n",
    "                attention_mask = torch.cat([attention_mask, torch.ones((1, 1), dtype = torch.long, device = input_ids.device)], dim = 1)\n",
    "\n",
    "            output = model(input_ids, attention_mask, moe_method = 'forward_slow', use_lflb = True, use_checkpointing = False)\n",
    "\n",
    "            next_token_id = torch.argmax(output['logits'][0, -1, :], dim = -1).unsqueeze(0)\n",
    "\n",
    "            if next_token_id.item() in [tokenizer.eos_token_id, tokenizer.encode('\\n')[0]]:\n",
    "                break\n",
    "\n",
    "            print(colored(tokenizer.decode(next_token_id), 'red'), end = '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import glob\n",
    "\n",
    "yaml_files = glob.glob('./../../datasets/contextual-tokens/*.yaml')\n",
    "if len(yaml_files) == 0:\n",
    "    raise Exception('No files in search path!')\n",
    "print(f\"{str(len(yaml_files))} files found\")\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "for yaml_file in yaml_files:\n",
    "    with open(yaml_file, \"r\", encoding = \"utf-8\") as f:\n",
    "        token_info = yaml.safe_load(f)[0]\n",
    "\n",
    "        test_token = token_info['token']\n",
    "        test_meanings = [m['meaning_label'] for m in token_info['meanings']]\n",
    "        \n",
    "        all_tokens.append({\n",
    "            'token': test_token,\n",
    "            'meaning_labels': [x['meaning_label'] for x in token_info['meanings']],\n",
    "            'meaning_samples': [x['text_samples'] for x in token_info['meanings']]\n",
    "        })\n",
    "\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cats = all_tokens[10]['meanings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@torch.no_grad()\n",
    "def convert_topk_to_df(all_topk_experts, input_ids):\n",
    "    \"\"\"\n",
    "    Converts all_topk_experts into a pandas dataframe for later analysis.\n",
    "\n",
    "    Params:\n",
    "        @all_topk_experts: A tuple of n_layers length, with each element a tensor size (BN, topk) containing the expert IDs selected\n",
    "        @input_ids: A \n",
    "\n",
    "    Returns:\n",
    "        A dataframe at `sequence_ix` x `token_ix` x `layer_ix`, with columns:\n",
    "        - `sequence_ix`: The index of the batch sub-samples.\n",
    "        - `token_ix`: The token index of a single sequence.\n",
    "        - `layer_ix`: The layer index.\n",
    "        - `token_id`: The token ID at that `sequence_ix` x `token_ix`.\n",
    "        - `expert_1`, `expert_2`, ..., `expert_[topk]`:  The routing of that token at `sequence_ix` x `token_ix` x `layer_ix`.\n",
    "\n",
    "    Example:\n",
    "        prompt = 'Hello'\n",
    "        inputs = tokenizer(prompt, return_tensors = 'pt').to(main_device)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask)\n",
    "\n",
    "        topk_to_df(output['all_topk_experts'], input_ids)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    B, N = input_ids.shape\n",
    "    top_k = all_topk_experts[0].shape[1]\n",
    "    \n",
    "    # Flatten input_ids to match all_topk_experts shape\n",
    "    flat_input_ids = input_ids.reshape(-1).cpu().numpy()\n",
    "    \n",
    "    for layer_ix, layer_experts in enumerate(all_topk_experts):\n",
    "        layer_experts_np = layer_experts.cpu().numpy()\n",
    "        \n",
    "        # For each token position\n",
    "        for token_pos in range(B * N):\n",
    "            # Get batch and token indices\n",
    "            sequence_ix = token_pos // N\n",
    "            token_ix = token_pos % N\n",
    "            \n",
    "            # Get experts for this token\n",
    "            experts = layer_experts_np[token_pos]\n",
    "            \n",
    "            # Create a row with all info\n",
    "            row = {\"sequence_ix\": sequence_ix, \"token_ix\": token_ix, \"token_id\": flat_input_ids[token_pos], \"layer_ix\": layer_ix}\n",
    "            \n",
    "            # Add each expert\n",
    "            for k in range(top_k):\n",
    "                row[f\"expert_{k+1}\"] = experts[k]\n",
    "                \n",
    "            data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "df = convert_topk_to_df(output['all_topk_experts'], input_ids)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_and_store_topk(token: str, meaning_samples: list[list[str]], meaning_labels: list[str], tokenizer, B: int = 8):\n",
    "    \"\"\"\n",
    "    Takes contextual data samples, runs forward passes, and returns a single dataframe with a row representing: within a single (test token, test meaning),\n",
    "      for a single token at position (batch_ix, sequence_ix, token_ix) at layer layer_ix, what are all the topk routing choices?  \n",
    "\n",
    "    Params:\n",
    "        @token: The token to validate on\n",
    "        @meaning_samples: A nested list of meaning samples [ [x1, x2, x3], [y1, y2, y3], ...]\n",
    "        @meaning_labels: A list of the same length of meaning samples containing labels ['xlabel', 'ylabel', ...]\n",
    "        @tokenizer: The tokenizer to use\n",
    "\n",
    "    Example:\n",
    "        token_res = run_and_store_topk(all_tokens[0]['token'], all_tokens[0]['meaning_samples'], all_tokens[0]['meaning_labels'], tokenizer)\n",
    "        display(token_res)\n",
    "        display(token_res[token_res['is_test_token'] == 1])\n",
    "    \"\"\"\n",
    "    clean_topk_dfs = []\n",
    "    this_token_id = tokenizer.encode(token)[0]\n",
    "    \n",
    "    vocab_map =\\\n",
    "        pd.DataFrame([{\"token\": token.replace('', ' '), \"token_id\": token_id} for token, token_id in tokenizer.get_vocab().items()])\\\n",
    "        .sort_values(by = 'token_id')\\\n",
    "        .reset_index()\n",
    "    \n",
    "    for meaning_ix, meaning_set in enumerate(meaning_samples):\n",
    "    \n",
    "        for batch_ix, batch_start in enumerate(range(0, len(meaning_set), B)):\n",
    "            batch_data  = meaning_set[batch_start : batch_start + B]\n",
    "\n",
    "            inputs = tokenizer(batch_data, return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(main_device)\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "\n",
    "            # Run forward pass and get topk expert IDs as a dataframe\n",
    "            output = model(input_ids, attention_mask, moe_method = 'forward_slow', use_lflb = True, use_checkpointing = False)\n",
    "            raw_topk_df = convert_topk_to_df(output['all_topk_experts'], input_ids)\n",
    "\n",
    "            raw_topk_df = raw_topk_df[raw_topk_df['token_id'] != tokenizer.pad_token_id] # Filter out rows with attention_mask\n",
    "\n",
    "            clean_topk_df =\\\n",
    "                raw_topk_df\\\n",
    "                .assign(\n",
    "                    test_token = token,\n",
    "                    test_token_id = this_token_id,\n",
    "                    meaning_label = meaning_labels[meaning_ix],\n",
    "                    batch_ix = batch_ix,\n",
    "                    is_test_token = lambda df: np.where(df['token_id'] == this_token_id, 1, 0)\n",
    "                )\\\n",
    "                .merge(vocab_map, on = 'token_id', how = 'left')\\\n",
    "                [\n",
    "                    ['test_token', 'test_token_id', 'meaning_label', 'batch_ix', 'sequence_ix', 'token_ix', 'token_id', 'token', 'is_test_token', 'layer_ix'] +\\\n",
    "                    [x for x in raw_topk_df.columns.tolist() if 'expert_' in x]\n",
    "                ]\n",
    "\n",
    "            clean_topk_dfs.append(clean_topk_df)\n",
    "                \n",
    "    return pd.concat(clean_topk_dfs)\n",
    "    \n",
    "\n",
    "token_res = run_and_store_topk(all_tokens[0]['token'], all_tokens[0]['meaning_samples'], all_tokens[0]['meaning_labels'], tokenizer)\n",
    "display(token_res)\n",
    "display(token_res[token_res['is_test_token'] == 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_results = pd.concat([\n",
    "    run_and_store_topk(tok_el['token'], tok_el['meaning_samples'], tok_el['meaning_labels'], tokenizer)\n",
    "    for tok_el in tqdm(all_tokens)\n",
    "])\n",
    "\n",
    "tok_results.to_csv('contextual_token_activations.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mmlu_ds = load_dataset(\"TIGER-Lab/MMLU-Pro\", split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "\n",
    "# # Use your token from https://huggingface.co/settings/tokens\n",
    "# login(token = os.getenv('HF_TOKEN'))\n",
    "\n",
    "# paloma = load_dataset(\"allenai/paloma\", 'dolma_100_subreddits')\n",
    "\n",
    "# domains = list(set(paloma['val'][\"subdomain\"]))\n",
    "# print(f\"Available domains ({len(domains)}): {sorted(domains)}\")\n",
    "# domains = list(set(mmlu_ds[\"category\"]))\n",
    "# print(f\"Available domains ({len(domains)}): {sorted(domains)}\")\n",
    "\n",
    "# list(mmlu_ds.iter(batch_size = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {\"total\": {\"correct\": 0, \"total\": 0}}\n",
    "# examples_log = []\n",
    "\n",
    "# # Track domain/category performance\n",
    "# categories = {}\n",
    "\n",
    "# for i in range(len(mmlu_ds))[2:5]:\n",
    "#     category = mmlu_ds[i]['category']\n",
    "#     question = mmlu_ds[i]['question']\n",
    "#     options = mmlu_ds[i]['options']\n",
    "#     correct_answer_idx = mmlu_ds[i]['answer_index']\n",
    "    \n",
    "#     if category not in results:\n",
    "#         results[category] = {\"correct\": 0, \"total\": 0}\n",
    "    \n",
    "#     # Format prompt\n",
    "#     prompt = f\"Answer with the response, \\\"The correct answer is _\\\".\\nQuestion: {question}\\nOptions:\\n\"\n",
    "#     for i, option in enumerate(options):\n",
    "#         letter = chr(65 + i)  # A, B, C, etc.\n",
    "#         prompt += f\"{letter}. {option}\\n\"\n",
    "#     prompt += \"\\nAnswer:\"\n",
    "\n",
    "#     print(prompt)\n",
    "    \n",
    "#     # Tokenize input\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(main_device)\n",
    "#     input_ids = inputs[\"input_ids\"]\n",
    "#     attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "#     # Run model inference\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids=input_ids, attention_mask = attention_mask, moe_method = 'forward_slow', use_lflb = True)\n",
    "        \n",
    "#         # Get next token prediction (for single token answer)\n",
    "#         next_token_logits = outputs['logits'][0, -1, :]\n",
    "#         next_token_id = torch.argmax(next_token_logits).item()\n",
    "#         predicted_text = tokenizer.decode([next_token_id]).strip()\n",
    "        \n",
    "#         # Get expert assignment (for analysis)\n",
    "#         # expert_assignments = [layer_experts[0].cpu().numpy() for layer_experts in outputs['all_topk_experts']]\n",
    "    \n",
    "#     print(predicted_text)\n",
    "\n",
    "#     # Parse predicted answer - look for a letter\n",
    "#     predicted_letter = None\n",
    "#     for c in predicted_text:\n",
    "#         if c.upper() in [chr(65 + i) for i in range(len(options))]:\n",
    "#             predicted_letter = c.upper()\n",
    "#             break\n",
    "    \n",
    "#     if predicted_letter:\n",
    "#         predicted_idx = ord(predicted_letter) - ord('A')\n",
    "#     else:\n",
    "#         # If no letter found, check if it's a digit\n",
    "#         for c in predicted_text:\n",
    "#             if c.isdigit() and 0 <= int(c)-1 < len(options):\n",
    "#                 predicted_idx = int(c)-1\n",
    "#                 break\n",
    "#         else:\n",
    "#             predicted_idx = None\n",
    "    \n",
    "#     # Check if correct\n",
    "#     is_correct = (predicted_idx == correct_answer_idx)\n",
    "#     print(is_correct)\n",
    "#     # Update results\n",
    "#     if is_correct:\n",
    "#         results[category][\"correct\"] += 1\n",
    "#         results[\"total\"][\"correct\"] += 1\n",
    "#     results[category][\"total\"] += 1\n",
    "#     results[\"total\"][\"total\"] += 1\n",
    "    \n",
    "#     # Log detailed result\n",
    "#     examples_log.append({\n",
    "#         \"idx\": idx,\n",
    "#         \"category\": category,\n",
    "#         \"question\": question,\n",
    "#         \"options\": options,\n",
    "#         \"correct_answer_idx\": correct_answer_idx,\n",
    "#         \"correct_answer\": chr(65 + correct_answer_idx),\n",
    "#         \"predicted_text\": predicted_text,\n",
    "#         \"predicted_idx\": predicted_idx,\n",
    "#         \"predicted_letter\": predicted_letter,\n",
    "#         \"is_correct\": is_correct,\n",
    "#         \"expert_assignments\": expert_assignments\n",
    "#     })\n",
    "    \n",
    "#     # Print progress\n",
    "#     if (idx + 1) % 10 == 0:\n",
    "#         accuracy = results[\"total\"][\"correct\"] / results[\"total\"][\"total\"] * 100\n",
    "#         print(f\"Progress: {idx+1}/{len(eval_ds)} - Current accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# # Calculate final results\n",
    "# for category, stats in results.items():\n",
    "#     if stats[\"total\"] > 0:\n",
    "#         stats[\"accuracy\"] = stats[\"correct\"] / stats[\"total\"] * 100\n",
    "#         print(f\"{category}: {stats['accuracy']:.2f}% ({stats['correct']}/{stats['total']})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
