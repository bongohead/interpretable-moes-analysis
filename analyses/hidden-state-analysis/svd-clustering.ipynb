{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to use SVD to decompose hidden states based on whether they're used by routing or not & visualize clustering/PCA.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "from utils.svd import decompose_orthogonal, decompose_sideways\n",
    "from utils.vis import combine_plots\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_ix = 6\n",
    "models_list = [\n",
    "    ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 0),\n",
    "    ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 0),\n",
    "    ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 1),\n",
    "    ('Qwen/Qwen3-30B-A3B-Instruct-2507', 'qwen3moe', 0),\n",
    "    ('moonshotai/Kimi-VL-A3B-Instruct', 'kimivl', 1),\n",
    "    ('ibm-granite/granite-4.0-tiny-preview', 'granite', 0),\n",
    "    ('zai-org/GLM-4.5-Air-FP8', 'glm4moe', 1)\n",
    "]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_prefix):\n",
    "    local_path = os.path.join('/workspace/models', model_prefix)\n",
    "    model_path = local_path if os.path.exists(local_path) else model_id    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype = torch.bfloat16, trust_remote_code = True, device_map = 'auto').eval()\n",
    "    if model_path == model_id:\n",
    "        os.makedirs(local_path, exist_ok = True)\n",
    "        tokenizer.save_pretrained(local_path)\n",
    "        model.save_pretrained(local_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "model_id, model_prefix, model_pre_mlp_layers = models_list[model_ix]\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix, max_data_files):\n",
    "    \"\"\"\n",
    "    Load data saved by `export-activations-sm.ipynb`\n",
    "    \"\"\"\n",
    "    folders = [f'./../export-data/activations-sm/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    all_pre_mlp_hs = []\n",
    "    sample_df = []\n",
    "    topk_df = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_df.append(pd.read_pickle(f'{f}/samples.pkl'))\n",
    "        topk_df.append(pd.read_pickle(f'{f}/topks.pkl'))\n",
    "        all_pre_mlp_hs.append(torch.load(f'{f}/all-pre-mlp-hidden-states.pt'))\n",
    "\n",
    "    sample_df = pd.concat(sample_df)\n",
    "    topk_df = pd.concat(topk_df)\n",
    "    all_pre_mlp_hs = torch.concat(all_pre_mlp_hs)    \n",
    "\n",
    "    with open(f'./../export-data/activations-sm/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    gc.collect()\n",
    "    return sample_df, topk_df, all_pre_mlp_hs, metadata['all_pre_mlp_hidden_states_layers']\n",
    "\n",
    "# Due to mem constraints, for Qwen3Moe max_data_files = 3\n",
    "sample_df_import, topk_df_import, all_pre_mlp_hs_import, act_map = load_data(model_prefix, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .assign(layer_ix = lambda df: df['layer_ix'] + model_pre_mlp_layers)\n",
    "\n",
    "topk1_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "def get_sample_df_for_layer(sample_df, topk_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Helper to take the sample df and merge layer-level expert selection information\n",
    "    \"\"\"\n",
    "    topk_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix])\n",
    "    topk_l1_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 1])\n",
    "    topk_l2_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 2])\n",
    "\n",
    "    layer_df =\\\n",
    "        sample_df\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 1])[['sample_ix', 'expert']], how = 'inner', on = 'sample_ix')\\\n",
    "        .merge(topk_l1_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_l2_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev2_expert'})[['sample_ix', 'prev2_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 2]).rename(columns = {'expert': 'expert2'})[['sample_ix', 'expert2']], how = 'left', on = 'sample_ix')\\\n",
    "        .assign(leading_path = lambda df: df['prev2_expert'] + '-' + df['prev_expert'])\n",
    "    \n",
    "    return layer_df\n",
    "\n",
    "del sample_df_import, sample_df_raw, topk_df_import\n",
    "\n",
    "gc.collect()\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {(layer_ix + model_pre_mlp_layers): all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Only keep specific layers for analysis\n",
    "\"\"\"\n",
    "every_fourth_layer = [layer_ix for layer_ix in list(all_pre_mlp_hs.keys()) if layer_ix % 4 == 1]\n",
    "midpoint_layer = int(np.floor(np.median(list(all_pre_mlp_hs.keys()))).item())\n",
    "\n",
    "retain_layers = sorted(list(set(every_fourth_layer + [midpoint_layer])))\n",
    "all_pre_mlp_hs = {layer_ix: all_pre_mlp_hs[layer_ix] for layer_ix in retain_layers}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's take the pre-MLP hidden states and split them using SVD into parallel and orthogonal components.\n",
    "\"\"\"\n",
    "h_para_by_layer = {}\n",
    "h_orth_by_layer = {}\n",
    "\n",
    "for layer_ix in tqdm(list(all_pre_mlp_hs.keys())):\n",
    "    h_para_by_layer[layer_ix], h_orth_by_layer[layer_ix] = decompose_orthogonal(\n",
    "        all_pre_mlp_hs[layer_ix].to(torch.float32),\n",
    "        model.model.layers[layer_ix].mlp.gate.weight.detach().cpu().to(torch.float32),\n",
    "        'svd'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction & clustering helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reduction helpers\n",
    "\"\"\"\n",
    "def reduce_pca(input_tensor: torch.Tensor, n_components = 2):\n",
    "    hs_cupy = cupy.asarray(input_tensor.to(torch.float32))\n",
    "    model = cuml.PCA(\n",
    "        iterated_power = 100,\n",
    "        n_components = n_components,\n",
    "        verbose = True\n",
    "    )\n",
    "    model.fit(hs_cupy)\n",
    "    # print(f'Explained variance ratio: {model.explained_variance_ratio_}')\n",
    "    print(f'Cumulative variance ratio: {np.cumsum(model.explained_variance_ratio_)[-1]}')\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return pred\n",
    "\n",
    "def reduce_umap(input_tensor: torch.Tensor, n_components = 2, metric = 'cosine', n_epochs = 200):\n",
    "    hs_cupy = cupy.asarray(input_tensor.to(torch.float32))\n",
    "    model = cuml.UMAP(\n",
    "        n_components = n_components, \n",
    "        n_neighbors = 20, # 15 for default, smaller = more local data preserved [2 - 100]\n",
    "        metric = metric, # euclidean, cosine, manhattan, l2, hamming\n",
    "        min_dist = 0.2, # 0.1 by default, effective distance between embedded points\n",
    "        n_epochs = n_epochs, # 200 by default for large datasets\n",
    "        random_state = None, # Allow parallelism\n",
    "        verbose = False\n",
    "    )\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return pred\n",
    "\n",
    "def plot_manifold(plot_df, color_col, hover_col, title = None):\n",
    "    plot = px.scatter(\n",
    "        plot_df,\n",
    "        x = 'd1', y = 'd2', color = color_col, hover_data = [hover_col],\n",
    "        title = title, opacity = 0.9\n",
    "    ).update_layout(autosize = False, height = 400).update_traces(marker = dict(size = 5))\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orth vs Para Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for clustering\n",
    "\"\"\"\n",
    "def print_samples(df, grouping_cols):\n",
    "    \"\"\"\n",
    "    Takes a wide dataframe and groups it, then prints random groups\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        df\\\n",
    "        .groupby(grouping_cols, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .sample(35)\n",
    "    \n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's cluster the para and ortho using k-means and see what clusters we get\n",
    "\"\"\"\n",
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 512):\n",
    "    \"\"\"\n",
    "    K-means clustering\n",
    "    \"\"\"\n",
    "    kmeans_model = cuml.cluster.KMeans(\n",
    "        n_clusters = n_clusters,\n",
    "        max_iter = 1000,\n",
    "        random_state = seed\n",
    "    )\n",
    "    kmeans_model.fit(cupy.asarray(layer_hs.to(torch.float32)))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return kmeans_model.labels_.tolist()\n",
    "\n",
    "\n",
    "def get_cluster(sample_df, hidden_states_by_layer, n_clusters = 256):\n",
    "    \"\"\"\n",
    "    Get k-means clusters across hidden state layers\n",
    "    \"\"\"\n",
    "    cluster_ids_by_layer = [\n",
    "        {'layer_ix': layer_ix, 'cluster_ids': cluster_kmeans(layer_hs, n_clusters)} \n",
    "        for layer_ix, layer_hs in tqdm(hidden_states_by_layer.items())\n",
    "    ]\n",
    "    cluster_ids_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cluster_ids_by_layer], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "    \n",
    "    display(\n",
    "        cluster_ids_df.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False)\n",
    "    )\n",
    "\n",
    "    return cluster_ids_df\n",
    "\n",
    "para_clusters_df = get_cluster(sample_df, h_para_by_layer)\n",
    "orth_clusters_df = get_cluster(sample_df, h_orth_by_layer)\n",
    "\n",
    "# print_samples(para_clusters_df, ['layer_1_id', 'layer_2_id'])\n",
    "# print_samples(orth_clusters_df, ['layer_1_id', 'layer_2_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_samples(para_clusters_df, ['layer_6_id', 'layer_7_id'])\n",
    "print_samples(orth_clusters_df, ['layer_6_id', 'layer_7_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count how many clusters are token-specific\n",
    "\"\"\"\n",
    "def get_single_token_cluster_counts(cluster_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Get how many tokens belong to a single cluster\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        cluster_df\\\n",
    "        .groupby([f'layer_{str(layer_ix)}_id'], as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 20)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .assign(is_eq = lambda df: df.samples.apply(lambda s: 1 if len(set(s)) == 1 else 0))\\\n",
    "        .groupby('is_eq', as_index = False)\\\n",
    "        .agg(count = ('is_eq', 'count'))\n",
    "\n",
    "    return(res)\n",
    "\n",
    "display(get_single_token_cluster_counts(para_clusters_df, 5))\n",
    "display(get_single_token_cluster_counts(orth_clusters_df, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count entropy distribution\n",
    "\"\"\"\n",
    "def get_entropy_distribution(cluster_df, layer_ix, min_cluster_size = 1):\n",
    "    cluster_id_col = f'layer_{str(layer_ix)}_id'\n",
    "\n",
    "    def calculate_dominance(series):\n",
    "        \"\"\"Calculates the proportion of the most frequent item.\"\"\"\n",
    "        if series.empty:\n",
    "            return np.nan\n",
    "        counts = series.value_counts()\n",
    "        return counts.iloc[0] / counts.sum()\n",
    "\n",
    "    def calculate_normalized_entropy(series):\n",
    "        \"\"\"Calculates entropy normalized by log2(n_unique_tokens).\"\"\"\n",
    "        if series.empty:\n",
    "            return np.nan\n",
    "        counts = series.value_counts()\n",
    "        n_unique = len(counts)\n",
    "        \n",
    "        if n_unique <= 1:\n",
    "            return 0.0 # Perfectly pure cluster has zero entropy\n",
    "\n",
    "        ent = scipy.stats.entropy(counts, base=2)\n",
    "        \n",
    "        # Normalize by log2 of the number of unique elements\n",
    "        return ent / np.log2(n_unique)\n",
    "\n",
    "    # Perform aggregation\n",
    "    agg_metrics =\\\n",
    "        cluster_df\\\n",
    "        .groupby(cluster_id_col, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples=('token', 'size'),\n",
    "            n_unique_tokens=('token', 'nunique'),\n",
    "            dominance=('token', calculate_dominance),\n",
    "            normalized_entropy=('token', calculate_normalized_entropy)\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= min_cluster_size])\n",
    "\n",
    "    return agg_metrics\n",
    "\n",
    "para_entropy = get_entropy_distribution(para_clusters_df, 1)\n",
    "orth_entropy = get_entropy_distribution(orth_clusters_df, 1)\n",
    "\n",
    "print(f\"Para entropy: {para_entropy['normalized_entropy'].mean()}\")\n",
    "print(f\"Orth entropy: {orth_entropy['normalized_entropy'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
