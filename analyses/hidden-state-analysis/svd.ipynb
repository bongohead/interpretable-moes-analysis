{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to use SVD to decompose hidden states based on whether they're used by routing or not.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CUDA memory cleared on all devices.\n",
      "Device 0: NVIDIA H100 PCIe\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Total: 79.10 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "from utils.svd import decompose_orthogonal, decompose_sideways\n",
    "from utils.vis import combine_plots\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa84531fc424a91a5e4a0bc14bbb7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_ix = 3\n",
    "models_list = [\n",
    "    ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 0),\n",
    "    ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 0),\n",
    "    ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 1),\n",
    "    ('Qwen/Qwen3-30B-A3B', 'qwen3moe', 0)\n",
    "]\n",
    "\n",
    "model_id, model_prefix, model_pre_mlp_layers = models_list[model_ix]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:19<00:00,  9.62s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix, max_data_files):\n",
    "    \"\"\"\n",
    "    Load data saved by `export-activations-sm.ipynb`\n",
    "    \"\"\"\n",
    "    folders = [f'./../export-data/activations-sm/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    all_pre_mlp_hs = []\n",
    "    sample_df = []\n",
    "    topk_df = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_df.append(pd.read_pickle(f'{f}/samples.pkl'))\n",
    "        topk_df.append(pd.read_pickle(f'{f}/topks.pkl'))\n",
    "        all_pre_mlp_hs.append(torch.load(f'{f}/all-pre-mlp-hidden-states.pt'))\n",
    "\n",
    "    sample_df = pd.concat(sample_df)\n",
    "    topk_df = pd.concat(topk_df)\n",
    "    all_pre_mlp_hs = torch.concat(all_pre_mlp_hs)    \n",
    "\n",
    "    with open(f'./../export-data/activations-sm/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    gc.collect()\n",
    "    return sample_df, topk_df, all_pre_mlp_hs, metadata['all_pre_mlp_hidden_states_layers']\n",
    "\n",
    "# Due to mem constraints, for Qwen3Moe max_data_files = 2\n",
    "sample_df_import, topk_df_import, all_pre_mlp_hs_import, act_map = load_data(model_prefix, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_ix</th>\n",
       "      <th>topk_ix</th>\n",
       "      <th>expert</th>\n",
       "      <th>weight</th>\n",
       "      <th>sample_ix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>118</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59693563</th>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>0.09</td>\n",
       "      <td>155451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59693564</th>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "      <td>0.09</td>\n",
       "      <td>155451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59693565</th>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>155451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59693566</th>\n",
       "      <td>47</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>0.08</td>\n",
       "      <td>155451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59693567</th>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.08</td>\n",
       "      <td>155451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59693568 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          layer_ix  topk_ix  expert  weight  sample_ix\n",
       "0                0        1      74    0.21          0\n",
       "1                0        2     118    0.19          0\n",
       "2                0        3      24    0.15          0\n",
       "3                0        4      47    0.14          0\n",
       "4                0        5       4    0.13          0\n",
       "...            ...      ...     ...     ...        ...\n",
       "59693563        47        4      61    0.09     155451\n",
       "59693564        47        5     103    0.09     155451\n",
       "59693565        47        6      12    0.08     155451\n",
       "59693566        47        7      48    0.08     155451\n",
       "59693567        47        8       7    0.08     155451\n",
       "\n",
       "[59693568 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>token_ix</th>\n",
       "      <th>token_id</th>\n",
       "      <th>output_id</th>\n",
       "      <th>output_prob</th>\n",
       "      <th>token</th>\n",
       "      <th>source</th>\n",
       "      <th>sample_ix</th>\n",
       "      <th>seq_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>1593</td>\n",
       "      <td>353</td>\n",
       "      <td>0.70</td>\n",
       "      <td>LO</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>29676</td>\n",
       "      <td>3385</td>\n",
       "      <td>0.24</td>\n",
       "      <td>QUE</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>434</td>\n",
       "      <td>969</td>\n",
       "      <td>0.87</td>\n",
       "      <td>F</td>\n",
       "      <td>es</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>35830</td>\n",
       "      <td>56550</td>\n",
       "      <td>0.94</td>\n",
       "      <td>ALT</td>\n",
       "      <td>es</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>66</td>\n",
       "      <td>56550</td>\n",
       "      <td>25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>ABA</td>\n",
       "      <td>es</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155447</th>\n",
       "      <td>79849</td>\n",
       "      <td>507</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "      <td>es</td>\n",
       "      <td>155447</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155448</th>\n",
       "      <td>79850</td>\n",
       "      <td>508</td>\n",
       "      <td>21</td>\n",
       "      <td>17449</td>\n",
       "      <td>0.95</td>\n",
       "      <td>6</td>\n",
       "      <td>es</td>\n",
       "      <td>155448</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155449</th>\n",
       "      <td>79851</td>\n",
       "      <td>509</td>\n",
       "      <td>17449</td>\n",
       "      <td>220</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Sep</td>\n",
       "      <td>es</td>\n",
       "      <td>155449</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155450</th>\n",
       "      <td>79852</td>\n",
       "      <td>510</td>\n",
       "      <td>220</td>\n",
       "      <td>17</td>\n",
       "      <td>1.00</td>\n",
       "      <td></td>\n",
       "      <td>es</td>\n",
       "      <td>155450</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155451</th>\n",
       "      <td>79853</td>\n",
       "      <td>511</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "      <td>es</td>\n",
       "      <td>155451</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155452 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  token_ix  token_id  output_id  output_prob token source  \\\n",
       "0           0        62      1593        353         0.70    LO     es   \n",
       "1           1        63     29676       3385         0.24   QUE     es   \n",
       "2           2        64       434        969         0.87     F     es   \n",
       "3           3        65     35830      56550         0.94   ALT     es   \n",
       "4           4        66     56550         25         0.22   ABA     es   \n",
       "...       ...       ...       ...        ...          ...   ...    ...   \n",
       "155447  79849       507        15         20         0.70     0     es   \n",
       "155448  79850       508        21      17449         0.95     6     es   \n",
       "155449  79851       509     17449        220         1.00   Sep     es   \n",
       "155450  79852       510       220         17         1.00           es   \n",
       "155451  79853       511        17         15         1.00     2     es   \n",
       "\n",
       "        sample_ix  seq_id  \n",
       "0               0       0  \n",
       "1               1       0  \n",
       "2               2       0  \n",
       "3               3       0  \n",
       "4               4       0  \n",
       "...           ...     ...  \n",
       "155447     155447     499  \n",
       "155448     155448     499  \n",
       "155449     155449     499  \n",
       "155450     155450     499  \n",
       "155451     155451     499  \n",
       "\n",
       "[155452 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .assign(layer_ix = lambda df: df['layer_ix'] + model_pre_mlp_layers)\n",
    "\n",
    "topk1_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "def get_sample_df_for_layer(sample_df, topk_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Helper to take the sample df and merge layer-level expert selection information\n",
    "    \"\"\"\n",
    "    topk_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix])\n",
    "    topk_l1_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 1])\n",
    "    topk_l2_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 2])\n",
    "\n",
    "    layer_df =\\\n",
    "        sample_df\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 1])[['sample_ix', 'expert']], how = 'inner', on = 'sample_ix')\\\n",
    "        .merge(topk_l1_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_l2_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev2_expert'})[['sample_ix', 'prev2_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 2]).rename(columns = {'expert': 'expert2'})[['sample_ix', 'expert2']], how = 'left', on = 'sample_ix')\\\n",
    "        .assign(leading_path = lambda df: df['prev2_expert'] + '-' + df['prev_expert'])\n",
    "    \n",
    "    return layer_df\n",
    "\n",
    "del sample_df_import, sample_df_raw, topk_df_import\n",
    "\n",
    "gc.collect()\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {(layer_ix + model_pre_mlp_layers): all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [02:43<00:00,  3.40s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's take the pre-MLP hidden states and split them using SVD into parallel and orthogonal components.\n",
    "\"\"\"\n",
    "h_para_by_layer = {}\n",
    "h_orth_by_layer = {}\n",
    "\n",
    "for layer_ix in tqdm(list(all_pre_mlp_hs.keys())):\n",
    "    h_para_by_layer[layer_ix], h_orth_by_layer[layer_ix] = decompose_orthogonal(\n",
    "        all_pre_mlp_hs[layer_ix].to(torch.float32),\n",
    "        model.model.layers[layer_ix].mlp.gate.weight.detach().cpu().to(torch.float32),\n",
    "        'svd'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orth vs Para Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean across layer transitions + samples: 0.71 +/- 0.04\n"
     ]
    }
   ],
   "source": [
    "bootstrap_samples = 50\n",
    "\n",
    "def get_sample_res(hs_by_layer, samples_to_test = 1):\n",
    "    \n",
    "    samples = np.random.randint(0, hs_by_layer[0].shape[0], samples_to_test)\n",
    "\n",
    "    # Cast into sample-level list\n",
    "    sample_tensors = torch.stack([layer_hs[samples, :] for _, layer_hs in hs_by_layer.items()], dim = 1).unbind(dim = 0)\n",
    "\n",
    "    sims = []\n",
    "    for s in sample_tensors:\n",
    "        cos_sim = sklearn.metrics.pairwise.cosine_similarity(s)\n",
    "        sims.append(np.diag(cos_sim, 1))\n",
    "\n",
    "    return np.mean(np.stack(sims, axis = 0), axis = 0)\n",
    "\n",
    "para_res = np.stack([get_sample_res(h_para_by_layer) for _ in range(bootstrap_samples)], axis = 0) # bootstrap_samples x layer_diffs\n",
    "\n",
    "para_mean_across_layers = para_res.mean(axis = 0)\n",
    "para_cis_across_layers = 1.96 * np.std(para_res, axis = 0)\n",
    "\n",
    "para_mean_overall = np.mean(para_mean_across_layers)\n",
    "para_mean_ci = 1.96 * np.std(np.mean(para_res, axis = 1)).item()\n",
    "\n",
    "# print(f\"Mean across layer transitions: {para_mean_across_layers}\")\n",
    "print(f\"Mean across layer transitions + samples: {para_mean_overall:.2f} +/- {para_mean_ci:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean across layer transitions + samples: 0.84 +/- 0.05\n"
     ]
    }
   ],
   "source": [
    "orth_res = np.stack([get_sample_res(h_orth_by_layer) for _ in range(bootstrap_samples)], axis = 0) # bootstrap_samples x layer_diffs\n",
    "\n",
    "orth_mean_across_layers = orth_res.mean(axis = 0)\n",
    "orth_cis_across_layers = 1.96 * np.std(orth_res, axis = 0)\n",
    "\n",
    "orth_mean_overall = np.mean(orth_mean_across_layers)\n",
    "orth_mean_ci = 1.96 * np.std(np.mean(orth_res, axis = 1)).item()\n",
    "\n",
    "# print(f\"Mean across layer transitions: {orth_mean_across_layers}\")\n",
    "print(f\"Mean across layer transitions + samples: {orth_mean_overall:.2f} +/- {orth_mean_ci:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = pd.DataFrame({\n",
    "    'layer_ix': list(range(model_pre_mlp_layers + 1, len(all_pre_mlp_hs) + model_pre_mlp_layers)),\n",
    "    'para_mean_across_layers': para_mean_across_layers,\n",
    "    'orth_mean_across_layers': orth_mean_across_layers,\n",
    "    'para_cis': para_cis_across_layers,\n",
    "    'orth_cis': orth_cis_across_layers\n",
    "})\n",
    "\n",
    "export_df.to_csv(f'exports/transition-stability-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orth vs Para Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for clustering\n",
    "\"\"\"\n",
    "def print_samples(df, grouping_cols):\n",
    "    \"\"\"\n",
    "    Takes a wide dataframe and groups it, then prints random groups\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        df\\\n",
    "        .groupby(grouping_cols, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .sample(35)\n",
    "    \n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's cluster the para and ortho using k-means and see what clusters we get\n",
    "\"\"\"\n",
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 512):\n",
    "    \"\"\"\n",
    "    K-means clustering\n",
    "    \"\"\"\n",
    "    kmeans_model = cuml.cluster.KMeans(n_clusters = n_clusters, max_iter = 1000, random_state = 123)\n",
    "    kmeans_model.fit(cupy.asarray(layer_hs.to(torch.float32)))\n",
    "    clear_all_cuda_memory(False)\n",
    "\n",
    "    return kmeans_model.labels_.tolist()\n",
    "\n",
    "def get_cluster(sample_df, hidden_states_by_layer, n_clusters = 256):\n",
    "    \"\"\"\n",
    "    Get k-means clusters across hidden state layers\n",
    "    \"\"\"\n",
    "    cluster_ids_by_layer = [\n",
    "        {'layer_ix': layer_ix, 'cluster_ids': cluster_kmeans(layer_hs, n_clusters)} \n",
    "        for layer_ix, layer_hs in tqdm(hidden_states_by_layer.items())\n",
    "    ]\n",
    "\n",
    "    cluster_ids_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cluster_ids_by_layer], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "    \n",
    "    display(\n",
    "        cluster_ids_df.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False)\n",
    "    )\n",
    "\n",
    "    return cluster_ids_df\n",
    "\n",
    "para_clusters_df = get_cluster(sample_df, h_para_by_layer)\n",
    "orth_clusters_df = get_cluster(sample_df, h_orth_by_layer)\n",
    "\n",
    "print_samples(para_clusters_df, ['layer_1_id', 'layer_2_id'])\n",
    "print_samples(orth_clusters_df, ['layer_1_id', 'layer_2_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_samples(para_clusters_df, ['layer_6_id', 'layer_7_id'])\n",
    "print_samples(orth_clusters_df, ['layer_6_id', 'layer_7_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count how many clusters are token-specific\n",
    "\"\"\"\n",
    "def get_single_token_cluster_counts(cluster_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Get how many tokens belong to a single cluster\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        cluster_df\\\n",
    "        .groupby([f'layer_{str(layer_ix)}_id'], as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 20)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .assign(is_eq = lambda df: df.samples.apply(lambda s: 1 if len(set(s)) == 1 else 0))\\\n",
    "        .groupby('is_eq', as_index = False)\\\n",
    "        .agg(count = ('is_eq', 'count'))\n",
    "\n",
    "    return(res)\n",
    "\n",
    "display(get_single_token_cluster_counts(para_clusters_df, 7))\n",
    "display(get_single_token_cluster_counts(orth_clusters_df, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count entropy distribution\n",
    "\"\"\"\n",
    "def get_entropy_distribution(cluster_df, layer_ix, min_cluster_size = 1):\n",
    "    cluster_id_col = f'layer_{str(layer_ix)}_id'\n",
    "\n",
    "    def calculate_dominance(series):\n",
    "        \"\"\"Calculates the proportion of the most frequent item.\"\"\"\n",
    "        if series.empty:\n",
    "            return np.nan\n",
    "        counts = series.value_counts()\n",
    "        return counts.iloc[0] / counts.sum()\n",
    "\n",
    "    def calculate_normalized_entropy(series):\n",
    "        \"\"\"Calculates entropy normalized by log2(n_unique_tokens).\"\"\"\n",
    "        if series.empty:\n",
    "            return np.nan\n",
    "        counts = series.value_counts()\n",
    "        n_unique = len(counts)\n",
    "        \n",
    "        if n_unique <= 1:\n",
    "            return 0.0 # Perfectly pure cluster has zero entropy\n",
    "\n",
    "        ent = scipy.stats.entropy(counts, base=2)\n",
    "        \n",
    "        # Normalize by log2 of the number of unique elements\n",
    "        return ent / np.log2(n_unique)\n",
    "\n",
    "    # Perform aggregation\n",
    "    agg_metrics =\\\n",
    "        cluster_df\\\n",
    "        .groupby(cluster_id_col, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples=('token', 'size'),\n",
    "            n_unique_tokens=('token', 'nunique'),\n",
    "            dominance=('token', calculate_dominance),\n",
    "            normalized_entropy=('token', calculate_normalized_entropy)\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= min_cluster_size])\n",
    "\n",
    "    return agg_metrics\n",
    "\n",
    "para_entropy = get_entropy_distribution(para_clusters_df, 1)\n",
    "orth_entropy = get_entropy_distribution(orth_clusters_df, 1)\n",
    "\n",
    "print(f\"Para entropy: {para_entropy['normalized_entropy'].mean()}\")\n",
    "print(f\"Orth entropy: {orth_entropy['normalized_entropy'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction/probing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic regression - predict topk using h_orth?\n",
    "\"\"\"\n",
    "# Test layer \n",
    "test_layer = 0\n",
    "\n",
    "def run_lr(x_cp, y_cp):\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.1, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 10000, fit_intercept = False)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, expert_ids_cp)\n",
    "run_lr(x_cp_orth, expert_ids_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layer = 2\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, expert_ids_cp)\n",
    "run_lr(x_cp_orth, expert_ids_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict NEXT layer expert ids\n",
    "\"\"\"\n",
    "test_layer = 1\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer + 1])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "# x_cp_ccat = cupy.asarray(torch.cat(\n",
    "#     [h_para_by_layer[test_layer].to(torch.float16).detach().cpu(), h_orth_by_layer[test_layer].to(torch.float16).detach().cpu()],\n",
    "#     dim = 1\n",
    "#     ))\n",
    "\n",
    "run_lr(x_cp_para, expert_ids_cp)\n",
    "run_lr(x_cp_orth, expert_ids_cp)\n",
    "# run_lr(x_cp_ccat, expert_ids_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict token ID\n",
    "\"\"\"\n",
    "display(\n",
    "    sample_df.groupby('token', as_index = False).agg(n = ('token', 'count')).sort_values(by = 'n', ascending = False).head(30)\n",
    ")\n",
    "\n",
    "test_layer = 0\n",
    "\n",
    "y_df =\\\n",
    "    sample_df\\\n",
    "    .assign(is_sample = lambda df: np.where(df['token'].isin([' the']), 1, 0))\\\n",
    "    ['is_sample'].tolist()\n",
    "\n",
    "y_cp = cupy.asarray(y_df)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, y_cp)\n",
    "run_lr(x_cp_orth, y_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Logit lens - take a single prompt and see what the different hidden states are predicting \n",
    "\"\"\"\n",
    "\n",
    "sample_ix = []\n",
    "\n",
    "pre_mlp_hidden_states = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
