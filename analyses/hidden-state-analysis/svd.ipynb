{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to use SVD to decompose hidden states based on whether they're used by routing or not.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "from utils.svd import decompose_orthogonal, decompose_sideways\n",
    "from utils.vis import combine_plots\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_ix = 0\n",
    "models_list = [\n",
    "    ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 0),\n",
    "    ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 0),\n",
    "    ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 1),\n",
    "    ('Qwen/Qwen3-30B-A3B', 'qwen3moe', 0)\n",
    "]\n",
    "\n",
    "model_id, model_prefix, model_pre_mlp_layers = models_list[model_ix]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix, max_data_files = 8):\n",
    "    \"\"\"\n",
    "    Load data saved by `export-activations-sm.ipynb`\n",
    "    \"\"\"\n",
    "    folders = [f'./../export-data/activations-sm/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    all_pre_mlp_hs = []\n",
    "    sample_df = []\n",
    "    topk_df = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_df.append(pd.read_pickle(f'{f}/samples.pkl'))\n",
    "        topk_df.append(pd.read_pickle(f'{f}/topks.pkl'))\n",
    "        all_pre_mlp_hs.append(torch.load(f'{f}/all-pre-mlp-hidden-states.pt'))\n",
    "\n",
    "    sample_df = pd.concat(sample_df)\n",
    "    topk_df = pd.concat(topk_df)\n",
    "    all_pre_mlp_hs = torch.concat(all_pre_mlp_hs)    \n",
    "\n",
    "    with open(f'./../export-data/activations/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    gc.collect()\n",
    "    return sample_df, topk_df, all_pre_mlp_hs, metadata['all_pre_mlp_hidden_states_layers']\n",
    "\n",
    "sample_df_import, topk_df_import, all_pre_mlp_hs_import, act_map = load_data(model_prefix, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .assign(layer_ix = lambda df: df['layer_ix'] + model_pre_mlp_layers)\n",
    "\n",
    "topk1_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "def get_sample_df_for_layer(sample_df, topk_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Helper to take the sample df and merge layer-level expert selection information\n",
    "    \"\"\"\n",
    "    topk_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix])\n",
    "    topk_l1_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 1])\n",
    "    topk_l2_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 2])\n",
    "\n",
    "    layer_df =\\\n",
    "        sample_df\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 1])[['sample_ix', 'expert']], how = 'inner', on = 'sample_ix')\\\n",
    "        .merge(topk_l1_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_l2_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev2_expert'})[['sample_ix', 'prev2_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 2]).rename(columns = {'expert': 'expert2'})[['sample_ix', 'expert2']], how = 'left', on = 'sample_ix')\\\n",
    "        .assign(leading_path = lambda df: df['prev2_expert'] + '-' + df['prev_expert'])\n",
    "    \n",
    "    return layer_df\n",
    "\n",
    "del sample_df_import, sample_df_raw, topk_df_import\n",
    "\n",
    "gc.collect()\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {(layer_ix + model_pre_mlp_layers): all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's take the pre-MLP hidden states and split them using SVD into parallel and orthogonal components.\n",
    "\"\"\"\n",
    "h_para_by_layer = {}\n",
    "h_orth_by_layer = {}\n",
    "\n",
    "for layer_ix in tqdm(list(all_pre_mlp_hs.keys())):\n",
    "    h_para_by_layer[layer_ix], h_orth_by_layer[layer_ix] = decompose_orthogonal(\n",
    "        all_pre_mlp_hs[layer_ix].to(torch.float32),\n",
    "        model.model.layers[layer_ix].mlp.gate.weight.detach().cpu().to(torch.float32),\n",
    "        'svd'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orth vs Para Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_samples = 50\n",
    "\n",
    "def get_sample_res(hs_by_layer, samples_to_test = 20):\n",
    "    \n",
    "    samples = np.random.randint(0, hs_by_layer[0].shape[0], samples_to_test)\n",
    "\n",
    "    # Cast into sample-level list\n",
    "    sample_tensors = torch.stack([layer_hs[samples, :] for _, layer_hs in hs_by_layer.items()], dim = 1).unbind(dim = 0)\n",
    "\n",
    "    sims = []\n",
    "    for s in sample_tensors:\n",
    "        cos_sim = sklearn.metrics.pairwise.cosine_similarity(s)\n",
    "        sims.append(np.diag(cos_sim, 1))\n",
    "\n",
    "    return np.mean(np.stack(sims, axis = 0), axis = 0)\n",
    "\n",
    "para_res = np.stack([get_sample_res(h_para_by_layer) for _ in range(bootstrap_samples)], axis = 0) # bootstrap_samples x layer_diffs\n",
    "\n",
    "para_mean_across_layers = para_res.mean(axis = 0)\n",
    "para_cis_across_layers = 1.96 * np.std(para_mean_across_layers, axis = 0).item()\n",
    "\n",
    "para_mean_overall = np.mean(para_mean_across_layers)\n",
    "para_mean_ci = 1.96 * np.std(np.mean(para_res, axis = 1)).item()\n",
    "\n",
    "print(f\"Mean across layer transitions: {para_mean_across_layers}\")\n",
    "print(f\"Mean across layer transitions + samples: {para_mean_overall:.2f} +/- {para_mean_ci:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orth_res = np.stack([get_sample_res(h_orth_by_layer) for _ in range(bootstrap_samples)], axis = 0) # bootstrap_samples x layer_diffs\n",
    "\n",
    "orth_mean_across_layers = orth_res.mean(axis = 0)\n",
    "orth_cis_across_layers = 1.96 * np.std(orth_mean_across_layers, axis = 0).item()\n",
    "\n",
    "orth_mean_overall = np.mean(orth_mean_across_layers)\n",
    "orth_mean_ci = 1.96 * np.std(np.mean(orth_res, axis = 1)).item()\n",
    "\n",
    "print(f\"Mean across layer transitions: {para_mean_across_layers}\")\n",
    "print(f\"Mean across layer transitions + samples: {orth_mean_overall:.2f} +/- {orth_mean_ci:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tensors = torch.stack([layer_hs[samples, :] for _, layer_hs in h_orth_by_layer.items()], dim = 1).unbind(dim = 0)\n",
    "\n",
    "sims = []\n",
    "for s in sample_tensors:\n",
    "    cos_sim = sklearn.metrics.pairwise.cosine_similarity(s)\n",
    "    sims.append(np.diag(cos_sim, 1))\n",
    "\n",
    "print(f\"Mean across layer transitions: {np.mean(np.stack(sims, axis = 0), axis = 0)}\")\n",
    "print(f\"Mean across samples: {np.mean(np.stack(sims, axis = 0), axis = 1)}\")\n",
    "print(f\"Mean across layer transitions + samples: {np.mean(np.mean(np.stack(sims, axis = 0)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orth vs Para Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for clustering\n",
    "\"\"\"\n",
    "def print_samples(df, grouping_cols):\n",
    "    \"\"\"\n",
    "    Takes a wide dataframe and groups it, then prints random groups\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        df\\\n",
    "        .groupby(grouping_cols, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .sample(35)\n",
    "    \n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's cluster the para and ortho using k-means and see what clusters we get\n",
    "\"\"\"\n",
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 512):\n",
    "    \"\"\"\n",
    "    K-means clustering\n",
    "    \"\"\"\n",
    "    kmeans_model = cuml.cluster.KMeans(n_clusters = n_clusters, max_iter = 1000, random_state = 123)\n",
    "    kmeans_model.fit(cupy.asarray(layer_hs.to(torch.float32)))\n",
    "    clear_all_cuda_memory(False)\n",
    "\n",
    "    return kmeans_model.labels_.tolist()\n",
    "\n",
    "def get_cluster(sample_df, hidden_states_by_layer, n_clusters = 256):\n",
    "    \"\"\"\n",
    "    Get k-means clusters across hidden state layers\n",
    "    \"\"\"\n",
    "    cluster_ids_by_layer = [\n",
    "        {'layer_ix': layer_ix, 'cluster_ids': cluster_kmeans(layer_hs, n_clusters)} \n",
    "        for layer_ix, layer_hs in tqdm(hidden_states_by_layer.items())\n",
    "    ]\n",
    "\n",
    "    cluster_ids_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cluster_ids_by_layer], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "    \n",
    "    display(\n",
    "        cluster_ids_df.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False)\n",
    "    )\n",
    "\n",
    "    return cluster_ids_df\n",
    "\n",
    "para_clusters_df = get_cluster(sample_df, h_para_by_layer)\n",
    "orth_clusters_df = get_cluster(sample_df, h_orth_by_layer)\n",
    "\n",
    "print_samples(para_clusters_df, ['layer_1_id', 'layer_2_id'])\n",
    "print_samples(orth_clusters_df, ['layer_1_id', 'layer_2_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_samples(para_clusters_df, ['layer_6_id', 'layer_7_id'])\n",
    "print_samples(orth_clusters_df, ['layer_6_id', 'layer_7_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count how many clusters are token-specific\n",
    "\"\"\"\n",
    "def get_single_token_cluster_counts(cluster_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Get how many tokens belong to a single cluster\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        cluster_df\\\n",
    "        .groupby([f'layer_{str(layer_ix)}_id'], as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 20)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .assign(is_eq = lambda df: df.samples.apply(lambda s: 1 if len(set(s)) == 1 else 0))\\\n",
    "        .groupby('is_eq', as_index = False)\\\n",
    "        .agg(count = ('is_eq', 'count'))\n",
    "\n",
    "    return(res)\n",
    "\n",
    "display(get_single_token_cluster_counts(para_clusters_df, 7))\n",
    "display(get_single_token_cluster_counts(orth_clusters_df, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count entropy distribution\n",
    "\"\"\"\n",
    "def get_entropy_distribution(cluster_df, layer_ix, min_cluster_size = 1):\n",
    "    cluster_id_col = f'layer_{str(layer_ix)}_id'\n",
    "\n",
    "    def calculate_dominance(series):\n",
    "        \"\"\"Calculates the proportion of the most frequent item.\"\"\"\n",
    "        if series.empty:\n",
    "            return np.nan\n",
    "        counts = series.value_counts()\n",
    "        return counts.iloc[0] / counts.sum()\n",
    "\n",
    "    def calculate_normalized_entropy(series):\n",
    "        \"\"\"Calculates entropy normalized by log2(n_unique_tokens).\"\"\"\n",
    "        if series.empty:\n",
    "            return np.nan\n",
    "        counts = series.value_counts()\n",
    "        n_unique = len(counts)\n",
    "        \n",
    "        if n_unique <= 1:\n",
    "            return 0.0 # Perfectly pure cluster has zero entropy\n",
    "\n",
    "        ent = scipy.stats.entropy(counts, base=2)\n",
    "        \n",
    "        # Normalize by log2 of the number of unique elements\n",
    "        return ent / np.log2(n_unique)\n",
    "\n",
    "    # Perform aggregation\n",
    "    agg_metrics =\\\n",
    "        cluster_df\\\n",
    "        .groupby(cluster_id_col, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples=('token', 'size'),\n",
    "            n_unique_tokens=('token', 'nunique'),\n",
    "            dominance=('token', calculate_dominance),\n",
    "            normalized_entropy=('token', calculate_normalized_entropy)\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= min_cluster_size])\n",
    "\n",
    "    return agg_metrics\n",
    "\n",
    "para_entropy = get_entropy_distribution(para_clusters_df, 1)\n",
    "orth_entropy = get_entropy_distribution(orth_clusters_df, 1)\n",
    "\n",
    "print(f\"Para entropy: {para_entropy['normalized_entropy'].mean()}\")\n",
    "print(f\"Orth entropy: {orth_entropy['normalized_entropy'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction/probing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic regression - predict topk using h_orth?\n",
    "\"\"\"\n",
    "# Test layer \n",
    "test_layer = 0\n",
    "\n",
    "def run_lr(x_cp, y_cp):\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.1, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 10000, fit_intercept = False)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, expert_ids_cp)\n",
    "run_lr(x_cp_orth, expert_ids_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layer = 2\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, expert_ids_cp)\n",
    "run_lr(x_cp_orth, expert_ids_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict NEXT layer expert ids\n",
    "\"\"\"\n",
    "test_layer = 1\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer + 1])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "# x_cp_ccat = cupy.asarray(torch.cat(\n",
    "#     [h_para_by_layer[test_layer].to(torch.float16).detach().cpu(), h_orth_by_layer[test_layer].to(torch.float16).detach().cpu()],\n",
    "#     dim = 1\n",
    "#     ))\n",
    "\n",
    "run_lr(x_cp_para, expert_ids_cp)\n",
    "run_lr(x_cp_orth, expert_ids_cp)\n",
    "# run_lr(x_cp_ccat, expert_ids_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict token ID\n",
    "\"\"\"\n",
    "display(\n",
    "    sample_df.groupby('token', as_index = False).agg(n = ('token', 'count')).sort_values(by = 'n', ascending = False).head(30)\n",
    ")\n",
    "\n",
    "test_layer = 0\n",
    "\n",
    "y_df =\\\n",
    "    sample_df\\\n",
    "    .assign(is_sample = lambda df: np.where(df['token'].isin([' the']), 1, 0))\\\n",
    "    ['is_sample'].tolist()\n",
    "\n",
    "y_cp = cupy.asarray(y_df)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, y_cp)\n",
    "run_lr(x_cp_orth, y_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Logit lens - take a single prompt and see what the different hidden states are predicting \n",
    "\"\"\"\n",
    "\n",
    "sample_ix = []\n",
    "\n",
    "pre_mlp_hidden_states = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
