{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to use SVD to decompose hidden states based on whether they're used by routing or not.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "from utils.svd import decompose_orthogonal, decompose_sideways\n",
    "from utils.vis import combine_plots\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_ix = 0\n",
    "models_list = [\n",
    "    ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 0),\n",
    "    ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 0),\n",
    "    ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 1),\n",
    "    ('Qwen/Qwen3-30B-A3B', 'qwen3moe', 0)\n",
    "]\n",
    "\n",
    "model_id, model_prefix, model_pre_mlp_layers = models_list[model_ix]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix):\n",
    "    all_pre_mlp_hs = torch.load(f'./../export-data/activations/{model_prefix}/all-pre-mlp-hidden-states.pt')\n",
    "    with open(f'./../export-data/activations/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "\n",
    "    return all_pre_mlp_hs, metadata['sample_df'], metadata['topk_df'], metadata['all_pre_mlp_hidden_states_layers']\n",
    "\n",
    "all_pre_mlp_hs_import, sample_df_import, topk_df_import, act_map = load_data(model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "MAX_SAMPLES = 2_000_000\n",
    "\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .assign(layer_ix = lambda df: df['layer_ix'] + model_pre_mlp_layers)\n",
    "\n",
    "topk1_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "def get_sample_df_for_layer(sample_df, topk_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Helper to take the sample df and merge layer-level expert selection information\n",
    "    \"\"\"\n",
    "    topk_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix])\n",
    "    topk_l1_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 1])\n",
    "    topk_l2_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 2])\n",
    "\n",
    "    layer_df =\\\n",
    "        sample_df\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 1])[['sample_ix', 'expert']], how = 'inner', on = 'sample_ix')\\\n",
    "        .merge(topk_l1_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_l2_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev2_expert'})[['sample_ix', 'prev2_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 2]).rename(columns = {'expert': 'expert2'})[['sample_ix', 'expert2']], how = 'left', on = 'sample_ix')\\\n",
    "        .assign(leading_path = lambda df: df['prev2_expert'] + '-' + df['prev_expert'])\n",
    "    \n",
    "    return layer_df\n",
    "\n",
    "del sample_df_import, sample_df_raw, topk_df_import\n",
    "\n",
    "all_pre_mlp_hs_import = all_pre_mlp_hs_import[0:MAX_SAMPLES, :, :]\n",
    "sample_df = sample_df[sample_df['sample_ix'] < MAX_SAMPLES]\n",
    "topk_df = topk_df[topk_df['sample_ix'] < MAX_SAMPLES]\n",
    "topk1_df = topk1_df[topk1_df['sample_ix'] < MAX_SAMPLES]\n",
    "\n",
    "gc.collect()\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {(layer_ix + model_pre_mlp_layers): all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's take the pre-MLP hidden states and split them using SVD into parallel and orthogonal components.\n",
    "\"\"\"\n",
    "h_para_by_layer = {}\n",
    "h_orth_by_layer = {}\n",
    "\n",
    "for layer_ix in tqdm(list(all_pre_mlp_hs.keys())):\n",
    "    h_para_by_layer[layer_ix], h_orth_by_layer[layer_ix] = decompose_orthogonal(\n",
    "        all_pre_mlp_hs[layer_ix].to(torch.float32),\n",
    "        model.model.layers[layer_ix].mlp.gate.weight.detach().cpu().to(torch.float32),\n",
    "        'svd'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orth vs Para Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to average firrst\n",
    "\n",
    "para_means = torch.stack(\n",
    "    [torch.mean(layer_hs[0:100, ], dim = 0) for _, layer_hs in tqdm(sorted(h_para_by_layer.items()))],\n",
    "    dim = 0\n",
    ")\n",
    "\n",
    "# para_means = para_means - para_means.mean(dim = 0)\n",
    "\n",
    "para_sim = sklearn.metrics.pairwise.cosine_similarity(para_means.numpy())\n",
    "para_sim[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_sim[np.triu_indices(para_sim.shape[0], k = 1)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orth_means = torch.stack(\n",
    "    [torch.mean(layer_hs[0:100], dim = 0) for _, layer_hs in tqdm(sorted(h_orth_by_layer.items()))],\n",
    "    dim = 0\n",
    ")\n",
    "\n",
    "# orth_means = orth_means - orth_means.mean(dim = 0)\n",
    "\n",
    "orth_sim = sklearn.metrics.pairwise.cosine_similarity(orth_means.numpy())\n",
    "orth_sim[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orth_sim[np.triu_indices(orth_sim.shape[0], k = 1)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_para_prev = h_para_by_layer[8] # shape (T,Dh)  from layer ℓ-1\n",
    "h_para_curr = h_para_by_layer[4] # shape (T,Dh)  from layer ℓ\n",
    "cos = (h_para_prev * h_para_curr).sum(1) / (\n",
    "        h_para_prev.norm(dim=1) * h_para_curr.norm(dim=1) + 1e-9)\n",
    "theta = torch.acos(torch.clamp(cos, -1.0, 1.0))  # radians\n",
    "stay = (expert_id_prev == expert_id_curr)\n",
    "print(theta[stay].mean(), theta[~stay].mean())\n",
    "\n",
    "# Large gap ⇒ token-wise header rewrite when the path branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Should we then project the deltas onto the next layer's router? Would that be worth the effort?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orth vs Para Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for clustering\n",
    "\"\"\"\n",
    "def print_samples(df, grouping_cols):\n",
    "    \"\"\"\n",
    "    Takes a wide dataframe and groups it, then prints random groups\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        df\\\n",
    "        .groupby(grouping_cols, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .sample(35)\n",
    "    \n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's cluster the para and ortho using k-means and see what clusters we get\n",
    "\"\"\"\n",
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 512):\n",
    "    \"\"\"\n",
    "    K-means clustering\n",
    "    \"\"\"\n",
    "    kmeans_model = cuml.cluster.KMeans(n_clusters = n_clusters, max_iter = 1000, random_state = 123)\n",
    "    kmeans_model.fit(cupy.asarray(layer_hs.to(torch.float32)))\n",
    "    clear_all_cuda_memory(False)\n",
    "\n",
    "    return kmeans_model.labels_.tolist()\n",
    "\n",
    "def get_cluster(sample_df, hidden_states_by_layer, n_clusters = 256):\n",
    "    \"\"\"\n",
    "    Get k-means clusters across hidden state layers\n",
    "    \"\"\"\n",
    "    cluster_ids_by_layer = [\n",
    "        {'layer_ix': layer_ix, 'cluster_ids': cluster_kmeans(layer_hs, n_clusters)} \n",
    "        for layer_ix, layer_hs in tqdm(hidden_states_by_layer.items())\n",
    "    ]\n",
    "\n",
    "    cluster_ids_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cluster_ids_by_layer], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "    \n",
    "    display(\n",
    "        cluster_ids_df.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False)\n",
    "    )\n",
    "\n",
    "    return cluster_ids_df\n",
    "\n",
    "para_clusters_df = get_cluster(sample_df, h_para_by_layer)\n",
    "orth_clusters_df = get_cluster(sample_df, h_orth_by_layer)\n",
    "\n",
    "print_samples(para_clusters_df, ['layer_1_id', 'layer_2_id'])\n",
    "print_samples(orth_clusters_df, ['layer_1_id', 'layer_2_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_samples(para_clusters_df, ['layer_6_id', 'layer_7_id'])\n",
    "print_samples(orth_clusters_df, ['layer_6_id', 'layer_7_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count how many clusters are token-specific\n",
    "\"\"\"\n",
    "def get_single_token_cluster_counts(cluster_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Get how many tokens belong to a single cluster\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        cluster_df\\\n",
    "        .groupby([f'layer_{str(layer_ix)}_id'], as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 20)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .assign(is_eq = lambda df: df.samples.apply(lambda s: 1 if len(set(s)) == 1 else 0))\\\n",
    "        .groupby('is_eq', as_index = False)\\\n",
    "        .agg(count = ('is_eq', 'count'))\n",
    "\n",
    "    return(res)\n",
    "\n",
    "display(get_single_token_cluster_counts(para_clusters_df, 7))\n",
    "display(get_single_token_cluster_counts(orth_clusters_df, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count entropy distribution\n",
    "\"\"\"\n",
    "def get_entropy_distribution(cluster_df, layer_ix, min_cluster_size = 1):\n",
    "    cluster_id_col = f'layer_{str(layer_ix)}_id'\n",
    "\n",
    "    def calculate_dominance(series):\n",
    "        \"\"\"Calculates the proportion of the most frequent item.\"\"\"\n",
    "        if series.empty:\n",
    "            return np.nan\n",
    "        counts = series.value_counts()\n",
    "        return counts.iloc[0] / counts.sum()\n",
    "\n",
    "    def calculate_normalized_entropy(series):\n",
    "        \"\"\"Calculates entropy normalized by log2(n_unique_tokens).\"\"\"\n",
    "        if series.empty:\n",
    "            return np.nan\n",
    "        counts = series.value_counts()\n",
    "        n_unique = len(counts)\n",
    "        \n",
    "        if n_unique <= 1:\n",
    "            return 0.0 # Perfectly pure cluster has zero entropy\n",
    "\n",
    "        ent = scipy.stats.entropy(counts, base=2)\n",
    "        \n",
    "        # Normalize by log2 of the number of unique elements\n",
    "        return ent / np.log2(n_unique)\n",
    "\n",
    "    # Perform aggregation\n",
    "    agg_metrics =\\\n",
    "        cluster_df\\\n",
    "        .groupby(cluster_id_col, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples=('token', 'size'),\n",
    "            n_unique_tokens=('token', 'nunique'),\n",
    "            dominance=('token', calculate_dominance),\n",
    "            normalized_entropy=('token', calculate_normalized_entropy)\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= min_cluster_size])\n",
    "\n",
    "    return agg_metrics\n",
    "\n",
    "para_entropy = get_entropy_distribution(para_clusters_df, 1)\n",
    "orth_entropy = get_entropy_distribution(orth_clusters_df, 1)\n",
    "\n",
    "print(f\"Para entropy: {para_entropy['normalized_entropy'].mean()}\")\n",
    "print(f\"Orth entropy: {orth_entropy['normalized_entropy'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction/probing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic regression - predict topk using h_orth?\n",
    "\"\"\"\n",
    "# Test layer \n",
    "test_layer = 0\n",
    "\n",
    "def run_lr(x_cp, y_cp):\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.1, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 10000, fit_intercept = False)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, expert_ids_cp)\n",
    "run_lr(x_cp_orth, expert_ids_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layer = 2\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, expert_ids_cp)\n",
    "run_lr(x_cp_orth, expert_ids_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict NEXT layer expert ids\n",
    "\"\"\"\n",
    "test_layer = 1\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer + 1])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "# x_cp_ccat = cupy.asarray(torch.cat(\n",
    "#     [h_para_by_layer[test_layer].to(torch.float16).detach().cpu(), h_orth_by_layer[test_layer].to(torch.float16).detach().cpu()],\n",
    "#     dim = 1\n",
    "#     ))\n",
    "\n",
    "run_lr(x_cp_para, expert_ids_cp)\n",
    "run_lr(x_cp_orth, expert_ids_cp)\n",
    "# run_lr(x_cp_ccat, expert_ids_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict token ID\n",
    "\"\"\"\n",
    "display(\n",
    "    sample_df.groupby('token', as_index = False).agg(n = ('token', 'count')).sort_values(by = 'n', ascending = False).head(30)\n",
    ")\n",
    "\n",
    "test_layer = 0\n",
    "\n",
    "y_df =\\\n",
    "    sample_df\\\n",
    "    .assign(is_sample = lambda df: np.where(df['token'].isin([' the']), 1, 0))\\\n",
    "    ['is_sample'].tolist()\n",
    "\n",
    "y_cp = cupy.asarray(y_df)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, y_cp)\n",
    "run_lr(x_cp_orth, y_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analyze stability over layers\n",
    "\"\"\"\n",
    "def calculate_layer_transition_stability(h_orth_layers: dict, h_para_layers: dict, layer_l: int):\n",
    "    \"\"\"\n",
    "    Calculates the stability of h_orth and h_para representations between layer_l and layer_l+1 using cosine similarity and Euclidean distance.\n",
    "\n",
    "    Params:\n",
    "        @h_orth_layers: Dictionary where keys are layer indices (int) and values are (n_samples, D) tensors for h_orth.\n",
    "        @h_para_layers: Dictionary where keys are layer indices (int) andvalues are (n_samples, D) tensors for h_para.\n",
    "        layer_l: The starting layer index for the transition (e.g., 6 for L6->L7).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing:\n",
    "        - 'cosine_similarity_orth': (n_samples,) tensor of cosine similarities for h_orth.\n",
    "        - 'cosine_similarity_para': (n_samples,) tensor of cosine similarities for h_para.\n",
    "        - 'euclidean_distance_orth': (n_samples,) tensor of L2 distances for h_orth.\n",
    "        - 'euclidean_distance_para': (n_samples,) tensor of L2 distances for h_para.\n",
    "    \"\"\"\n",
    "    layer_lp1 = layer_l + 1\n",
    "\n",
    "    # Get tensors for the specified layers\n",
    "    h_orth_l = h_orth_layers[layer_l]\n",
    "    h_orth_lp1 = h_orth_layers[layer_lp1]\n",
    "    h_para_l = h_para_layers[layer_l]\n",
    "    h_para_lp1 = h_para_layers[layer_lp1]\n",
    "\n",
    "    # --- Calculate Cosine Similarities (Higher is more stable) ---\n",
    "    # dim = 1 calculates similarity row-wise\n",
    "    sim_orth = torch.nn.functional.cosine_similarity(h_orth_l, h_orth_lp1, dim = 1)\n",
    "    sim_para = torch.nn.functional.cosine_similarity(h_para_l, h_para_lp1, dim = 1)\n",
    "\n",
    "    # --- Calculate Euclidean Distances (Lower is more stable) ---\n",
    "    # torch.linalg.norm computes norms. ord=2 is L2 norm.\n",
    "    dist_orth = torch.linalg.norm(h_orth_l - h_orth_lp1, ord = 2, dim = 1)\n",
    "    dist_para = torch.linalg.norm(h_para_l - h_para_lp1, ord = 2, dim = 1)\n",
    "\n",
    "    return {\n",
    "        'cosine_similarity_orth': sim_orth.to(torch.float16),\n",
    "        'cosine_similarity_para': sim_para.to(torch.float16),\n",
    "        'euclidean_distance_orth': dist_orth.to(torch.float16),\n",
    "        'euclidean_distance_para': dist_para.to(torch.float16),\n",
    "    }\n",
    "\n",
    "stability_results = calculate_layer_transition_stability(\n",
    "    h_orth_layers = {\n",
    "        0: h_orth_by_layer[0].to(torch.float32).detach().cpu(),\n",
    "        1: h_orth_by_layer[1].to(torch.float32).detach().cpu()\n",
    "    },\n",
    "    h_para_layers = {\n",
    "        0: h_para_by_layer[0].to(torch.float32).detach().cpu(), \n",
    "        1: h_para_by_layer[1].to(torch.float32).detach().cpu() \n",
    "    },\n",
    "    layer_l = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking sim_orth for non-finite values:\", torch.isfinite(sim_orth).all())\n",
    "print(\"Checking dist_orth for non-finite values:\", torch.isfinite(dist_orth).all())\n",
    "    \n",
    "\n",
    "print(\"Zero vectors in h_orth_l:\", torch.where(torch.linalg.norm(h_orth_l, dim=1) == 0))\n",
    "print(\"Zero vectors in h_orth_lp1:\", torch.where(torch.linalg.norm(h_orth_lp1, dim=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_orth_np = stability_results['cosine_similarity_orth'].numpy()\n",
    "sim_para_np = stability_results['cosine_similarity_para'].numpy()\n",
    "dist_orth_np = stability_results['euclidean_distance_orth'].numpy()\n",
    "dist_para_np = stability_results['euclidean_distance_para'].numpy()\n",
    "\n",
    "# (Keep the print statements for descriptive statistics as before)\n",
    "\n",
    "print(\"\\nCosine Similarity (Higher is More Stable):\")\n",
    "print(f\"  h_orth: Mean={np.mean(sim_orth_np):.4f}, Median={np.median(sim_orth_np):.4f}, Std={np.std(sim_orth_np):.4f}\")\n",
    "print(f\"  h_para: Mean={np.mean(sim_para_np):.4f}, Median={np.median(sim_para_np):.4f}, Std={np.std(sim_para_np):.4f}\")\n",
    "print(\"\\nEuclidean Distance (Lower is More Stable):\")\n",
    "print(f\"  h_orth: Mean={np.mean(dist_orth_np):.4f}, Median={np.median(dist_orth_np):.4f}, Std={np.std(dist_orth_np):.4f}\")\n",
    "print(f\"  h_para: Mean={np.mean(dist_para_np):.4f}, Median={np.median(dist_para_np):.4f}, Std={np.std(dist_para_np):.4f}\")\n",
    "\n",
    "layer_idx = 0\n",
    "\n",
    "data_sim = pd.DataFrame({\n",
    "    'value': np.concatenate([sim_orth_np, sim_para_np]),\n",
    "    'component': ['h_orth'] * len(sim_orth_np) + ['h_para'] * len(sim_para_np),\n",
    "    'metric': 'cos'\n",
    "})\n",
    "\n",
    "data_dist = pd.DataFrame({\n",
    "    'value': np.concatenate([dist_orth_np, dist_para_np]),\n",
    "    'component': ['h_orth'] * len(dist_orth_np) + ['h_para'] * len(dist_para_np),\n",
    "    'metric': 'euc'\n",
    "})\n",
    "\n",
    "df_plot = pd.concat([data_sim, data_dist], ignore_index=True)\n",
    "\n",
    "fig = px.histogram(\n",
    "    df_plot,\n",
    "    x = 'value', # Values for the histogram\n",
    "    color = 'component', # Creates separate histograms for 'h_orth' and 'h_para'\n",
    "    facet_col = 'metric', # Creates separate subplots (columns) for each metric type\n",
    "    histnorm = 'probability density', # Normalize histograms\n",
    "    barmode = 'overlay', # Overlay histograms within each subplot\n",
    "    opacity = 0.75,\n",
    "    title = f'Stability Comparison: Layer {layer_idx} to {layer_idx+1}',\n",
    "    labels = {'component': 'Component Type'} \n",
    "    )\\\n",
    "    .update_xaxes(title_text = \"Cosine Similarity\", col = 1)\\\n",
    "    .update_xaxes(title_text = \"Euclidean Distance\", col = 2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# --- Statistical Test (Example: Mann-Whitney U test) ---\n",
    "u_stat_sim, p_value_sim = scipy.stats.mannwhitneyu(sim_orth_np, sim_para_np, alternative='greater') # Test if orth > para\n",
    "u_stat_dist, p_value_dist = scipy.stats.mannwhitneyu(dist_para_np, dist_orth_np, alternative='greater') # Test if para > orth\n",
    "print(\"\\nMann-Whitney U Test Results:\")\n",
    "print(f\"  Cosine Similarity (H1: orth > para): p-value = {p_value_sim:.2e} | rejection = h_orth sim is higher\")\n",
    "print(f\"  Euclidean Distance (H1: para > orth): p-value = {p_value_dist:.2e} | rejection = h_orth distance is lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Logit lens - take a single prompt and see what the different hidden states are predicting \n",
    "\"\"\"\n",
    "\n",
    "sample_ix = []\n",
    "\n",
    "pre_mlp_hidden_states = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
