{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to use SVD to decompose hidden states based on whether they're used by routing or not.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "from utils.svd import decompose_orthogonal, decompose_sideways\n",
    "from utils.vis import combine_plots\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()\n",
    "\n",
    "ws = '/workspace/interpretable-moes-analysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "selected_model_index = 4\n",
    "\n",
    "def get_model(index):\n",
    "    # Returns:\n",
    "    # - model_id: HF model id\n",
    "    # - model_prefix: Name to save it under\n",
    "    # - model_architecture: Architectures as named in utils/pretrained_models/*\n",
    "    # - modle_attn: The attention implementation to use; None to use default\n",
    "    # - model_use_hf: True=use HF build-in implementation, False=use modeling_* from model repo\n",
    "    # - model_n_moe_layers: Number of total MoE layers\n",
    "    # - model_n_dense_layers: Number of dense layers before the MoE layers\n",
    "    models = {\n",
    "        0: ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 'olmoe', None, True, 16, 0),\n",
    "        1: ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 'qwen2moe', None, True, 24, 0),\n",
    "        2: ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 'dsv2', None, True, 26, 1),\n",
    "        3: ('moonshotai/Moonlight-16B-A3B', 'moonlight', 'dsv3', None, True, 26, 1),\n",
    "        4: ('Qwen/Qwen3-30B-A3B-Instruct-2507', 'qwen3moe', 'qwen3moe', None, True, 48, 0),\n",
    "        5: ('moonshotai/Kimi-VL-A3B-Instruct', 'kimivl', 'kimivl', None, False, 26, 1),\n",
    "        6: ('ibm-granite/granite-4.0-tiny-preview', 'granite', 'granite', None, True, 40, 0),\n",
    "        7: ('zai-org/GLM-4.5-Air-FP8', 'glm4moe', 'glm4moe', None, True, 45, 1),\n",
    "        8: ('openai/gpt-oss-120b', 'gptoss120', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 36, 0), # Will load experts in MXFP4 if triton kernels installed\n",
    "        8.1: ('openai/gpt-oss-20b', 'gptoss20', 'gptoss', 'kernels-community/vllm-flash-attn3', True, 24, 0),\n",
    "        9: ('inclusionAI/Ring-mini-2.0', 'ringmini2', 'ringmini2', None, False, 19, 1)\n",
    "    }\n",
    "    return models[index]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from HF, or from file if already downloaded.\n",
    "    \"\"\"\n",
    "    cache_dir = '/workspace/hf'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir = cache_dir, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir = cache_dir, dtype = torch.bfloat16, trust_remote_code = not model_use_hf, device_map = 'auto', attn_implementation = model_attn).eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "model_id, model_prefix, model_architecture, model_attn, model_use_hf, model_n_moe_layers, model_n_dense_layers = get_model(selected_model_index)\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_prefix, model_attn, model_use_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix, max_data_files):\n",
    "    \"\"\"\n",
    "    Load data saved by `export-activations.ipynb`\n",
    "    \"\"\"\n",
    "    folders = [f'{ws}/analyses/export-data/activations/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    all_pre_mlp_hs = []\n",
    "    sample_df = []\n",
    "    topk_df = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_df.append(pd.read_pickle(f'{f}/samples.pkl'))\n",
    "        topk_df.append(pd.read_pickle(f'{f}/topks.pkl'))\n",
    "        all_pre_mlp_hs.append(torch.load(f'{f}/all-pre-mlp-hidden-states.pt'))\n",
    "\n",
    "    sample_df = pd.concat(sample_df)\n",
    "    topk_df = pd.concat(topk_df)\n",
    "    all_pre_mlp_hs = torch.concat(all_pre_mlp_hs)    \n",
    "\n",
    "    with open(f'{ws}/analyses/export-data/activations/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    gc.collect()\n",
    "    return sample_df, topk_df, all_pre_mlp_hs, metadata['all_pre_mlp_hidden_states_layers']\n",
    "\n",
    "# Due to mem constraints, for Qwen3Moe max_data_files = 3\n",
    "sample_df_import, topk_df_import, all_pre_mlp_hs_import, act_map = load_data(model_prefix, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .assign(layer_ix = lambda df: df['layer_ix'] + model_n_dense_layers)\n",
    "\n",
    "topk1_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "def get_sample_df_for_layer(sample_df, topk_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Helper to take the sample df and merge layer-level expert selection information\n",
    "    \"\"\"\n",
    "    topk_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix])\n",
    "    topk_l1_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 1])\n",
    "    topk_l2_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 2])\n",
    "\n",
    "    layer_df =\\\n",
    "        sample_df\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 1])[['sample_ix', 'expert']], how = 'inner', on = 'sample_ix')\\\n",
    "        .merge(topk_l1_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_l2_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev2_expert'})[['sample_ix', 'prev2_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 2]).rename(columns = {'expert': 'expert2'})[['sample_ix', 'expert2']], how = 'left', on = 'sample_ix')\\\n",
    "        .assign(leading_path = lambda df: df['prev2_expert'] + '-' + df['prev_expert'])\n",
    "    \n",
    "    return layer_df\n",
    "\n",
    "del sample_df_import, sample_df_raw, topk_df_import\n",
    "\n",
    "gc.collect()\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {(layer_ix + model_n_dense_layers): all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's take the pre-MLP hidden states and split them using SVD into parallel and orthogonal components.\n",
    "\"\"\"\n",
    "h_para_by_layer = {}\n",
    "h_orth_by_layer = {}\n",
    "\n",
    "for layer_ix in tqdm(list(all_pre_mlp_hs.keys())):\n",
    "    if model_prefix in ['gptoss20', 'gptoss120']:\n",
    "        gate_obj = model.model.layers[layer_ix].mlp.router.weight\n",
    "    elif model_prefix == 'kimivl': \n",
    "        gate_obj = model.language_model.model.layers[layer_ix].mlp.gate.weight\n",
    "    elif model_prefix == 'granite':\n",
    "        gate_obj = model.model.layers[layer_ix].block_sparse_moe.router.layer.weight\n",
    "    else:\n",
    "        gate_obj = model.model.layers[layer_ix].mlp.gate.weight\n",
    "\n",
    "    h_para_by_layer[layer_ix], h_orth_by_layer[layer_ix] = decompose_orthogonal(\n",
    "        all_pre_mlp_hs[layer_ix].to(torch.float32),\n",
    "        gate_obj.detach().cpu().to(torch.float32),\n",
    "        'svd'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orth vs Para Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get row-space rotation stability\n",
    "\"\"\"\n",
    "bootstrap_samples = 50\n",
    "\n",
    "def get_sample_res(hs_by_layer, samples_to_test = 1):\n",
    "    \n",
    "    samples = np.random.randint(0, hs_by_layer[1].shape[0], samples_to_test)\n",
    "\n",
    "    # Cast into sample-level list\n",
    "    sample_tensors = torch.stack([layer_hs[samples, :] for _, layer_hs in hs_by_layer.items()], dim = 1).unbind(dim = 0)\n",
    "\n",
    "    sims = []\n",
    "    for s in sample_tensors:\n",
    "        cos_sim = sklearn.metrics.pairwise.cosine_similarity(s)\n",
    "        sims.append(np.diag(cos_sim, 1))\n",
    "\n",
    "    return np.mean(np.stack(sims, axis = 0), axis = 0)\n",
    "\n",
    "para_res = np.stack([get_sample_res(h_para_by_layer) for _ in range(bootstrap_samples)], axis = 0) # bootstrap_samples x layer_diffs\n",
    "\n",
    "para_mean_across_layers = para_res.mean(axis = 0)\n",
    "para_cis_across_layers = 1.96 * np.std(para_res, axis = 0)\n",
    "\n",
    "para_mean_overall = np.mean(para_mean_across_layers)\n",
    "para_mean_ci = 1.96 * np.std(np.mean(para_res, axis = 1)).item()\n",
    "\n",
    "# print(f\"Mean across layer transitions: {para_mean_across_layers}\")\n",
    "print(f\"Mean across layer transitions + samples: {para_mean_overall:.2f} +/- {para_mean_ci:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get null-space rotation stability\n",
    "\"\"\"\n",
    "orth_res = np.stack([get_sample_res(h_orth_by_layer) for _ in range(bootstrap_samples)], axis = 0) # bootstrap_samples x layer_diffs\n",
    "\n",
    "orth_mean_across_layers = orth_res.mean(axis = 0)\n",
    "orth_cis_across_layers = 1.96 * np.std(orth_res, axis = 0)\n",
    "\n",
    "orth_mean_overall = np.mean(orth_mean_across_layers)\n",
    "orth_mean_ci = 1.96 * np.std(np.mean(orth_res, axis = 1)).item()\n",
    "\n",
    "# print(f\"Mean across layer transitions: {orth_mean_across_layers}\")\n",
    "print(f\"Mean across layer transitions + samples: {orth_mean_overall:.2f} +/- {orth_mean_ci:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "export_df = pd.DataFrame({\n",
    "    'layer_ix_1': list(range(model_n_dense_layers + 1, len(all_pre_mlp_hs) + model_n_dense_layers)), # +1 to 1 index\n",
    "    'para_mean_across_layers': para_mean_across_layers,\n",
    "    'orth_mean_across_layers': orth_mean_across_layers,\n",
    "    'para_cis': para_cis_across_layers,\n",
    "    'orth_cis': orth_cis_across_layers\n",
    "})\n",
    "\n",
    "export_df.to_csv(f'{ws}/analyses/hidden-state-analysis/exports/svd-analysis/svd-transition-stability-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction/probing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic regression - predict expert ID\n",
    "\"\"\"\n",
    "def run_lr(x_cp, y_cp):\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.2, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 1000, fit_intercept = True)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    return accuracy\n",
    "\n",
    "def run_lr_with_mi(x_cp, y_cp):\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.2, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 1000, fit_intercept = True)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    train_acc = lr_model.score(x_train, y_train)\n",
    "    y_actual_np = cupy.asnumpy(y_test)\n",
    "    y_pred_np = cupy.asnumpy(lr_model.predict(x_test))\n",
    "    mi = sklearn.metrics.mutual_info_score(y_actual_np, y_pred_np) # nats\n",
    "    max_entropy = sklearn.metrics.mutual_info_score(y_actual_np, y_actual_np) # H(y)\n",
    "    return accuracy, mi, max_entropy, train_acc\n",
    "\n",
    "current_layer_accuracy = []\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())):\n",
    "    expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "\n",
    "    expert_ids_cp = cupy.asarray(expert_ids)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "    para_res = run_lr_with_mi(x_cp_para, expert_ids_cp)\n",
    "    orth_res = run_lr_with_mi(x_cp_orth, expert_ids_cp)\n",
    "\n",
    "    current_layer_accuracy.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_res[0],\n",
    "        'para_train_acc': para_res[3],\n",
    "        'para_mi_bits': para_res[1]/np.log(2.0), # Convert from nats to bits\n",
    "        'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "        'para_mi_pct': para_res[1]/para_res[2],\n",
    "        'orth_acc': orth_res[0],\n",
    "        'para_train_acc': para_res[3],\n",
    "        'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "        'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "        'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "    })\n",
    "\n",
    "pd.DataFrame(current_layer_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict NEXT layer expert ids (note - this does not remove expert info, remove below)\n",
    "\"\"\"\n",
    "# next_layer_accuracy = []\n",
    "# for test_layer in tqdm(list(h_para_by_layer.keys())[:-1]):\n",
    "#     expert_ids =\\\n",
    "#         topk_df\\\n",
    "#         .pipe(lambda df: df[df['layer_ix'] == test_layer + 1])\\\n",
    "#         .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "#         ['expert'].tolist()\n",
    "\n",
    "#     expert_ids_cp = cupy.asarray(expert_ids)\n",
    "#     x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "#     x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "#     para_res = run_lr_with_mi(x_cp_para, expert_ids_cp)\n",
    "#     orth_res = run_lr_with_mi(x_cp_orth, expert_ids_cp)\n",
    "    \n",
    "#     next_layer_accuracy.append({\n",
    "#         'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "#         'para_acc': para_res[0],\n",
    "#         'para_mi_bits': para_res[1]/np.log(2.0),\n",
    "#         'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "#         'para_mi_pct': para_res[1]/para_res[2],\n",
    "#         'orth_acc': orth_res[0],\n",
    "#         'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "#         'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "#         'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "#     })\n",
    "\n",
    "# pd.DataFrame(next_layer_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict NEXT layer expert ids. Remove expert centroids first.\n",
    "\"\"\"\n",
    "centroids_para = {}\n",
    "centroids_orth = {}\n",
    "\n",
    "# Get current-layer expert IDs for layer\n",
    "for layer_ix in h_para_by_layer.keys():\n",
    "\n",
    "    cur_layer_expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == layer_ix])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "\n",
    "    cur_layer_expert_ids_cp = cupy.asarray(cur_layer_expert_ids)\n",
    "\n",
    "    # H_para/h_orth for layer\n",
    "    h_para_cp = cupy.asarray(h_para_by_layer[layer_ix].to(torch.float16).detach().cpu())\n",
    "    h_orth_cp = cupy.asarray(h_orth_by_layer[layer_ix].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Compute centroids per expert id\n",
    "    centroids_para[layer_ix] = {}\n",
    "    centroids_orth[layer_ix] = {}\n",
    "\n",
    "    for e in set(cur_layer_expert_ids):\n",
    "        idx_cp = cupy.where(cur_layer_expert_ids_cp == e)[0]\n",
    "        centroids_para[layer_ix][e] = h_para_cp[idx_cp].mean(axis = 0)\n",
    "        centroids_orth[layer_ix][e] = h_orth_cp[idx_cp].mean(axis = 0)\n",
    "\n",
    "next_layer_accuracy_cond = []\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[:-1]):\n",
    "    # Target = next-layer slot-1 expert IDs (same as before)\n",
    "    y_cp =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer + 1])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "    y_cp = cupy.asarray(y_cp)\n",
    "\n",
    "    # Current-layer top-1 expert IDs - needed for residual lookup\n",
    "    cur_exp_cp =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "    cur_exp_cp = cupy.asarray(cur_exp_cp)\n",
    "\n",
    "    # Pull h_para / h_orth tensors and convert to cupy\n",
    "    h_para_cp = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    h_orth_cp = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Subtract extract centroids\n",
    "    mu_para_mat = cupy.stack([centroids_para[test_layer][int(e)] for e in cur_exp_cp])\n",
    "    mu_orth_mat = cupy.stack([centroids_orth[test_layer][int(e)] for e in cur_exp_cp])\n",
    "    h_para_res = h_para_cp - mu_para_mat\n",
    "    h_orth_res = h_orth_cp - mu_orth_mat\n",
    "\n",
    "    # Run the unchanged probe\n",
    "    para_res = run_lr_with_mi(h_para_res, y_cp)\n",
    "    orth_res = run_lr_with_mi(h_orth_res, y_cp)\n",
    "\n",
    "    next_layer_accuracy_cond.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_res[0],\n",
    "        'para_mi_bits': para_res[1]/np.log(2.0),\n",
    "        'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "        'para_mi_pct': para_res[1]/para_res[2],\n",
    "        'orth_acc': orth_res[0],\n",
    "        'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "        'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "        'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(next_layer_accuracy_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export results\n",
    "\"\"\"\n",
    "layer_transitions_export_df = pd.concat([\n",
    "    pd.DataFrame(current_layer_accuracy).assign(target = 'current_layer'),\n",
    "    pd.DataFrame(next_layer_accuracy_cond).assign(target = 'next_layer')\n",
    "]).assign(model = model_prefix)\n",
    "\n",
    "display(layer_transitions_export_df)\n",
    "\n",
    "layer_transitions_export_df.to_csv(f'{ws}/analyses/hidden-state-analysis/exports/svd-analysis/svd-probe-expert-id-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict language - presplit, seperate TIDs\n",
    "\"\"\"\n",
    "# def run_lr_with_mi_presplit(x_train, x_test, y_train, y_test):\n",
    "#     lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 1000, fit_intercept = True)\n",
    "#     lr_model.fit(x_train, y_train)\n",
    "#     accuracy = lr_model.score(x_test, y_test)\n",
    "#     train_acc = lr_model.score(x_train, y_train)\n",
    "#     y_actual_np = cupy.asnumpy(y_test)\n",
    "#     y_pred_np = cupy.asnumpy(lr_model.predict(x_test))\n",
    "#     mi = sklearn.metrics.mutual_info_score(y_actual_np, y_pred_np) # nats\n",
    "#     max_entropy = sklearn.metrics.mutual_info_score(y_actual_np, y_actual_np) # H(y)\n",
    "#     return accuracy, mi.item(), max_entropy.item(), train_acc\n",
    "\n",
    "# lang_probe_accs = []\n",
    "# # Split train/test, different TIDs in each\n",
    "# gss = sklearn.model_selection.GroupShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 123)\n",
    "# train_ix, test_ix = next(gss.split(sample_df, groups = sample_df['token_id']))\n",
    "\n",
    "# train_sample_df = sample_df.take(train_ix)\n",
    "# test_sample_df = sample_df.take(test_ix)\n",
    "\n",
    "# # Prep y values\n",
    "# source_mapping = {source: i for i, source in enumerate(sample_df['source'].unique())}\n",
    "\n",
    "# y_train = cupy.asarray(train_sample_df.assign(source = lambda df: df['source'].map(source_mapping))['source'].tolist())\n",
    "# y_test = cupy.asarray(test_sample_df.assign(source = lambda df: df['source'].map(source_mapping))['source'].tolist())\n",
    "\n",
    "# for test_layer in tqdm(list(h_para_by_layer.keys())[::2]):\n",
    "\n",
    "#     x_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "#     x_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    \n",
    "#     x_train_para = x_para[train_sample_df['sample_ix'].tolist(), :]\n",
    "#     x_test_para = x_para[test_sample_df['sample_ix'].tolist(), :]\n",
    "#     x_train_orth = x_orth[train_sample_df['sample_ix'].tolist(), :]\n",
    "#     x_test_orth = x_orth[test_sample_df['sample_ix'].tolist(), :]\n",
    "\n",
    "#     para_res = run_lr_with_mi_presplit(x_train_para, x_test_para, y_train, y_test)\n",
    "#     orth_res = run_lr_with_mi_presplit(x_train_orth, x_test_orth, y_train, y_test)\n",
    "\n",
    "#     lang_probe_accs.append({\n",
    "#         'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "#         'para_acc': para_res[0],\n",
    "#         'para_train_acc': para_res[3],\n",
    "#         'para_mi_bits': para_res[1]/np.log(2.0),\n",
    "#         'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "#         'para_mi_pct': para_res[1]/para_res[2],\n",
    "#         'orth_acc': orth_res[0],\n",
    "#         'orth_train_acc': orth_res[3],\n",
    "#         'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "#         'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "#         'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "#     })\n",
    "\n",
    "#     display(pd.DataFrame(lang_probe_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict Language\n",
    "\"\"\"\n",
    "lang_probe_accs = []\n",
    "\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[::]):\n",
    "\n",
    "    source_mapping = {source: i for i, source in enumerate(sample_df['source'].unique())}\n",
    "\n",
    "    y_df =\\\n",
    "        sample_df\\\n",
    "        .assign(source = lambda df: df['source'].map(source_mapping))\\\n",
    "        ['source']\\\n",
    "        .tolist()\n",
    "\n",
    "    y_cp = cupy.asarray(y_df)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    \n",
    "    para_res = run_lr_with_mi(x_cp_para, y_cp)\n",
    "    orth_res = run_lr_with_mi(x_cp_orth, y_cp)\n",
    "\n",
    "    lang_probe_accs.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_res[0],\n",
    "        'para_train_acc': para_res[3],\n",
    "        'para_mi_bits': para_res[1]/np.log(2.0),\n",
    "        'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "        'para_mi_pct': para_res[1]/para_res[2],\n",
    "        'orth_acc': orth_res[0],\n",
    "        'orth_train_acc': orth_res[3],\n",
    "        'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "        'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "        'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(lang_probe_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "display(sample_df.groupby('source', as_index = False).agg(z = ('sample_ix', 'count')))\n",
    "lang_export_df = pd.DataFrame(lang_probe_accs)\n",
    "display(lang_export_df)\n",
    "\n",
    "lang_export_df.to_csv(f'{ws}/analyses/hidden-state-analysis/exports/svd-analysis/svd-probe-lang-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict TID\n",
    "\"\"\"\n",
    "tid_probe_accs = []\n",
    "\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[::2]):\n",
    "\n",
    "    clear_all_cuda_memory(False)\n",
    "\n",
    "    top_tids =\\\n",
    "        sample_df\\\n",
    "        .pipe(lambda df: df[df['source'] == 'en'])\\\n",
    "        .groupby(['token_id', 'token'], as_index = False)\\\n",
    "        .agg(n = ('token', 'count')).sort_values(by = 'n', ascending = False)\\\n",
    "        .head(500)\n",
    "\n",
    "    valid_samples =\\\n",
    "        sample_df\\\n",
    "        .assign(token_id = lambda df: np.where(df['token_id'].isin(top_tids['token_id']), df['token_id'], 999999))\n",
    "        # .pipe(lambda df: df[df['token_id'].isin(top_tids['token_id'].tolist())])\n",
    "\n",
    "    y_df =\\\n",
    "        valid_samples\\\n",
    "        ['token_id']\\\n",
    "        .tolist()\n",
    "\n",
    "    y_cp = cupy.asarray(y_df)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer][valid_samples['sample_ix'].tolist(), :].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer][valid_samples['sample_ix'].tolist(), :].to(torch.float16).detach().cpu())\n",
    "\n",
    "    para_res = run_lr_with_mi(x_cp_para, y_cp)\n",
    "    orth_res = run_lr_with_mi(x_cp_orth, y_cp)\n",
    "\n",
    "    tid_probe_accs.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_res[0],\n",
    "        'para_train_acc': para_res[3],\n",
    "        'para_mi_bits': para_res[1]/np.log(2.0),\n",
    "        'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "        'para_mi_pct': para_res[1]/para_res[2],\n",
    "        'orth_acc': orth_res[0],\n",
    "        'orth_train_acc': orth_res[3],\n",
    "        'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "        'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "        'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "    })\n",
    "\n",
    "pd.DataFrame(tid_probe_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "tid_export_df = pd.DataFrame(tid_probe_accs)\n",
    "display(tid_export_df)\n",
    "\n",
    "tid_export_df.to_csv(f'{ws}/analyses/hidden-state-analysis/exports/svd-analysis/svd-probe-tid-{model_prefix}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
