{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to use SVD to decompose hidden states based on whether they're used by routing or not.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "from utils.svd import decompose_orthogonal, decompose_sideways\n",
    "from utils.vis import combine_plots\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_ix = 2\n",
    "models_list = [\n",
    "    ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 0),\n",
    "    ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 0),\n",
    "    ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 1),\n",
    "    ('Qwen/Qwen3-30B-A3B', 'qwen3moe', 0)\n",
    "]\n",
    "\n",
    "model_id, model_prefix, model_pre_mlp_layers = models_list[model_ix]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix, max_data_files):\n",
    "    \"\"\"\n",
    "    Load data saved by `export-activations-sm.ipynb`\n",
    "    \"\"\"\n",
    "    folders = [f'./../export-data/activations-sm/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    all_pre_mlp_hs = []\n",
    "    sample_df = []\n",
    "    topk_df = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_df.append(pd.read_pickle(f'{f}/samples.pkl'))\n",
    "        topk_df.append(pd.read_pickle(f'{f}/topks.pkl'))\n",
    "        all_pre_mlp_hs.append(torch.load(f'{f}/all-pre-mlp-hidden-states.pt'))\n",
    "\n",
    "    sample_df = pd.concat(sample_df)\n",
    "    topk_df = pd.concat(topk_df)\n",
    "    all_pre_mlp_hs = torch.concat(all_pre_mlp_hs)    \n",
    "\n",
    "    with open(f'./../export-data/activations-sm/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    gc.collect()\n",
    "    return sample_df, topk_df, all_pre_mlp_hs, metadata['all_pre_mlp_hidden_states_layers']\n",
    "\n",
    "# Due to mem constraints, for Qwen3Moe max_data_files = 3\n",
    "sample_df_import, topk_df_import, all_pre_mlp_hs_import, act_map = load_data(model_prefix, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .assign(layer_ix = lambda df: df['layer_ix'] + model_pre_mlp_layers)\n",
    "\n",
    "topk1_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "def get_sample_df_for_layer(sample_df, topk_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Helper to take the sample df and merge layer-level expert selection information\n",
    "    \"\"\"\n",
    "    topk_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix])\n",
    "    topk_l1_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 1])\n",
    "    topk_l2_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 2])\n",
    "\n",
    "    layer_df =\\\n",
    "        sample_df\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 1])[['sample_ix', 'expert']], how = 'inner', on = 'sample_ix')\\\n",
    "        .merge(topk_l1_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_l2_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev2_expert'})[['sample_ix', 'prev2_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 2]).rename(columns = {'expert': 'expert2'})[['sample_ix', 'expert2']], how = 'left', on = 'sample_ix')\\\n",
    "        .assign(leading_path = lambda df: df['prev2_expert'] + '-' + df['prev_expert'])\n",
    "    \n",
    "    return layer_df\n",
    "\n",
    "del sample_df_import, sample_df_raw, topk_df_import\n",
    "\n",
    "gc.collect()\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {(layer_ix + model_pre_mlp_layers): all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's take the pre-MLP hidden states and split them using SVD into parallel and orthogonal components.\n",
    "\"\"\"\n",
    "h_para_by_layer = {}\n",
    "h_orth_by_layer = {}\n",
    "\n",
    "for layer_ix in tqdm(list(all_pre_mlp_hs.keys())):\n",
    "    h_para_by_layer[layer_ix], h_orth_by_layer[layer_ix] = decompose_orthogonal(\n",
    "        all_pre_mlp_hs[layer_ix].to(torch.float32),\n",
    "        model.model.layers[layer_ix].mlp.gate.weight.detach().cpu().to(torch.float32),\n",
    "        'svd'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orth vs Para Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get row-space rotation stability\n",
    "\"\"\"\n",
    "bootstrap_samples = 50\n",
    "\n",
    "def get_sample_res(hs_by_layer, samples_to_test = 1):\n",
    "    \n",
    "    samples = np.random.randint(0, hs_by_layer[1].shape[0], samples_to_test)\n",
    "\n",
    "    # Cast into sample-level list\n",
    "    sample_tensors = torch.stack([layer_hs[samples, :] for _, layer_hs in hs_by_layer.items()], dim = 1).unbind(dim = 0)\n",
    "\n",
    "    sims = []\n",
    "    for s in sample_tensors:\n",
    "        cos_sim = sklearn.metrics.pairwise.cosine_similarity(s)\n",
    "        sims.append(np.diag(cos_sim, 1))\n",
    "\n",
    "    return np.mean(np.stack(sims, axis = 0), axis = 0)\n",
    "\n",
    "para_res = np.stack([get_sample_res(h_para_by_layer) for _ in range(bootstrap_samples)], axis = 0) # bootstrap_samples x layer_diffs\n",
    "\n",
    "para_mean_across_layers = para_res.mean(axis = 0)\n",
    "para_cis_across_layers = 1.96 * np.std(para_res, axis = 0)\n",
    "\n",
    "para_mean_overall = np.mean(para_mean_across_layers)\n",
    "para_mean_ci = 1.96 * np.std(np.mean(para_res, axis = 1)).item()\n",
    "\n",
    "# print(f\"Mean across layer transitions: {para_mean_across_layers}\")\n",
    "print(f\"Mean across layer transitions + samples: {para_mean_overall:.2f} +/- {para_mean_ci:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get null-space rotation stability\n",
    "\"\"\"\n",
    "orth_res = np.stack([get_sample_res(h_orth_by_layer) for _ in range(bootstrap_samples)], axis = 0) # bootstrap_samples x layer_diffs\n",
    "\n",
    "orth_mean_across_layers = orth_res.mean(axis = 0)\n",
    "orth_cis_across_layers = 1.96 * np.std(orth_res, axis = 0)\n",
    "\n",
    "orth_mean_overall = np.mean(orth_mean_across_layers)\n",
    "orth_mean_ci = 1.96 * np.std(np.mean(orth_res, axis = 1)).item()\n",
    "\n",
    "# print(f\"Mean across layer transitions: {orth_mean_across_layers}\")\n",
    "print(f\"Mean across layer transitions + samples: {orth_mean_overall:.2f} +/- {orth_mean_ci:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "export_df = pd.DataFrame({\n",
    "    'layer_ix_1': list(range(model_pre_mlp_layers + 1, len(all_pre_mlp_hs) + model_pre_mlp_layers)), # +1 to 1 index\n",
    "    'para_mean_across_layers': para_mean_across_layers,\n",
    "    'orth_mean_across_layers': orth_mean_across_layers,\n",
    "    'para_cis': para_cis_across_layers,\n",
    "    'orth_cis': orth_cis_across_layers\n",
    "})\n",
    "\n",
    "export_df.to_csv(f'exports/svd-transition-stability-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction/probing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic regression - predict expert ID\n",
    "\"\"\"\n",
    "def run_lr(x_cp, y_cp):\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.2, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 1000, fit_intercept = True)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    return accuracy\n",
    "\n",
    "def run_lr_with_mi(x_cp, y_cp):\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.2, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 1000, fit_intercept = True)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    train_acc = lr_model.score(x_train, y_train)\n",
    "    y_actual_np = cupy.asnumpy(y_test)\n",
    "    y_pred_np = cupy.asnumpy(lr_model.predict(x_test))\n",
    "    mi = sklearn.metrics.mutual_info_score(y_actual_np, y_pred_np) # nats\n",
    "    max_entropy = sklearn.metrics.mutual_info_score(y_actual_np, y_actual_np) # H(y)\n",
    "    return accuracy, mi.item(), max_entropy.item(), train_acc\n",
    "\n",
    "current_layer_accuracy = []\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())):\n",
    "    expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "\n",
    "    expert_ids_cp = cupy.asarray(expert_ids)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "    para_res = run_lr_with_mi(x_cp_para, expert_ids_cp)\n",
    "    orth_res = run_lr_with_mi(x_cp_orth, expert_ids_cp)\n",
    "\n",
    "    current_layer_accuracy.append({\n",
    "        'test_layer_1': test_layer + model_pre_mlp_layers + 1,\n",
    "        'para_acc': para_res[0],\n",
    "        'para_train_acc': para_res[3],\n",
    "        'para_mi_bits': para_res[1]/np.log(2.0), # Convert from nats to bits\n",
    "        'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "        'para_mi_pct': para_res[1]/para_res[2],\n",
    "        'orth_acc': orth_res[0],\n",
    "        'para_train_acc': para_res[3],\n",
    "        'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "        'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "        'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "    })\n",
    "\n",
    "pd.DataFrame(current_layer_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict NEXT layer expert ids (note - this does not remove expert info, remove below)\n",
    "\"\"\"\n",
    "# next_layer_accuracy = []\n",
    "# for test_layer in tqdm(list(h_para_by_layer.keys())[:-1]):\n",
    "#     expert_ids =\\\n",
    "#         topk_df\\\n",
    "#         .pipe(lambda df: df[df['layer_ix'] == test_layer + 1])\\\n",
    "#         .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "#         ['expert'].tolist()\n",
    "\n",
    "#     expert_ids_cp = cupy.asarray(expert_ids)\n",
    "#     x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "#     x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "#     para_res = run_lr_with_mi(x_cp_para, expert_ids_cp)\n",
    "#     orth_res = run_lr_with_mi(x_cp_orth, expert_ids_cp)\n",
    "    \n",
    "#     next_layer_accuracy.append({\n",
    "#         'test_layer_1': test_layer + model_pre_mlp_layers + 1,\n",
    "#         'para_acc': para_res[0],\n",
    "#         'para_mi_bits': para_res[1]/np.log(2.0),\n",
    "#         'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "#         'para_mi_pct': para_res[1]/para_res[2],\n",
    "#         'orth_acc': orth_res[0],\n",
    "#         'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "#         'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "#         'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "#     })\n",
    "\n",
    "# pd.DataFrame(next_layer_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict NEXT layer expert ids. Remove expert centroids first.\n",
    "\"\"\"\n",
    "centroids_para = {}\n",
    "centroids_orth = {}\n",
    "\n",
    "# Get current-layer expert IDs for layer\n",
    "for layer_ix in h_para_by_layer.keys():\n",
    "\n",
    "    cur_layer_expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == layer_ix])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "\n",
    "    cur_layer_expert_ids_cp = cupy.asarray(cur_layer_expert_ids)\n",
    "\n",
    "    # H_para/h_orth for layer\n",
    "    h_para_cp = cupy.asarray(h_para_by_layer[layer_ix].to(torch.float16).detach().cpu())\n",
    "    h_orth_cp = cupy.asarray(h_orth_by_layer[layer_ix].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Compute centroids per expert id\n",
    "    centroids_para[layer_ix] = {}\n",
    "    centroids_orth[layer_ix] = {}\n",
    "\n",
    "    for e in set(cur_layer_expert_ids):\n",
    "        idx_cp = cupy.where(cur_layer_expert_ids_cp == e)[0]\n",
    "        centroids_para[layer_ix][e] = h_para_cp[idx_cp].mean(axis = 0)\n",
    "        centroids_orth[layer_ix][e] = h_orth_cp[idx_cp].mean(axis = 0)\n",
    "\n",
    "next_layer_accuracy_cond = []\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[:-1]):\n",
    "    # Target = next-layer slot-1 expert IDs (same as before)\n",
    "    y_cp =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer + 1])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "    y_cp = cupy.asarray(y_cp)\n",
    "\n",
    "    # Current-layer top-1 expert IDs - needed for residual lookup\n",
    "    cur_exp_cp =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "    cur_exp_cp = cupy.asarray(cur_exp_cp)\n",
    "\n",
    "    # Pull h_para / h_orth tensors and convert to cupy\n",
    "    h_para_cp = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    h_orth_cp = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Subtract extract centroids\n",
    "    mu_para_mat = cupy.stack([centroids_para[test_layer][int(e)] for e in cur_exp_cp])\n",
    "    mu_orth_mat = cupy.stack([centroids_orth[test_layer][int(e)] for e in cur_exp_cp])\n",
    "    h_para_res = h_para_cp - mu_para_mat\n",
    "    h_orth_res = h_orth_cp - mu_orth_mat\n",
    "\n",
    "    # Run the unchanged probe\n",
    "    para_res = run_lr_with_mi(h_para_res, y_cp)\n",
    "    orth_res = run_lr_with_mi(h_orth_res, y_cp)\n",
    "\n",
    "    next_layer_accuracy_cond.append({\n",
    "        'test_layer_1': test_layer + model_pre_mlp_layers + 1,\n",
    "        'para_acc': para_res[0],\n",
    "        'para_mi_bits': para_res[1]/np.log(2.0),\n",
    "        'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "        'para_mi_pct': para_res[1]/para_res[2],\n",
    "        'orth_acc': orth_res[0],\n",
    "        'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "        'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "        'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(next_layer_accuracy_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export results\n",
    "\"\"\"\n",
    "layer_transitions_export_df = pd.concat([\n",
    "    pd.DataFrame(current_layer_accuracy).assign(target = 'current_layer'),\n",
    "    pd.DataFrame(next_layer_accuracy_cond).assign(target = 'next_layer')\n",
    "]).assign(model = model_prefix)\n",
    "\n",
    "display(layer_transitions_export_df)\n",
    "\n",
    "layer_transitions_export_df.to_csv(f'exports/svd-probe-expert-id-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict language - presplit, seperate TIDs\n",
    "\"\"\"\n",
    "# def run_lr_with_mi_presplit(x_train, x_test, y_train, y_test):\n",
    "#     lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 1000, fit_intercept = True)\n",
    "#     lr_model.fit(x_train, y_train)\n",
    "#     accuracy = lr_model.score(x_test, y_test)\n",
    "#     train_acc = lr_model.score(x_train, y_train)\n",
    "#     y_actual_np = cupy.asnumpy(y_test)\n",
    "#     y_pred_np = cupy.asnumpy(lr_model.predict(x_test))\n",
    "#     mi = sklearn.metrics.mutual_info_score(y_actual_np, y_pred_np) # nats\n",
    "#     max_entropy = sklearn.metrics.mutual_info_score(y_actual_np, y_actual_np) # H(y)\n",
    "#     return accuracy, mi.item(), max_entropy.item(), train_acc\n",
    "\n",
    "# lang_probe_accs = []\n",
    "# # Split train/test, different TIDs in each\n",
    "# gss = sklearn.model_selection.GroupShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 123)\n",
    "# train_ix, test_ix = next(gss.split(sample_df, groups = sample_df['token_id']))\n",
    "\n",
    "# train_sample_df = sample_df.take(train_ix)\n",
    "# test_sample_df = sample_df.take(test_ix)\n",
    "\n",
    "# # Prep y values\n",
    "# source_mapping = {source: i for i, source in enumerate(sample_df['source'].unique())}\n",
    "\n",
    "# y_train = cupy.asarray(train_sample_df.assign(source = lambda df: df['source'].map(source_mapping))['source'].tolist())\n",
    "# y_test = cupy.asarray(test_sample_df.assign(source = lambda df: df['source'].map(source_mapping))['source'].tolist())\n",
    "\n",
    "# for test_layer in tqdm(list(h_para_by_layer.keys())[::2]):\n",
    "\n",
    "#     x_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "#     x_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    \n",
    "#     x_train_para = x_para[train_sample_df['sample_ix'].tolist(), :]\n",
    "#     x_test_para = x_para[test_sample_df['sample_ix'].tolist(), :]\n",
    "#     x_train_orth = x_orth[train_sample_df['sample_ix'].tolist(), :]\n",
    "#     x_test_orth = x_orth[test_sample_df['sample_ix'].tolist(), :]\n",
    "\n",
    "#     para_res = run_lr_with_mi_presplit(x_train_para, x_test_para, y_train, y_test)\n",
    "#     orth_res = run_lr_with_mi_presplit(x_train_orth, x_test_orth, y_train, y_test)\n",
    "\n",
    "#     lang_probe_accs.append({\n",
    "#         'test_layer_1': test_layer + model_pre_mlp_layers + 1,\n",
    "#         'para_acc': para_res[0],\n",
    "#         'para_train_acc': para_res[3],\n",
    "#         'para_mi_bits': para_res[1]/np.log(2.0),\n",
    "#         'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "#         'para_mi_pct': para_res[1]/para_res[2],\n",
    "#         'orth_acc': orth_res[0],\n",
    "#         'orth_train_acc': orth_res[3],\n",
    "#         'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "#         'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "#         'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "#     })\n",
    "\n",
    "#     display(pd.DataFrame(lang_probe_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict Language\n",
    "\"\"\"\n",
    "lang_probe_accs = []\n",
    "\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[::]):\n",
    "\n",
    "    source_mapping = {source: i for i, source in enumerate(sample_df['source'].unique())}\n",
    "\n",
    "    y_df =\\\n",
    "        sample_df\\\n",
    "        .assign(source = lambda df: df['source'].map(source_mapping))\\\n",
    "        ['source']\\\n",
    "        .tolist()\n",
    "\n",
    "    y_cp = cupy.asarray(y_df)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    \n",
    "    para_res = run_lr_with_mi(x_cp_para, y_cp)\n",
    "    orth_res = run_lr_with_mi(x_cp_orth, y_cp)\n",
    "\n",
    "    lang_probe_accs.append({\n",
    "        'test_layer_1': test_layer + model_pre_mlp_layers + 1,\n",
    "        'para_acc': para_res[0],\n",
    "        'para_train_acc': para_res[3],\n",
    "        'para_mi_bits': para_res[1]/np.log(2.0),\n",
    "        'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "        'para_mi_pct': para_res[1]/para_res[2],\n",
    "        'orth_acc': orth_res[0],\n",
    "        'orth_train_acc': orth_res[3],\n",
    "        'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "        'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "        'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(lang_probe_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "display(sample_df.groupby('source', as_index = False).agg(z = ('sample_ix', 'count')))\n",
    "lang_export_df = pd.DataFrame(lang_probe_accs)\n",
    "display(lang_export_df)\n",
    "\n",
    "lang_export_df.to_csv(f'exports/svd-probe-lang-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict TID\n",
    "\"\"\"\n",
    "tid_probe_accs = []\n",
    "\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[::2]):\n",
    "\n",
    "    clear_all_cuda_memory(False)\n",
    "\n",
    "    top_tids =\\\n",
    "        sample_df\\\n",
    "        .pipe(lambda df: df[df['source'] == 'en'])\\\n",
    "        .groupby(['token_id', 'token'], as_index = False)\\\n",
    "        .agg(n = ('token', 'count')).sort_values(by = 'n', ascending = False)\\\n",
    "        .head(500)\n",
    "\n",
    "    valid_samples =\\\n",
    "        sample_df\\\n",
    "        .assign(token_id = lambda df: np.where(df['token_id'].isin(top_tids['token_id']), df['token_id'], 999999))\n",
    "        # .pipe(lambda df: df[df['token_id'].isin(top_tids['token_id'].tolist())])\n",
    "\n",
    "    y_df =\\\n",
    "        valid_samples\\\n",
    "        ['token_id']\\\n",
    "        .tolist()\n",
    "\n",
    "    y_cp = cupy.asarray(y_df)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer][valid_samples['sample_ix'].tolist(), :].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer][valid_samples['sample_ix'].tolist(), :].to(torch.float16).detach().cpu())\n",
    "\n",
    "    para_res = run_lr_with_mi(x_cp_para, y_cp)\n",
    "    orth_res = run_lr_with_mi(x_cp_orth, y_cp)\n",
    "\n",
    "    tid_probe_accs.append({\n",
    "        'test_layer_1': test_layer + model_pre_mlp_layers + 1,\n",
    "        'para_acc': para_res[0],\n",
    "        'para_train_acc': para_res[3],\n",
    "        'para_mi_bits': para_res[1]/np.log(2.0),\n",
    "        'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "        'para_mi_pct': para_res[1]/para_res[2],\n",
    "        'orth_acc': orth_res[0],\n",
    "        'orth_train_acc': orth_res[3],\n",
    "        'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "        'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "        'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "    })\n",
    "\n",
    "pd.DataFrame(tid_probe_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "tid_export_df = pd.DataFrame(tid_probe_accs)\n",
    "display(tid_export_df)\n",
    "\n",
    "tid_export_df.to_csv(f'exports/svd-probe-tid-{model_prefix}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
