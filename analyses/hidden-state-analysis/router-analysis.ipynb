{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to understand how routing correlates to the hidden state. Requires that `export-data/export-activations.ipynb` has been run.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "from utils.vis import combine_plots\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_ix = 2\n",
    "models_list = [\n",
    "    ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 0),\n",
    "    ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 0),\n",
    "    ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 1),\n",
    "    ('Qwen/Qwen3-30B-A3B', 'qwen3moe', 0)\n",
    "]\n",
    "\n",
    "def load_model_and_tokenizer(model_id, model_prefix):\n",
    "    local_path = os.path.join('/workspace/models', model_prefix)\n",
    "    model_path = local_path if os.path.exists(local_path) else model_id    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()    \n",
    "    if model_path == model_id:\n",
    "        os.makedirs(local_path, exist_ok = True)\n",
    "        tokenizer.save_pretrained(local_path)\n",
    "        model.save_pretrained(local_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "model_id, model_prefix, model_pre_mlp_layers = models_list[model_ix]\n",
    "tokenizer, model = load_model_and_tokenizer(model_id, model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix, max_data_files):\n",
    "    \"\"\"\n",
    "    Load data saved by `export-activations-sm.ipynb`\n",
    "    \"\"\"\n",
    "    folders = [f'./../export-data/activations-sm/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    all_pre_mlp_hs = []\n",
    "    sample_df = []\n",
    "    topk_df = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_df.append(pd.read_pickle(f'{f}/samples.pkl'))\n",
    "        topk_df.append(pd.read_pickle(f'{f}/topks.pkl'))\n",
    "        all_pre_mlp_hs.append(torch.load(f'{f}/all-pre-mlp-hidden-states.pt'))\n",
    "\n",
    "    sample_df = pd.concat(sample_df)\n",
    "    topk_df = pd.concat(topk_df)\n",
    "    all_pre_mlp_hs = torch.concat(all_pre_mlp_hs)    \n",
    "\n",
    "    with open(f'./../export-data/activations-sm/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    gc.collect()\n",
    "    return sample_df, topk_df, all_pre_mlp_hs, metadata['all_pre_mlp_hidden_states_layers']\n",
    "\n",
    "sample_df_import, topk_df_import, all_pre_mlp_hs_import, act_map = load_data(model_prefix, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .assign(layer_ix = lambda df: df['layer_ix'] + model_pre_mlp_layers)\n",
    "\n",
    "topk1_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "def get_sample_df_for_layer(sample_df, topk_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Helper to take the sample df and merge layer-level expert selection information\n",
    "    \"\"\"\n",
    "    topk_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix])\n",
    "    topk_l1_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 1])\n",
    "    topk_l2_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 2])\n",
    "\n",
    "    layer_df =\\\n",
    "        sample_df\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 1])[['sample_ix', 'expert']], how = 'inner', on = 'sample_ix')\\\n",
    "        .merge(topk_l1_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_l2_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev2_expert'})[['sample_ix', 'prev2_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 2]).rename(columns = {'expert': 'expert2'})[['sample_ix', 'expert2']], how = 'left', on = 'sample_ix')\\\n",
    "        .assign(leading_path = lambda df: df['prev2_expert'] + '-' + df['prev_expert'])\n",
    "    \n",
    "    return layer_df\n",
    "\n",
    "del sample_df_import, sample_df_raw, topk_df_import\n",
    "\n",
    "gc.collect()\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {(layer_ix + model_pre_mlp_layers): all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze routing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Norms by expert and layer\n",
    "\"\"\"\n",
    "norms_by_expert_layer = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        'layer_ix': layer_ix,\n",
    "        'norm': torch.linalg.norm(model.model.layers[layer_ix].mlp.gate.weight, dim = 1, ord = 1).to(torch.float16).cpu().detach().numpy(),\n",
    "        'expert': list(range(1, model.model.layers[layer_ix].mlp.gate.weight.shape[0] + 1))\n",
    "    })\n",
    "    for layer_ix in list(all_pre_mlp_hs.keys())\n",
    "])\n",
    "\n",
    "plot_df = norms_by_expert_layer.pivot(index = 'layer_ix', columns = 'expert', values = 'norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Norm by Expert and Layer\"\n",
    ").update_layout(autosize = False, width = 800).show()\n",
    "\n",
    "scaled_df =\\\n",
    "    norms_by_expert_layer\\\n",
    "    .assign(layer_mean = lambda df: df.groupby('layer_ix')['norm'].transform('mean'))\\\n",
    "    .assign(norm_scaled = lambda df: df['norm'] / df['layer_mean'] - 1)\n",
    "\n",
    "scaled_plot_df = scaled_df.pivot(index = 'layer_ix', columns = 'expert', values = 'norm_scaled')\n",
    "px.imshow(\n",
    "    scaled_plot_df,\n",
    "    x = scaled_plot_df.columns, y = scaled_plot_df.index,\n",
    "    color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Norm by Expert and Layer\"\n",
    ").update_layout(autosize = False, width = 800).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For a single layer, what do the weights and RMSnorms look like?\n",
    "\"\"\"\n",
    "plot_layer_ix = 9\n",
    "show_dims = list(range(0, 400))\n",
    "\n",
    "# RMSNorm\n",
    "rms_tensor = model.model.layers[plot_layer_ix].post_attention_layernorm.weight\n",
    "rms_df = pd.DataFrame({\n",
    "    'gamma': rms_tensor.to(torch.float16).cpu().detach().numpy(),\n",
    "    'coef': 1,\n",
    "    'dimension': list(range(0, rms_tensor.shape[0]))\n",
    "})\n",
    "plot_df = rms_df.pipe(lambda df: df[df['dimension'].isin(show_dims)]).pivot(index = 'coef', columns = 'dimension', values = 'gamma')\n",
    "\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"RMSNorm Scaling Values\"\n",
    ").update_layout(autosize = False, width = 1400, height = 400).show()\n",
    "\n",
    "# Weights\n",
    "wt_tensor = model.model.layers[plot_layer_ix].mlp.gate.weight\n",
    "wt_df = pd.DataFrame({\n",
    "    'value': wt_tensor.view(-1).to(torch.float16).cpu().detach().numpy(),\n",
    "    'expert': [i // wt_tensor.shape[1] for i in range(wt_tensor.view(-1).shape[0])],\n",
    "    'dimension': [i % wt_tensor.shape[1] for i in range(wt_tensor.view(-1).shape[0])]\n",
    "})\n",
    "\n",
    "plot_df = wt_df.pipe(lambda df: df[df['dimension'].isin(show_dims)]).pivot(index = 'expert', columns = 'dimension', values = 'value')\n",
    "\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Routing Weights\"\n",
    ").update_layout(autosize = False, width = 1400).show()\n",
    "\n",
    "# Scale weights by RMSNorm\n",
    "scaled_df = wt_df.merge(rms_df, on = 'dimension', how = 'inner').assign(gamma_scaled_value = lambda df: df['gamma'] * df['value'])\n",
    "plot_df = scaled_df.pipe(lambda df: df[df['dimension'].isin(show_dims)]).pivot(index = 'expert', columns = 'dimension', values = 'gamma_scaled_value')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Scaled Routing Weights\"\n",
    ").update_layout(autosize = False, width = 1400).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mean norms across layers and dimension (averaged across experts)\n",
    "\"\"\"\n",
    "dfs_list = []\n",
    "for layer_ix in list(all_pre_mlp_hs.keys()):\n",
    "    wt_tensor = model.model.layers[layer_ix].mlp.gate.weight.to(torch.float16).cpu().detach()\n",
    "    rms_tensor = model.model.layers[layer_ix].post_attention_layernorm.weight.to(torch.float16).cpu().detach()\n",
    "    scaled = (wt_tensor * rms_tensor) # Multiply by RMS norm\n",
    "    scaled = scaled.abs().mean(dim = 0) # Take mean L1 norm\n",
    "    dfs_list.append(pd.DataFrame({\n",
    "        'mean_norm': scaled.numpy(),\n",
    "        'layer_ix': layer_ix,\n",
    "        'dim': list(range(1, scaled.shape[0] + 1))\n",
    "    }))\n",
    "\n",
    "my_df = pd.concat(dfs_list)\n",
    "# Additionally scale by layer average\n",
    "my_df_ex_scale =\\\n",
    "    my_df\\\n",
    "    .assign(layer_mean = lambda df: df.groupby('layer_ix')['mean_norm'].transform('mean'))\\\n",
    "    .assign(mean_norm = lambda df: df['mean_norm'] / df['layer_mean'])\n",
    "\n",
    "plot_df = my_df_ex_scale.pipe(lambda df: df[df['dim']  <= 200]).pivot(index = 'layer_ix', columns = 'dim', values = 'mean_norm')\n",
    "\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    zmin = 0, zmax = 8,\n",
    "    aspect = 'auto', # Allow non-square boxes\n",
    "    color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "At dimension x layer-level, analyze activations (averaged across samples) versus routing weights (averaged across experts).\n",
    "\"\"\"\n",
    "show_dims = list(range(0, 800))\n",
    "\n",
    "dfs_list = []\n",
    "for layer_ix, pre_mlp_for_layer in tqdm(all_pre_mlp_hs.items()):\n",
    "    wt_tensor = model.model.layers[layer_ix].mlp.gate.weight[:, :].to(torch.float16).cuda().detach() # (n_experts, D)\n",
    "    act_tensor = all_pre_mlp_hs[layer_ix].abs().mean(dim = 0).cuda().detach() # n_samples x D => D via L1 norm\n",
    "    scaled = (wt_tensor * act_tensor) # Multiply by activation tensor\n",
    "    scaled = scaled.abs().mean(dim = 0) # Take mean L1 norm\n",
    "    dfs_list.append(pd.DataFrame({\n",
    "        'layer_ix': layer_ix,\n",
    "        'act_norm': act_tensor.cpu().numpy(), # D,\n",
    "        'wt_norm': wt_tensor.abs().mean(dim = 0).cpu().numpy(), # n_experts x D => D,\n",
    "        'mean_scaled_norm': scaled.cpu().numpy(),\n",
    "        'dim': list(range(1, scaled.shape[0] + 1)) # show_dims\n",
    "    }))\n",
    "\n",
    "pre_mlp_df = pd.concat(dfs_list)\n",
    "del dfs_list\n",
    "\n",
    "plot_df = pre_mlp_df.pipe(lambda df: df[df['dim'].isin(show_dims)]).pivot(index = 'layer_ix', columns = 'dim', values = 'mean_scaled_norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    zmin = 0,\n",
    "    zmax = .2,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean scaled wt * activation norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "plot_df = pre_mlp_df.pipe(lambda df: df[df['dim'].isin(show_dims)]).pivot(index = 'layer_ix', columns = 'dim', values = 'act_norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    zmin = 0, zmax = 8,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean activation norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "plot_df = pre_mlp_df.pipe(lambda df: df[df['dim'].isin(show_dims)]).pivot(index = 'layer_ix', columns = 'dim', values = 'wt_norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean weight norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "# scipy.stats.kurtosis(pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == 6]['act_norm'].tolist() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build and export the correlation plot\n",
    "\"\"\"\n",
    "font_size = 22\n",
    "\n",
    "def get_layer_acts_and_wts(pre_mlp_df, layer_ix):\n",
    "    layer_df =\\\n",
    "        pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == layer_ix])\\\n",
    "        .pipe(lambda df: df[(df['wt_norm'] > 0) & (df['act_norm'] > 0)])\\\n",
    "        .assign(log_wt = lambda df: np.log10(df['wt_norm']), log_act = lambda df: np.log10(df['act_norm']))\n",
    "\n",
    "    return layer_df\n",
    "\n",
    "def get_ols(layer_df):\n",
    "    calc_df = layer_df\\\n",
    "        .pipe(lambda df: df[\n",
    "            (df['act_norm'] > np.quantile(df['act_norm'], .01)) & \n",
    "            (df['wt_norm'] > np.quantile(df['wt_norm'], .01)) & \n",
    "            (df['act_norm'] < np.quantile(df['act_norm'], .99)) & \n",
    "            (df['wt_norm'] < np.quantile(df['wt_norm'], .99))\n",
    "        ])\n",
    "    b1, b0, r, _, _ = scipy.stats.linregress(calc_df['log_act'], calc_df['log_wt'])\n",
    "    return {'b1': b1.item(), 'b0': b0.item(), 'r': r.item()}\n",
    "\n",
    "layer_acts_by_layer = {layer_ix: get_layer_acts_and_wts(pre_mlp_df, layer_ix) for layer_ix in all_pre_mlp_hs.keys()}\n",
    "ols_acts_by_layer = {layer_ix: get_ols(layer_act) for layer_ix, layer_act in layer_acts_by_layer.items()}\n",
    "\n",
    "layers_to_plot = [\n",
    "    int(np.floor(np.quantile(list(all_pre_mlp_hs.keys()), 0.25)).item()),\n",
    "    int(np.floor(np.quantile(list(all_pre_mlp_hs.keys()), 0.50)).item()),\n",
    "    int(np.floor(np.quantile(list(all_pre_mlp_hs.keys()), 0.75)).item())\n",
    "]\n",
    "\n",
    "color_map = {\n",
    "    str(layer_ix): px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)]\n",
    "    for i, layer_ix in enumerate(sorted(layers_to_plot))\n",
    "}\n",
    "\n",
    "plot_df = pd.concat([layer_acts_by_layer[layer_ix] for layer_ix in layers_to_plot]).assign(layer_ix = lambda df: df['layer_ix'].astype(str))\n",
    "\n",
    "fig = px.scatter(\n",
    "    plot_df\\\n",
    "        .pipe(lambda df: df[(df['act_norm'] > np.quantile(df['act_norm'], .0005))  & (df['wt_norm'] > np.quantile(df['wt_norm'], .0005))])\\\n",
    "        .pipe(lambda df: df[(df['act_norm'] < np.quantile(df['act_norm'], .9995))  & (df['wt_norm'] < np.quantile(df['wt_norm'], .9995))]),\n",
    "    x = 'act_norm', y = 'wt_norm',\n",
    "    color = 'layer_ix',\n",
    "    log_x = True, log_y = True,\n",
    "    color_discrete_map = color_map,\n",
    "    template = 'plotly_white',\n",
    "    title = f'Per-dimension activation vs. router weight (log - log)',\n",
    "    )\\\n",
    "    .update_traces(marker = dict(size = 5, opacity = 0.4, line = dict(width = 0)))\n",
    "\n",
    "# Add regression line\n",
    "for i, layer_ix in enumerate(layers_to_plot):\n",
    "    layer_plot_df =\\\n",
    "        plot_df[plot_df['layer_ix'] == str(layer_ix)]\\\n",
    "        .pipe(lambda df: df[\n",
    "            (df['act_norm'] > np.quantile(df['act_norm'], .01)) & \n",
    "            (df['wt_norm'] > np.quantile(df['wt_norm'], .01)) & \n",
    "            (df['act_norm'] < np.quantile(df['act_norm'], .99)) & \n",
    "            (df['wt_norm'] < np.quantile(df['wt_norm'], .99))\n",
    "        ])\n",
    "\n",
    "    x_fit_log_act = np.array([layer_plot_df['log_act'].min() - 0.1, layer_plot_df['log_act'].max() + 0.1])\n",
    "    y_fit_log_wt = ols_acts_by_layer[layer_ix]['b0'] + ols_acts_by_layer[layer_ix]['b1']  * x_fit_log_act\n",
    "\n",
    "    fig.add_scatter(\n",
    "        x = 10 ** x_fit_log_act,\n",
    "        y = 10 ** y_fit_log_wt,\n",
    "        mode = 'lines',\n",
    "        line = dict(width = 2, color = color_map[str(layer_ix)]),\n",
    "        name = f'OLS Layer {layer_ix}',\n",
    "        showlegend = False\n",
    "    )\n",
    "\n",
    "    x_anchor = np.quantile(layer_plot_df['act_norm'], .99) + 0.05\n",
    "    y_anchor = 10 ** (ols_acts_by_layer[layer_ix]['b0'] + ols_acts_by_layer[layer_ix]['b1'] * np.log10(x_anchor)) + ((0.0 if i == 0 else -0.03) if i != 1 else 0.13) # Or .13 => 0.03\n",
    "    fig.add_annotation(\n",
    "        x = np.log10(x_anchor),\n",
    "        y = np.log10(y_anchor),\n",
    "        text = f\"<b>layer {str(layer_ix + 1)} (<i>&#961;={ols_acts_by_layer[layer_ix]['r']:.2f}</i>)</b>\",\n",
    "        xanchor = 'center', yanchor = 'bottom',\n",
    "        showarrow = False,\n",
    "        font = dict(family = 'CMU Serif', size = font_size - 1, color = color_map[str(layer_ix)]),\n",
    "        bgcolor='rgba(255, 255, 255, 0.9)', # White with 75% opacity\n",
    "        borderpad=2 # Small padding around text\n",
    "    )\n",
    "\n",
    "\n",
    "# Clean layout\n",
    "fig.update_layout(\n",
    "    width = 800, height = 500,\n",
    "    margin = dict(l = 10, r = 10, t = 40, b = 10),\n",
    "    coloraxis_colorbar = dict(title = 'Residual'),\n",
    "    font_family = 'CMU Serif',\n",
    "    showlegend = False,\n",
    "    font_size = font_size,\n",
    "    coloraxis_showscale = False,\n",
    "    xaxis_title = 'Router weight magnitude',\n",
    "    yaxis_title = 'Hidden state magnitude',\n",
    "    # xaxis = dict(tickmode = 'auto', nticks = 5, showexponent = 'all', exponentformat = 'power'),\n",
    "    # yaxis = dict(tickmode = 'auto', nticks = 5, showexponent = 'all', exponentformat = 'power'),\n",
    "    title = None,\n",
    "    xaxis=dict(type = \"log\", dtick=1),     # keep decades only\n",
    "    yaxis=dict(type = \"log\", dtick=1)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# pio.write_image(fig, f\"exports/router-corr-{model_prefix}-md.pdf\", width = 800, height = 500)\n",
    "# pio.write_image(fig, f\"exports/router-corr-{model_prefix}-md.svg\", width = 800, height = 500)\n",
    "# pio.write_image(\n",
    "#     fig\\\n",
    "#     .update_layout(\n",
    "#         xaxis_title = 'Router weight magnitude' if model_ix in [2, 3] else '',\n",
    "#         yaxis_title = 'Hidden state magnitude' if model_ix in [0, 2] else ''\n",
    "#     ),\n",
    "#     f\"exports/router-corr-{model_prefix}.pdf\", width = 800, height = 450\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get all layer corrs + export\n",
    "\"\"\"\n",
    "# layer_corrs =\\\n",
    "#     pd.DataFrame([{'layer_ix': layer_ix + 1, 'rho': ols_res['r']} for layer_ix, ols_res in ols_acts_by_layer.items()])\\\n",
    "#     .assign(model = model_prefix)\\\n",
    "#     .assign(phase = lambda df: pd.cut(\n",
    "#             df['layer_ix'] - 1,\n",
    "#             bins = [0, df['layer_ix'].max() / 3, 2 * df['layer_ix'].max() / 3, df['layer_ix'].max()],\n",
    "#             labels = ['early', 'mid', 'late'],\n",
    "#             include_lowest = True,\n",
    "#             right = False\n",
    "#         )\n",
    "#     )\\\n",
    "#     .groupby(['model', 'phase'], sort = False, as_index = False)\\\n",
    "#     .agg(rho_mean = ('rho', 'mean'))\n",
    "\n",
    "layer_corrs =\\\n",
    "    pd.DataFrame([{'layer_ix': layer_ix + 1, 'rho': ols_res['r']} for layer_ix, ols_res in ols_acts_by_layer.items()])\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] % 4 == 1])\\\n",
    "    .assign(model = model_prefix)\n",
    "\n",
    "display(layer_corrs)\n",
    "# layer_corrs.to_csv(f'exports/router-corrs-{model_prefix}.csv', mode = 'w', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linear regression - test ability to reconstruct topk expert id + export\n",
    "\"\"\"\n",
    "def lr_for_layer(layer_to_test):\n",
    "    expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == layer_to_test])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "\n",
    "    expert_ids_cp = cupy.asarray(expert_ids)\n",
    "\n",
    "    lr_model = cuml.linear_model.LogisticRegression(\n",
    "        penalty = 'l2', \n",
    "        max_iter = 10000,\n",
    "        fit_intercept = False\n",
    "    )\n",
    "\n",
    "    dims = [\n",
    "        x - 1\n",
    "        for x in pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == layer_to_test]).sort_values(by = 'mean_scaled_norm', ascending = False)['dim'].tolist()\n",
    "    ]\n",
    "\n",
    "    layer_hs = cupy.asarray(all_pre_mlp_hs[layer_to_test][:, dims[0:all_pre_mlp_hs[layer_to_test].shape[1]//50]].to(torch.float16).detach().cpu())\n",
    "    lr_model.fit(layer_hs, expert_ids_cp)\n",
    "    accuracy = lr_model.score(layer_hs, expert_ids_cp)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    baseline_accs = []\n",
    "    for _ in range(10):\n",
    "        rand_dims = [int(x - 1) for x in np.random.choice(pre_mlp_df['dim'].tolist(), size = all_pre_mlp_hs[layer_to_test].shape[1] // 50, replace = False)]\n",
    "        rand_hs = cupy.asarray(all_pre_mlp_hs[layer_to_test][:, rand_dims].to(torch.float16).detach().cpu())\n",
    "        lr_model.fit(rand_hs, expert_ids_cp)\n",
    "        test_baseline_acc = lr_model.score(layer_hs, expert_ids_cp)\n",
    "        baseline_accs.append(test_baseline_acc)\n",
    "    baseline_acc = np.mean(baseline_accs)\n",
    "\n",
    "    print(f\"Baseline accuracy: {baseline_acc:.2%}\")\n",
    "\n",
    "    return {\n",
    "        'layer_ix_1': layer_to_test + 1,\n",
    "        'accuracy': np.round(accuracy, 4).item(),\n",
    "        'baseline_accuracy': baseline_acc.round(4).item()\n",
    "    }\n",
    "\n",
    "layer_res =\\\n",
    "    pd.DataFrame([lr_for_layer(layer_ix) for layer_ix in all_pre_mlp_hs if layer_ix % 2 == 0])\\\n",
    "    .assign(model = model_prefix)\\\n",
    "    .pipe(lambda df: df[df['layer_ix_1'] % 4 == 1])\n",
    "\n",
    "display(layer_res)\n",
    "# layer_res.to_csv(f'exports/router-probe-{model_prefix}.csv', mode = 'w', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Saturation curve - mid layer - export data\n",
    "\"\"\"\n",
    "layer_to_test = int(np.floor(np.quantile(list(all_pre_mlp_hs.keys()), 0.50)).item())\n",
    "\n",
    "def lr_for_layer_at_percent(layer_to_test, percent):\n",
    "    expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == layer_to_test])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "\n",
    "    expert_ids_cp = cupy.asarray(expert_ids)\n",
    "\n",
    "    lr_model = cuml.linear_model.LogisticRegression(\n",
    "        penalty = 'l2', \n",
    "        max_iter = 10000,\n",
    "        fit_intercept = False\n",
    "    )\n",
    "\n",
    "    dims = [\n",
    "        x - 1\n",
    "        for x in pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == layer_to_test]).sort_values(by = 'mean_scaled_norm', ascending = False)['dim'].tolist()\n",
    "    ]\n",
    "\n",
    "    # Calculate how many dimensions to use\n",
    "    n_dims = int(all_pre_mlp_hs[layer_to_test].shape[1] * percent / 100)\n",
    "    n_dims = max(1, n_dims)  # Ensure at least 1 dimension\n",
    "    \n",
    "    layer_hs = cupy.asarray(all_pre_mlp_hs[layer_to_test][:, dims[0:n_dims]].to(torch.float16).detach().cpu())\n",
    "    lr_model.fit(layer_hs, expert_ids_cp)\n",
    "    accuracy = lr_model.score(layer_hs, expert_ids_cp)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    return {\n",
    "        'layer_ix_1': layer_to_test + 1,\n",
    "        'pct_dims': percent,\n",
    "        'accuracy': np.round(accuracy, 4).item(),\n",
    "    }\n",
    "\n",
    "# Test different percentages\n",
    "percentages = list(range(0, 102, 2))\n",
    "\n",
    "saturation_data = [lr_for_layer_at_percent(layer_to_test, pct) for pct in tqdm(percentages)]\n",
    "saturation_df = pd.DataFrame(saturation_data).assign(model = model_prefix, layer_ix_1 = layer_to_test + 1)\n",
    "\n",
    "display(saturation_df)\n",
    "saturation_df.to_csv(f'exports/router-saturation-{model_prefix}.csv', mode = 'w', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compare PCA top dimensions versus scaled activation top dimensions\n",
    "\"\"\"\n",
    "layer_to_test = list(all_pre_mlp_hs.keys())[5]\n",
    "\n",
    "layer_hs = cupy.asarray(all_pre_mlp_hs[layer_to_test][0:200_000, :].to(torch.float16).detach().cpu())\n",
    "mean_vals = cupy.mean(layer_hs, axis=0)\n",
    "std_vals = cupy.std(layer_hs, axis=0)\n",
    "std_vals = cupy.where(std_vals == 0, cupy.asarray(1e-7), std_vals)\n",
    "layer_hs_std = (layer_hs - mean_vals)/std_vals\n",
    "\n",
    "pca = cuml.decomposition.PCA(n_components = 10, random_state = 123)\n",
    "pca.fit(layer_hs_std)\n",
    "\n",
    "pc_loadings = pca.components_\n",
    "sumsq = (pc_loadings ** 2).sum(axis=0)\n",
    "\n",
    "ranking = cupy.argsort(-sumsq)  # descending order\n",
    "pca_top_dims = ranking.tolist()\n",
    "\n",
    "plot_df =\\\n",
    "    pd.DataFrame({'pca_sumsq': cupy.asarray(sumsq).tolist(), 'dim': list(range(1, len(sumsq) + 1))})\\\n",
    "    .merge(\n",
    "        pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == layer_to_test])[['dim', 'mean_scaled_norm']],\n",
    "        on = 'dim',\n",
    "        how = 'inner'\n",
    "    )\n",
    "\n",
    "px.scatter(\n",
    "    plot_df,\n",
    "    x = 'mean_scaled_norm',\n",
    "    y = 'pca_sumsq'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What % of hidden states is explained by PCA?\n",
    "\"\"\"\n",
    "# 1) Gather some data\n",
    "clear_all_cuda_memory()\n",
    "layer_hs = cupy.asarray(all_pre_mlp_hs[5][0:200_000, :].to(torch.float16).detach().cpu())\n",
    "\n",
    "# 2) Fit PCA\n",
    "pca_model = cuml.PCA(iterated_power = 20, n_components = 10, verbose = True)\n",
    "pca_model.fit(layer_hs)\n",
    "\n",
    "print(\"Explained variance ratio:\", pca_model.explained_variance_ratio_)\n",
    "print(\"Cumulative ratio:\", np.cumsum(pca_model.explained_variance_ratio_.get())[-1])\n",
    "\n",
    "# 3) Retrieve components & variance ratio\n",
    "components = pca_model.components_.get()  # shape = (10, D)\n",
    "expl_ratios = pca_model.explained_variance_ratio_.get()  # shape = (10,)\n",
    "\n",
    "# 4) Compute dimension-level importance\n",
    "sq_loadings = components**2        # shape (10, D)\n",
    "dim_importance = sq_loadings.T @ expl_ratios   # shape (D,)\n",
    "\n",
    "# 5) Identify top 20 dims\n",
    "top_k = 10\n",
    "idx_sorted = np.argsort(dim_importance)[::-1]\n",
    "top_dims = idx_sorted[:top_k]\n",
    "sum_top = dim_importance[top_dims].sum()\n",
    "sum_all = dim_importance.sum()\n",
    "frac_top = sum_top / sum_all\n",
    "\n",
    "print(f\"Top {top_k} dims by PCA-based importance: {top_dims}\")\n",
    "print(f\"Sum of their importances: {sum_top:.4f}\")\n",
    "print(f\"Fraction of total importance: {frac_top:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_layers = np.array(sorted(list(set(topk_df['layer_ix']))))\n",
    "unique_experts = np.array(sorted(list(set(topk_df['expert'])))) \n",
    "\n",
    "topk_grouped_0 =\\\n",
    "    topk_df.groupby(['layer_ix', 'expert'], as_index = False)\\\n",
    "    .agg(\n",
    "        token_count = ('sample_ix', 'nunique'), # count distinct tokens\n",
    "        weight_sum = ('weight', 'sum') # sum of gating weights\n",
    "    )\n",
    "\n",
    "pd.merge(\n",
    "    pd.DataFrame({'layer_ix': unique_layers}),\n",
    "    pd.DataFrame({'expert': unique_experts}),\n",
    "    how = 'cross'\n",
    ")\\\n",
    ".merge(topk_grouped_0, how = 'left', on = ['layer_ix', 'expert'])\\\n",
    ".assign(\n",
    "    token_count = lambda df: df['token_count'].fillna(0),\n",
    "    weight_sum = lambda df: df['weight_sum'].fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate load balancing metrics\n",
    "\"\"\"\n",
    "topk_grouped_0 =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    .groupby(['layer_ix', 'expert'], as_index = False)\\\n",
    "    .agg(\n",
    "        token_count = ('sample_ix', 'nunique'), # count distinct tokens\n",
    "        weight_sum = ('weight', 'sum') # sum of gating weights\n",
    "    )\n",
    "\n",
    "unique_layers = np.array(sorted(list(set(topk_df['layer_ix']))))\n",
    "unique_experts = np.array(sorted(list(set(topk_df['expert'])))) \n",
    "\n",
    "# Fill in missing expert/layers\n",
    "topk_grouped =\\\n",
    "    pd.merge(\n",
    "        pd.DataFrame({'layer_ix': unique_layers}),\n",
    "        pd.DataFrame({'expert': unique_experts}),\n",
    "        how = 'cross'\n",
    "    )\\\n",
    "    .merge(topk_grouped_0, how = 'left', on = ['layer_ix', 'expert'])\\\n",
    "    .assign(\n",
    "        token_count = lambda df: df['token_count'].fillna(0),\n",
    "        weight_sum = lambda df: df['weight_sum'].fillna(0)\n",
    "    )\\\n",
    "    .assign(\n",
    "        layer_token_sums = lambda df: df.groupby('layer_ix')['token_count'].transform('sum'), # fraction of tokens that pick (layer, expert)\n",
    "        layer_weight_sums = lambda df: df.groupby('layer_ix')['weight_sum'].transform('sum'),\n",
    "        token_frac = lambda df: df['token_count'] / df['layer_token_sums'],\n",
    "        weight_frac = lambda df: df['weight_sum'] / df['layer_weight_sums']\n",
    "    )\n",
    "\n",
    "def shannon_entropy(probs):\n",
    "    # Avoid log(0)\n",
    "    probs = probs[probs > 0]\n",
    "    return -np.sum(probs * np.log2(probs))\n",
    "\n",
    "entropies = []\n",
    "for layer, layer_df in topk_grouped.groupby('layer_ix'):\n",
    "    token_entropy = shannon_entropy(layer_df['token_frac'].values)\n",
    "    weight_entropy = shannon_entropy(layer_df['weight_frac'].values)\n",
    "    entropies.append({\n",
    "        'layer_ix': layer,\n",
    "        'token_entropy': token_entropy,\n",
    "        'weight_entropy': weight_entropy\n",
    "    })\n",
    "entropy_df = pd.DataFrame(entropies)\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    mask = (p > 0) & (q > 0)\n",
    "    return np.sum(p[mask] * np.log2(p[mask]/q[mask]))\n",
    "\n",
    "kl_list = []\n",
    "for layer, layer_df in topk_grouped.groupby('layer_ix'):\n",
    "    p_token = layer_df['token_frac'].values    \n",
    "    q = np.full_like(p_token, 1/len(p_token))\n",
    "    \n",
    "    token_kl = kl_divergence(p_token, q)\n",
    "    weight_kl = kl_divergence(layer_df['weight_frac'].values, q)\n",
    "    \n",
    "    kl_list.append({\n",
    "        'layer_ix': layer,\n",
    "        'token_kl': token_kl,\n",
    "        'weight_kl': weight_kl\n",
    "    })\n",
    "kl_df = pd.DataFrame(kl_list)\n",
    "\n",
    "px.line(\n",
    "    kl_df,\n",
    "    x = 'layer_ix', y = ['weight_kl', 'token_kl'],\n",
    "    title = 'KL Divergence from Uniform'\n",
    ").update_layout(autosize = False, width = 800, height = 400).show()\n",
    "\n",
    "px.line(\n",
    "    entropy_df,\n",
    "    x = 'layer_ix', y = ['weight_entropy', 'token_entropy'],\n",
    "    title = 'Shannon Entropy'\n",
    ").update_layout(autosize = False, width = 800, height = 400).show()\n",
    "\n",
    "px.line(\n",
    "    topk_grouped.pipe(lambda df: df[df['expert'].isin(list(range(0, 100)))]),\n",
    "    x = 'layer_ix',\n",
    "    y = 'token_count',\n",
    "    color = 'expert'\n",
    ").update_layout(autosize = False, width = 800, height = 400).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
