{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from utils.memory import check_memory, profile_memory, clear_all_cuda_memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import plotly.express as px \n",
    "from tqdm import tqdm\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base model\n",
    "\"\"\"\n",
    "hf_model_id = 'deepseek-ai/DeepSeek-V2-Lite'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test a forward pass with hooks needed to extract topk weights\n",
    "\"\"\"\n",
    "# Hooks needed: https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite/blob/main/modeling_deepseek.py\n",
    "\n",
    "def attach_moe_gate_hooks(model):\n",
    "    \"\"\"\n",
    "    Registers forward-hooks on each MoE gating module in 'model' so that after a forward pass,\n",
    "    we can retrieve the BN x topk 'topk_ix', 'topk_weight', from each layer.\n",
    "    \n",
    "    Returns:\n",
    "        all_expert_ids: A list that will be filled at runtime withtuples of (layer_index, topk_idx_tensor).\n",
    "        handles: A dictionary of {layer_index: hook_handle}, so you can remove them if desired.\n",
    "    \"\"\"\n",
    "    all_expert_ids = []\n",
    "    all_expert_weights = []\n",
    "    handles = {}\n",
    "\n",
    "    def gate_forward_hook(module, input, output):\n",
    "        \"\"\"\n",
    "        This hook is triggered after MoEGate.forward(...).\n",
    "        'output' should be the tuple: (topk_ix, topk_weight, aux_loss).\n",
    "        topk_ix and topk_weight are both BN x topk\n",
    "        \"\"\"\n",
    "        topk_ix, topk_weight, _ = output \n",
    "        # Sort by descending weight\n",
    "        sorted_w, sorted_idx = topk_weight.sort(dim=-1, descending=True)\n",
    "        sorted_ix = torch.gather(topk_ix, dim=-1, index=sorted_idx)\n",
    "\n",
    "        all_expert_ids.append(sorted_ix.detach().cpu())\n",
    "        all_expert_weights.append(sorted_w.detach().cpu())\n",
    "\n",
    "    for layer_ix, layer in enumerate(model.model.layers):\n",
    "        # Layer 0 is not moe\n",
    "        if 'DeepseekV2MLP' not in str(type(layer.mlp)):\n",
    "            # attach an attribute so we know which layer this gating belongs to\n",
    "            layer.mlp.gate._layer_id = layer_ix\n",
    "            hook_handle = layer.mlp.gate.register_forward_hook(gate_forward_hook)\n",
    "            handles[layer_ix] = hook_handle\n",
    "\n",
    "    return all_expert_ids, all_expert_weights, handles\n",
    "\n",
    "def test_forward_pass_with_hooks():\n",
    "\n",
    "    all_expert_ids, all_expert_weights, hook_handles = attach_moe_gate_hooks(model)\n",
    "\n",
    "    inputs = tokenizer(['Hi this is dog', 'Where is the beef'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "\n",
    "    print(f\"all_expert_weights is of length {len(all_expert_ids)}\")\n",
    "    print(f\"all_expert_ids is of length {len(all_expert_weights)}\")\n",
    "\n",
    "    print(f\"all_expert_ids[0] shape = {all_expert_ids[0].shape}\") # should be BN x topk\n",
    "    print(f\"all_expert_weights[0] shape = {all_expert_weights[0].shape}\") # should be BN x topk\n",
    "    \n",
    "    print(f\"all_expert_ids[0]: {all_expert_ids[0]}\")\n",
    "    print(f\"all_expert_weights[0]: {all_expert_weights[0]}\")\n",
    "\n",
    "    for _, h in hook_handles.items():\n",
    "        h.remove()\n",
    "\n",
    "test_forward_pass_with_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Get MMLU data and domains to test\n",
    "\"\"\"\n",
    "raw_mmlu_ds = load_dataset(\"cais/mmlu\", 'all', split = 'test')\n",
    "print(raw_mmlu_ds[0])\n",
    "\n",
    "# all_domains = list(set(mmlu_ds['subject']))\n",
    "# for domain in all_domains:\n",
    "#     dom_length = len([q for q in mmlu_ds if q['subject'] == domain])\n",
    "#     print(f\"{domain}: {dom_length}\")\n",
    "\n",
    "# Dict containing {final_domains: [source1, source2], ...}\n",
    "mmlu_domain_mappings = {\n",
    "    'math': ['elementary_mathematics'], \n",
    "    'cs': ['high_school_computer_science', 'college_computer_science'], # 100 each\n",
    "    'history': ['high_school_world_history'],\n",
    "    # 'psych': ['high_school_psychology'],\n",
    "    'chemistry': ['high_school_chemistry'],\n",
    "    'biology': ['high_school_biology']\n",
    "}\n",
    "\n",
    "# Now let's put the MMLU questions into a list grouped by domain\n",
    "def group_mmlu_ds(raw_ds, domain_map, max_questions_per_domain):\n",
    "    source_to_domain_map = {source: domain for domain, sources in mmlu_domain_mappings.items() for source in sources} # Map each source => domain\n",
    "    final_ds = {domain: {'sources': sources, 'questions': []} for domain, sources in domain_map.items()} # Create empty dict to fill\n",
    "    for q in raw_ds: \n",
    "        if q['subject'] in source_to_domain_map.keys():\n",
    "            if (len(final_ds[source_to_domain_map[q['subject']]]['questions']) >= max_questions_per_domain):\n",
    "                continue\n",
    "            final_ds[source_to_domain_map[q['subject']]]['questions'].append({\n",
    "                'question': q['question'],\n",
    "                'choices': q['choices'],\n",
    "                'answer_index': q['answer'],\n",
    "                'answer_char': chr(65 + q['answer'])\n",
    "            })\n",
    "    return [{'domain': domain, **values} for domain, values in final_ds.items()] # Convert back to list of dicts\n",
    "\n",
    "mmlu_ds = group_mmlu_ds(raw_mmlu_ds, mmlu_domain_mappings, 200)\n",
    "mmlu_ds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create function to map MMLU data into string\n",
    "\"\"\"\n",
    "def prep_question(question, choices):\n",
    "    prompt = f\"Question: {question}\\nChoices:\\n\"\n",
    "    for i, option in enumerate(choices):\n",
    "        letter = chr(65 + i)\n",
    "        prompt += f\"({letter}) {option}\\n\"\n",
    "    return prompt\n",
    "\n",
    "fs_ex = [\n",
    "    [q for q in raw_mmlu_ds if q['subject'] == 'anatomy'][0],\n",
    "    [q for q in raw_mmlu_ds if q['subject'] == 'machine_learning'][0],\n",
    "    [q for q in raw_mmlu_ds if q['subject'] == 'astronomy'][0]\n",
    "]\n",
    "\n",
    "base_prompt = [\n",
    "    {'role': 'system', 'content': 'You will be provided with a multiple-choice question, as well as a list of possible answer choices. Respond exactly with: \"The correct answer is {X}\", substituting in X with the code for the correct choice.'},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[0]['question'], fs_ex[0]['choices'])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + fs_ex[0]['answer'])},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[1]['question'], fs_ex[1]['choices'])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + fs_ex[1]['answer'])},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[2]['question'], fs_ex[2]['choices'])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + chr(65 + fs_ex[2]['answer'])}\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(base_prompt, tokenize = False, add_generation_prompt = False, continue_final_message = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save topk for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_model_and_return_topk(mmlu_ds):\n",
    "\n",
    "    final_results = []\n",
    "\n",
    "    for this_domain in mmlu_ds:\n",
    "\n",
    "        domain_questions = this_domain['questions']\n",
    "        domain_results = []\n",
    "\n",
    "        for question_ix, q in enumerate(domain_questions):\n",
    "\n",
    "            input_prompt = tokenizer.apply_chat_template(\n",
    "                base_prompt + [{'role': 'user', 'content': prep_question(q['question'], q['choices'])}, {'role': 'assistant', 'content': 'The correct answer is'}],\n",
    "                tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    "            )\n",
    "            inputs = tokenizer(input_prompt, return_tensors = 'pt').to(main_device)\n",
    "\n",
    "            all_expert_ids, all_expert_weights, hook_handles = attach_moe_gate_hooks(model)\n",
    "            outputs = model(input_ids = inputs[\"input_ids\"], attention_mask = inputs[\"attention_mask\"])\n",
    "            for _, h in hook_handles.items():\n",
    "                h.remove()\n",
    "\n",
    "            topk_df = convert_topk_to_df(inputs[\"input_ids\"], all_expert_ids, all_expert_weights).assign(question_ix = question_ix).drop(columns = 'sequence_ix')\n",
    "            topk_df = topk_df[topk_df['token_id'] != tokenizer.pad_token_id] # Filter out rows with attention_mask\n",
    "            \n",
    "            predicted_text = tokenizer.decode([torch.argmax(outputs['logits'][0, -1, :]).item()]).strip()\n",
    "            predicted_letter = None\n",
    "            for c in predicted_text:\n",
    "                if c.upper() in [chr(65 + i) for i in range(len(q['choices']))]:\n",
    "                    predicted_letter = c.upper()\n",
    "                    break\n",
    "\n",
    "            domain_results.append({\n",
    "                'question_ix': question_ix, \n",
    "                'model_output': predicted_text,\n",
    "                'model_choice': predicted_letter,\n",
    "                'correct_choice': q['answer_char'],\n",
    "                'is_correct': 1 if predicted_letter == q['answer_char'] else 0,\n",
    "                'is_valid': 1 if predicted_letter is not None else 0,\n",
    "                'topk_df': topk_df\n",
    "            })\n",
    "\n",
    "        n_total = len(domain_results)\n",
    "        n_correct = len([x for x in domain_results if x['is_correct'] == 1])\n",
    "        n_invalid = len([x for x in domain_results if x['is_valid'] == 0])\n",
    "        print(f'{this_domain[\"domain\"]} | Correct: {str(n_correct)} | Incorrect: {str(n_total - n_correct)} | Invalid: {str(n_invalid)} | Accuracy: {(n_correct / (n_total)) * 100:.1f}%')\n",
    "        \n",
    "        final_results.append({\n",
    "            'domain': this_domain['domain'],\n",
    "            'question_df': pd.DataFrame([{k: v for k, v in x.items() if k != 'topk_df'} for x in domain_results]).assign(domain = this_domain['domain']),\n",
    "            'topk_df': pd.concat([x['topk_df'] for x in domain_results]).assign(domain = this_domain['domain']),\n",
    "            'n_correct': n_correct,\n",
    "            'n_total': n_total,\n",
    "            'n_invalid': n_invalid,\n",
    "        })\n",
    "\n",
    "\n",
    "    return {\n",
    "        'question_df': pd.concat([d['question_df'] for d in final_results]),\n",
    "        'topk_df': pd.concat([d['topk_df'] for d in final_results]),\n",
    "        'n_correct': sum([d['n_correct'] for d in final_results]),\n",
    "        'n_total': sum([d['n_total'] for d in final_results]),\n",
    "        'n_invalid': sum([d['n_invalid'] for d in final_results]),\n",
    "        'accuracy': sum([d['n_correct'] for d in final_results])/sum([d['n_total'] for d in final_results])\n",
    "        }\n",
    "\n",
    "baseline = run_model_and_return_topk(mmlu_ds)\n",
    "display(baseline['topk_df'])\n",
    "print(baseline['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map =\\\n",
    "    pd.DataFrame([{\"token\": token.replace('Ġ', ' '), \"token_id\": token_id} for token, token_id in tokenizer.get_vocab().items()])\\\n",
    "    .sort_values(by = 'token_id')\\\n",
    "    .reset_index()\\\n",
    "    .drop(columns = 'index')\n",
    "\n",
    "display(vocab_map)\n",
    "\n",
    "all_answers = baseline['question_df']\n",
    "display(all_answers)\n",
    "\n",
    "all_topks = baseline['topk_df'].assign(weight = lambda df: np.around(df['weight'], 4))\n",
    "display(all_topks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map.to_csv('vocab.csv', index = False)\n",
    "all_answers.to_csv('all_answers.csv', index = False)\n",
    "all_topks.to_csv('all_topks.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = tokenizer.apply_chat_template(\n",
    "    base_prompt + [\n",
    "        {'role': 'user', 'content': prep_question(mmlu_ds[0]['questions'][0]['question'], mmlu_ds[0]['questions'][0]['choices'])},\n",
    "        {'role': 'assistant', 'content': 'The correct answer is'}\n",
    "    ],\n",
    "    tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    ")\n",
    "inputs = tokenizer(input_prompt, return_tensors = 'pt').to(main_device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids = inputs[\"input_ids\"], attention_mask = inputs['attention_mask'])\n",
    "    print(outputs['logits'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.model(input_ids = inputs[\"input_ids\"], attention_mask = inputs['attention_mask'])\n",
    "    hidden_state = outputs[0]\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    print(logits)\n",
    "\n",
    "from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n",
    "\n",
    "with torch.no_grad():\n",
    "    B, N = inputs[\"input_ids\"].shape[:2]\n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "\n",
    "    inputs_embeds = model.model.embed_tokens(inputs[\"input_ids\"])\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(inputs['attention_mask'], (B, N), inputs_embeds, 0,)\n",
    "\n",
    "    hidden_state = inputs_embeds\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "\n",
    "    for decoder_layer in model.model.layers:\n",
    "        # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "        residual = hidden_state\n",
    "        hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "        # Self Attention\n",
    "        hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "        hidden_state = residual + hidden_state\n",
    "        # Fully Connected\n",
    "        residual = hidden_state\n",
    "        hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "        ## MLP\n",
    "        if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "            hidden_state = decoder_layer.mlp(hidden_state)\n",
    "        else:\n",
    "            identity = hidden_state\n",
    "            orig_shape = hidden_state.shape\n",
    "            ### moegate - originally topk_idx, topk_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "            bsz, seq_len, h = hidden_state.shape\n",
    "            moe_hidden_state = hidden_state.view(-1, h)\n",
    "            logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), decoder_layer.mlp.gate.weight.type(torch.float32), None)\n",
    "            scores = logits.softmax(dim=-1, dtype=torch.float32)\n",
    "            topk_weight, topk_idx = torch.topk(scores, k=decoder_layer.mlp.gate.top_k, dim=-1, sorted=False)\n",
    "            topk_weight = topk_weight * decoder_layer.mlp.gate.routed_scaling_factor\n",
    "            ####\n",
    "            hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "            flat_topk_idx = topk_idx.view(-1)\n",
    "            ### moe infer\n",
    "            x = hidden_state\n",
    "            topk_ids = topk_idx\n",
    "            cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "            cnts.scatter_(1, topk_ids, 1)\n",
    "            tokens_per_expert = cnts.sum(dim=0)\n",
    "            idxs = topk_ids.view(-1).argsort()\n",
    "            sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "            tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "            outputs = []\n",
    "            start_idx = 0\n",
    "            for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                end_idx = start_idx + num_tokens\n",
    "                if num_tokens == 0:\n",
    "                    continue\n",
    "                expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                expert_out = expert(tokens_for_this_expert)\n",
    "                outputs.append(expert_out)\n",
    "                start_idx = end_idx\n",
    "            outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "            new_x = torch.empty_like(outs)\n",
    "            new_x[idxs] = outs\n",
    "            final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "            ###\n",
    "            y = final_out.view(*orig_shape)\n",
    "            if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "            hidden_state = y\n",
    "\n",
    "            all_topk_experts.append(topk_ids)\n",
    "            all_topk_weights.append(topk_weight)\n",
    "\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    print(logits)\n",
    "    print(len(all_topk_experts))\n",
    "    print(all_topk_experts[0].shape)\n",
    "    print(all_topk_weights[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_model_no_ablation(input_ids, attention_mask):\n",
    "    B, N = input_ids.shape[:2]\n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "\n",
    "    inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "    hidden_state = inputs_embeds\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer_ix, decoder_layer in enumerate(model.model.layers):\n",
    "        # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "        residual = hidden_state\n",
    "        hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "        # Self Attention\n",
    "        hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "        hidden_state = residual + hidden_state\n",
    "        # Fully Connected\n",
    "        residual = hidden_state\n",
    "        hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "        ## MLP\n",
    "        if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "            hidden_state = decoder_layer.mlp(hidden_state)\n",
    "        else:\n",
    "            identity = hidden_state\n",
    "            orig_shape = hidden_state.shape\n",
    "            ### moegate - originally topk_idx, topk_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "            bsz, seq_len, h = hidden_state.shape\n",
    "            moe_hidden_state = hidden_state.view(-1, h)\n",
    "            logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), decoder_layer.mlp.gate.weight.type(torch.float32), None)\n",
    "            scores = logits.softmax(dim=-1, dtype=torch.float32)\n",
    "            topk_weight, topk_idx = torch.topk(scores, k=decoder_layer.mlp.gate.top_k, dim=-1, sorted=False)\n",
    "            topk_weight = topk_weight * decoder_layer.mlp.gate.routed_scaling_factor\n",
    "            ####\n",
    "            hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "            ### moe infer\n",
    "            x = hidden_state\n",
    "            topk_ids = topk_idx\n",
    "            cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "            cnts.scatter_(1, topk_ids, 1)\n",
    "            tokens_per_expert = cnts.sum(dim=0)\n",
    "            idxs = topk_ids.view(-1).argsort()\n",
    "            sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "            tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "            outputs = []\n",
    "            start_idx = 0\n",
    "            for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                end_idx = start_idx + num_tokens\n",
    "                if num_tokens == 0:\n",
    "                    continue\n",
    "                expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                expert_out = expert(tokens_for_this_expert)\n",
    "                outputs.append(expert_out)\n",
    "                start_idx = end_idx\n",
    "            outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "            new_x = torch.empty_like(outs)\n",
    "            new_x[idxs] = outs\n",
    "            final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "            ###\n",
    "            y = final_out.view(*orig_shape)\n",
    "            if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "            hidden_state = y\n",
    "\n",
    "            all_topk_experts.append(topk_ids)\n",
    "            all_topk_weights.append(topk_weight)\n",
    "\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_with_ablation(run_forward_fn, return_topk, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluate modified model\n",
    "\n",
    "    Params:\n",
    "        @run_forward_fn: A function that returns a model forward pass with inputs input_ids, attention_mask, and optional *args/**kwargs. \n",
    "          The function must return a dict with key `logits`.\n",
    "        @return_topk: Whether to return the expert IDs and weights as well. If True, `run_forward_fn` must also return keys\n",
    "         `all_topk_experts` and `all_topk_weights`.\n",
    "        @*args/**kwargs: Additional arguments to pass to `run_forward_fn`.\n",
    "    \"\"\"\n",
    "    final_results = []\n",
    "\n",
    "    for this_domain in mmlu_ds:\n",
    "\n",
    "        domain_questions = this_domain['questions']\n",
    "        domain_results = []\n",
    "\n",
    "        for question_ix, q in enumerate(domain_questions):\n",
    "\n",
    "            input_prompt = tokenizer.apply_chat_template(\n",
    "                base_prompt + [{'role': 'user', 'content': prep_question(q['question'], q['choices'])}, {'role': 'assistant', 'content': 'The correct answer is'}],\n",
    "                tokenize = False, add_generation_prompt = False, continue_final_message = True # Otherwise appends eos token\n",
    "            )\n",
    "            inputs = tokenizer(input_prompt, return_tensors = 'pt').to(main_device)\n",
    "\n",
    "            outputs = run_forward_fn(inputs['input_ids'], inputs['attention_mask'], *args, **kwargs)\n",
    "            \n",
    "            if return_topk == True:\n",
    "                topk_df = convert_topk_to_df(inputs[\"input_ids\"], outputs['all_topk_experts'], outputs['all_topk_weights']).assign(question_ix = question_ix).drop(columns = 'sequence_ix')\n",
    "                topk_df = topk_df[topk_df['token_id'] != tokenizer.pad_token_id] # Filter out rows with attention_mask\n",
    "\n",
    "            predicted_text = tokenizer.decode([torch.argmax(outputs['logits'][0, -1, :]).item()]).strip()\n",
    "            predicted_letter = None\n",
    "            for c in predicted_text:\n",
    "                if c.upper() in [chr(65 + i) for i in range(len(q['choices']))]:\n",
    "                    predicted_letter = c.upper()\n",
    "                    break\n",
    "\n",
    "            domain_results.append({\n",
    "                'question_ix': question_ix, \n",
    "                'model_output': predicted_text,\n",
    "                'model_choice': predicted_letter,\n",
    "                'correct_choice': q['answer_char'],\n",
    "                'is_correct': 1 if predicted_letter == q['answer_char'] else 0,\n",
    "                'is_valid': 1 if predicted_letter is not None else 0,\n",
    "                'topk_df': topk_df if return_topk == True else None\n",
    "            })\n",
    "\n",
    "        n_total = len(domain_results)\n",
    "        n_correct = len([x for x in domain_results if x['is_correct'] == 1])\n",
    "        n_invalid = len([x for x in domain_results if x['is_valid'] == 0])\n",
    "        print(f'{this_domain[\"domain\"]} | Correct: {str(n_correct)} | Incorrect: {str(n_total - n_correct)} | Invalid: {str(n_invalid)} | Accuracy: {(n_correct / (n_total)) * 100:.1f}%')\n",
    "        \n",
    "        final_results.append({\n",
    "            'domain': this_domain['domain'],\n",
    "            'question_df': pd.DataFrame([{k: v for k, v in x.items() if k != 'topk_df'} for x in domain_results]).assign(domain = this_domain['domain']),\n",
    "            'topk_df': pd.concat([x['topk_df'] for x in domain_results]).assign(domain = this_domain['domain']) if return_topk == True else None,\n",
    "            'n_correct': n_correct,\n",
    "            'n_total': n_total,\n",
    "            'n_invalid': n_invalid,\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'question_df': pd.concat([d['question_df'] for d in final_results]),\n",
    "        'topk_df': pd.concat([d['topk_df'] for d in final_results]) if return_topk == True else None,\n",
    "        'n_correct': sum([d['n_correct'] for d in final_results]),\n",
    "        'n_total': sum([d['n_total'] for d in final_results]),\n",
    "        'n_invalid': sum([d['n_invalid'] for d in final_results]),\n",
    "        'accuracy': sum([d['n_correct'] for d in final_results])/sum([d['n_total'] for d in final_results])\n",
    "        }\n",
    "\n",
    "evaluate_with_ablation(run_model_no_ablation, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_model_with_ablation(input_ids, attention_mask, layers_to_ablate = list(range(1, 27)), topk_to_ablate = [0], renorm = False):\n",
    "    \"\"\"\n",
    "    Ablates certain rank-ordered top-k columns for the specified layers.\n",
    "    \n",
    "    Params:\n",
    "        @layers_to_ablate: Which layer indices (0-based) should we ablate?\n",
    "        @renorm: Whether to rescale the remaining weights to keep the sum unchanged\n",
    "        @topk_to_ablate: A list of ranks to ablate, e.g. [0] = top-1, [2,3] = 3rd & 4th largest, etc.\n",
    "    \"\"\"\n",
    "    B, N = input_ids.shape[:2]\n",
    "    position_ids = torch.arange(0, N, dtype=torch.long, device = main_device).unsqueeze(0)\n",
    "\n",
    "    inputs_embeds = model.model.embed_tokens(input_ids)\n",
    "    attention_mask = _prepare_4d_causal_attention_mask(attention_mask, (B, N), inputs_embeds, 0,)\n",
    "\n",
    "    hidden_state = inputs_embeds\n",
    "    all_topk_experts = []\n",
    "    all_topk_weights = []\n",
    "    for layer_ix, decoder_layer in enumerate(model.model.layers):\n",
    "        # layer_outputs = decoder_layer(hidden_state, attention_mask = attention_mask, position_ids = position_ids,)\n",
    "        residual = hidden_state\n",
    "        hidden_state = decoder_layer.input_layernorm(hidden_state)\n",
    "        # Self Attention\n",
    "        hidden_state, self_attn_weights, present_key_value = decoder_layer.self_attn(hidden_states = hidden_state, attention_mask = attention_mask, position_ids = position_ids)\n",
    "        hidden_state = residual + hidden_state\n",
    "        # Fully Connected\n",
    "        residual = hidden_state\n",
    "        hidden_state = decoder_layer.post_attention_layernorm(hidden_state)\n",
    "        ## MLP\n",
    "        if 'DeepseekV2MLP' in str(type(decoder_layer.mlp)):\n",
    "            hidden_state = decoder_layer.mlp(hidden_state)\n",
    "        else:\n",
    "            identity = hidden_state\n",
    "            orig_shape = hidden_state.shape\n",
    "            ### moegate - originally topk_idx, topk_weight, aux_loss = decoder_layer.mlp.gate(hidden_state)\n",
    "            bsz, seq_len, h = hidden_state.shape\n",
    "            moe_hidden_state = hidden_state.view(-1, h)\n",
    "            logits = torch.nn.functional.linear(moe_hidden_state.type(torch.float32), decoder_layer.mlp.gate.weight.type(torch.float32), None)\n",
    "            scores = logits.softmax(dim=-1, dtype=torch.float32)\n",
    "            topk_weight, topk_idx = torch.topk(scores, k=decoder_layer.mlp.gate.top_k, dim=-1, sorted=True)\n",
    "            topk_weight = topk_weight * decoder_layer.mlp.gate.routed_scaling_factor\n",
    "            ####\n",
    "            ######################## ABLATION\n",
    "            # shape: topk_weight is [B*N, top_k]\n",
    "            if layer_ix in layers_to_ablate:\n",
    "                # (A) Sort the topk dimension locally to find which columns correspond to the rank-ordered experts (note shape of topk_weight: [BN, k])\n",
    "                sorted_w, sorted_idx = topk_weight.sort(dim=-1, descending=True)\n",
    "                # sorted_w[:,0] is the largest weight in each row, sorted_idx[:,0] gives the original column index for that largest weight.\n",
    "                row_sum_before = topk_weight.sum(dim=-1, keepdim=True)\n",
    "\n",
    "                # (B) For each rank in topk_to_ablate, zero out that column in topk_weight\n",
    "                for rank in topk_to_ablate:\n",
    "                    columns_to_ablate = sorted_idx[:, rank]  # columns_to_ablate is [BN], each entry is the \"original column\" that corresponds to rank `rank` in sorted order\n",
    "                    # Now zero out topk_weight[row, col]\n",
    "                    for row_i in range(topk_weight.shape[0]):\n",
    "                        col_j = columns_to_ablate[row_i].item()\n",
    "                        topk_weight[row_i, col_j] = 0.0\n",
    "\n",
    "                # Re-scale the remaining top-k weights to keep sum the same\n",
    "                if renorm:\n",
    "                    row_sum_after = topk_weight.sum(dim=-1, keepdim=True)\n",
    "                    scale_factor = row_sum_before / (row_sum_after + 1e-9)\n",
    "                    topk_weight *= scale_factor\n",
    "            ######################## The rest is unchanged\n",
    "            hidden_state = hidden_state.view(-1, hidden_state.shape[-1])\n",
    "            ### moe infer\n",
    "            x = hidden_state\n",
    "            topk_ids = topk_idx\n",
    "            cnts = topk_ids.new_zeros((topk_ids.shape[0], len(decoder_layer.mlp.experts)))\n",
    "            cnts.scatter_(1, topk_ids, 1)\n",
    "            tokens_per_expert = cnts.sum(dim=0)\n",
    "            idxs = topk_ids.view(-1).argsort()\n",
    "            sorted_tokens = x[idxs // topk_ids.shape[1]]\n",
    "            tokens_per_expert = tokens_per_expert.cpu().numpy()\n",
    "            outputs = []\n",
    "            start_idx = 0\n",
    "            for i, num_tokens in enumerate(tokens_per_expert):\n",
    "                end_idx = start_idx + num_tokens\n",
    "                if num_tokens == 0:\n",
    "                    continue\n",
    "                expert = decoder_layer.mlp.experts[i + decoder_layer.mlp.ep_rank * decoder_layer.mlp.experts_per_rank]\n",
    "                tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n",
    "                expert_out = expert(tokens_for_this_expert)\n",
    "                outputs.append(expert_out)\n",
    "                start_idx = end_idx\n",
    "            outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)\n",
    "            new_x = torch.empty_like(outs)\n",
    "            new_x[idxs] = outs\n",
    "            final_out = (new_x.view(*topk_ids.shape, -1).type(topk_weight.dtype).mul_(topk_weight.unsqueeze(dim=-1)).sum(dim=1).type(new_x.dtype))\n",
    "            ###\n",
    "            y = final_out.view(*orig_shape)\n",
    "            if decoder_layer.mlp.config.n_shared_experts is not None:\n",
    "                y = y + decoder_layer.mlp.shared_experts(identity)\n",
    "            hidden_state = y\n",
    "\n",
    "            all_topk_experts.append(topk_ids)\n",
    "            all_topk_weights.append(topk_weight)\n",
    "\n",
    "        hidden_state = residual + hidden_state\n",
    "\n",
    "    hidden_state = model.model.norm(hidden_state)\n",
    "    logits = model.lm_head(hidden_state)\n",
    "    return {'logits': logits, 'all_topk_experts': all_topk_experts, 'all_topk_weights': all_topk_weights}\n",
    "    \n",
    "# evaluate_with_ablation(run_model_with_ablation, return_topk = False, layers_to_ablate = list(range(1, 6)), topk_to_ablate = [0], renorm = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_ablate_list = [list(range(1, 27)), list(range(22, 27)), list(range(17, 27)), list(range(7, 27)), list(range(0, 4))]\n",
    "topk_to_ablate_list = [list(range(0, 1)), list(range(1, 6)), list(range(0, 6))]\n",
    "\n",
    "all_res = []\n",
    "for layers_to_ablate in layers_to_ablate_list:\n",
    "    for topk_to_ablate in topk_to_ablate_list:\n",
    "        print('\\n\\n---------------')\n",
    "        print(f\"Layers ablated: {','.join([str(x) for x in layers_to_ablate])}\")\n",
    "        print(f\"Topk ablated: {','.join([str(x) for x in topk_to_ablate])}\")\n",
    "        res = evaluate_with_ablation(run_model_with_ablation, return_topk = False, layers_to_ablate = layers_to_ablate, topk_to_ablate = topk_to_ablate, renorm = False)\n",
    "        print(res['accuracy'])\n",
    "        all_res.append({\n",
    "            'layers_ablated': layers_to_ablate,\n",
    "            'topk_ablated': topk_to_ablate,\n",
    "            'results': res\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
