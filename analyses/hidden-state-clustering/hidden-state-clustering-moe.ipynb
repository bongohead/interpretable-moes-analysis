{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to test MoE hidden states & understand routing weights.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "\n",
    "# https://docs.rapids.ai/install/\n",
    "import cupy\n",
    "import cuml\n",
    "\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_id = 'allenai/OLMoE-1B-7B-0125-Instruct'\n",
    "model_prefix = 'olmoe'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix):\n",
    "    all_pre_mlp_hs = torch.load(f'data/{model_prefix}-all-pre-mlp-hidden-states.pt')\n",
    "    with open(f'data/{model_prefix}-metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    return all_pre_mlp_hs, metadata['sample_df'], metadata['topk_df']\n",
    "\n",
    "all_pre_mlp_hs, sample_df_import, topk_df_import = load_data(model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze routing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Norms by expert and layer\n",
    "\"\"\"\n",
    "norms_by_expert_layer = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        'layer_ix': layer_ix,\n",
    "        'norm': torch.linalg.norm(layer.mlp.gate.weight, dim = 1, ord = 1).to(torch.float16).cpu().detach().numpy(),\n",
    "        'expert': list(range(1, layer.mlp.gate.weight.shape[0] + 1))\n",
    "    })\n",
    "    for layer_ix, layer in enumerate(model.model.layers)\n",
    "])\n",
    "\n",
    "plot_df = norms_by_expert_layer.pivot(index = 'layer_ix', columns = 'expert', values = 'norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Norm by Expert and Layer\"\n",
    ").update_layout(autosize = False, width = 800).show()\n",
    "\n",
    "scaled_df =\\\n",
    "    norms_by_expert_layer\\\n",
    "    .assign(layer_mean = lambda df: df.groupby('layer_ix')['norm'].transform('mean'))\\\n",
    "    .assign(norm_scaled = lambda df: df['norm'] / df['layer_mean'] - 1)\n",
    "\n",
    "scaled_plot_df = scaled_df.pivot(index = 'layer_ix', columns = 'expert', values = 'norm_scaled')\n",
    "px.imshow(\n",
    "    scaled_plot_df,\n",
    "    x = scaled_plot_df.columns, y = scaled_plot_df.index,\n",
    "    color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Norm by Expert and Layer\"\n",
    ").update_layout(autosize = False, width = 800).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For a single layer, what do the weights and RMSnorms look like?\n",
    "\"\"\"\n",
    "plot_layer_ix = 9\n",
    "show_dims = list(range(0, 400))\n",
    "\n",
    "# RMSNorm\n",
    "rms_tensor = model.model.layers[plot_layer_ix].post_attention_layernorm.weight\n",
    "rms_df = pd.DataFrame({\n",
    "    'gamma': rms_tensor.to(torch.float16).cpu().detach().numpy(),\n",
    "    'coef': 1,\n",
    "    'dimension': list(range(0, rms_tensor.shape[0]))\n",
    "})\n",
    "plot_df = rms_df.pipe(lambda df: df[df['dimension'].isin(show_dims)]).pivot(index = 'coef', columns = 'dimension', values = 'gamma')\n",
    "\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"RMSNorm Scaling Values\"\n",
    ").update_layout(autosize = False, width = 1400, height = 400).show()\n",
    "\n",
    "\n",
    "# Weights\n",
    "wt_tensor = model.model.layers[plot_layer_ix].mlp.gate.weight\n",
    "wt_df = pd.DataFrame({\n",
    "    'value': wt_tensor.view(-1).to(torch.float16).cpu().detach().numpy(),\n",
    "    'expert': [i // wt_tensor.shape[1] for i in range(wt_tensor.view(-1).shape[0])],\n",
    "    'dimension': [i % wt_tensor.shape[1] for i in range(wt_tensor.view(-1).shape[0])]\n",
    "})\n",
    "\n",
    "plot_df = wt_df.pipe(lambda df: df[df['dimension'].isin(show_dims)]).pivot(index = 'expert', columns = 'dimension', values = 'value')\n",
    "\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Routing Weights\"\n",
    ").update_layout(autosize = False, width = 1400).show()\n",
    "\n",
    "# Scale weights by RMSNorm\n",
    "scaled_df = wt_df.merge(rms_df, on = 'dimension', how = 'inner').assign(gamma_scaled_value = lambda df: df['gamma'] * df['value'])\n",
    "plot_df = scaled_df.pipe(lambda df: df[df['dimension'].isin(show_dims)]).pivot(index = 'expert', columns = 'dimension', values = 'gamma_scaled_value')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Scaled Routing Weights\"\n",
    ").update_layout(autosize = False, width = 1400).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mean norms across layers and dimension (averaged across experts)\n",
    "\"\"\"\n",
    "dfs_list = []\n",
    "for layer_ix, layer in enumerate(model.model.layers):\n",
    "    wt_tensor = layer.mlp.gate.weight.to(torch.float16).cpu().detach()\n",
    "    rms_tensor = layer.post_attention_layernorm.weight.to(torch.float16).cpu().detach()\n",
    "    scaled = (wt_tensor * rms_tensor) # Mltiply by RMS norm\n",
    "    scaled = scaled.abs().mean(dim = 0) # Take mean L1 norm\n",
    "    dfs_list.append(pd.DataFrame({\n",
    "        'mean_norm': scaled.numpy(),\n",
    "        'layer_ix': layer_ix,\n",
    "        'dim': list(range(1, scaled.shape[0] + 1))\n",
    "    }))\n",
    "\n",
    "my_df = pd.concat(dfs_list)\n",
    "# Additionally scale by layer average\n",
    "my_df_ex_scale =\\\n",
    "    my_df\\\n",
    "    .assign(layer_mean = lambda df: df.groupby('layer_ix')['mean_norm'].transform('mean'))\\\n",
    "    .assign(mean_norm = lambda df: df['mean_norm'] / df['layer_mean'])\n",
    "\n",
    "plot_df = my_df_ex_scale.pipe(lambda df: df[df['dim']  <= 200]).pivot(index = 'layer_ix', columns = 'dim', values = 'mean_norm')\n",
    "\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    zmin = 0, zmax = 8,\n",
    "    aspect = 'auto', # Allow non-square boxes\n",
    "    color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "At dimension x layer-level, analyze activations (averaged across samples) versus routing weights (averaged across experts).\n",
    "\"\"\"\n",
    "show_dims = list(range(0, 800))\n",
    "\n",
    "pre_mlp_for_layer_norms_all = all_pre_mlp_hs[0:500_000, :, :].abs().mean(dim = 0) # Collapse to n_layers x D dimensional\n",
    "\n",
    "dfs_list = []\n",
    "for layer_ix, pre_mlp_for_layer_norm in enumerate(pre_mlp_for_layer_norms_all.unbind(dim = 0)):\n",
    "    wt_tensor = model.model.layers[layer_ix].mlp.gate.weight[:, :].to(torch.float16).cpu().detach() # (n_experts, D)\n",
    "    act_tensor = pre_mlp_for_layer_norm.cpu().detach() # D-dimensional\n",
    "    scaled = (wt_tensor * act_tensor) # Multiply by activation tensor\n",
    "    scaled = scaled.abs().mean(dim = 0) # Take mean L1 norm\n",
    "    dfs_list.append(pd.DataFrame({\n",
    "        'act_norm': act_tensor.numpy(),\n",
    "        'wt_norm': wt_tensor.abs().mean(dim = 0),\n",
    "        'mean_scaled_norm': scaled.numpy(),\n",
    "        'layer_ix': layer_ix,\n",
    "        'dim': list(range(1, scaled.shape[0] + 1)) # show_dims\n",
    "    }))\n",
    "\n",
    "pre_mlp_df = pd.concat(dfs_list)\n",
    "del dfs_list, pre_mlp_for_layer_norms_all\n",
    "\n",
    "plot_df = pre_mlp_df.pipe(lambda df: df[df['dim'].isin(show_dims)]).pivot(index = 'layer_ix', columns = 'dim', values = 'mean_scaled_norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    zmin = 0,\n",
    "    zmax = .2,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean scaled wt * activation norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "plot_df = pre_mlp_df.pipe(lambda df: df[df['dim'].isin(show_dims)]).pivot(index = 'layer_ix', columns = 'dim', values = 'act_norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    zmin = 0, zmax = 8,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean activation norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "plot_df = pre_mlp_df.pipe(lambda df: df[df['dim'].isin(show_dims)]).pivot(index = 'layer_ix', columns = 'dim', values = 'wt_norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean weight norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "# Plot activation vs routing weight norms\n",
    "px.scatter(\n",
    "    pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == 6]),\n",
    "    x = 'wt_norm', y = 'act_norm', color = 'wt_norm',\n",
    "    log_y = True,\n",
    "    log_x = True,\n",
    "    color_continuous_scale = 'viridis', title = 'Per-Dimension Plot - Activation L1 Norm versus Routing Weight L1 Norm'\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "import scipy\n",
    "scipy.stats.kurtosis(pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == 6]['act_norm'].tolist() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linear regression - test ability to reconstruct topk expert id\n",
    "\"\"\"\n",
    "layer_to_test = 5\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == layer_to_test])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "\n",
    "lr_model = cuml.linear_model.LogisticRegression(\n",
    "    penalty = 'l2', \n",
    "    max_iter = 10000,\n",
    "    fit_intercept = False\n",
    ")\n",
    "\n",
    "dims = [\n",
    "    x - 1\n",
    "    for x in pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == layer_to_test]).sort_values(by = 'mean_scaled_norm', ascending = False)['dim'].tolist()\n",
    "]\n",
    "\n",
    "layer_hs = cupy.asarray(all_pre_mlp_hs[:, layer_to_test, dims[0:all_pre_mlp_hs.shape[2]//50]].to(torch.float16).detach().cpu())\n",
    "lr_model.fit(layer_hs, expert_ids_cp)\n",
    "accuracy = lr_model.score(layer_hs, expert_ids_cp)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "rand_dims = [int(x - 1) for x in np.random.choice(pre_mlp_df['dim'].tolist(), size = all_pre_mlp_hs.shape[2] // 50, replace = False)]\n",
    "rand_hs = cupy.asarray(all_pre_mlp_hs[:, layer_to_test, rand_dims].to(torch.float16).detach().cpu())\n",
    "lr_model.fit(rand_hs, expert_ids_cp)\n",
    "accuracy = lr_model.score(layer_hs, expert_ids_cp)\n",
    "print(f\"Baseline accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compare PCA top dimensinos versus scaled activation top dimensions\n",
    "\"\"\"\n",
    "layer_hs = cupy.asarray(all_pre_mlp_hs[0:200_000, layer_to_test, :].to(torch.float16).detach().cpu())\n",
    "mean_vals = cupy.mean(layer_hs, axis=0)\n",
    "std_vals = cupy.std(layer_hs, axis=0)\n",
    "std_vals = cupy.where(std_vals == 0, cupy.asarray(1e-7), std_vals)\n",
    "layer_hs_std = (layer_hs - mean_vals)/std_vals\n",
    "\n",
    "pca = cuml.decomposition.PCA(n_components = 10, random_state = 123)\n",
    "pca.fit(layer_hs_std)\n",
    "\n",
    "pc_loadings = pca.components_\n",
    "sumsq = (pc_loadings ** 2).sum(axis=0)\n",
    "\n",
    "ranking = cupy.argsort(-sumsq)  # descending order\n",
    "pca_top_dims = ranking.tolist()\n",
    "\n",
    "plot_df =\\\n",
    "    pd.DataFrame({'pca_sumsq': cupy.asarray(sumsq).tolist(), 'dim': list(range(1, len(sumsq) + 1))})\\\n",
    "    .merge(\n",
    "        pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == layer_to_test])[['dim', 'mean_scaled_norm']],\n",
    "        on = 'dim',\n",
    "        how = 'inner'\n",
    "    )\n",
    "\n",
    "px.scatter(\n",
    "    plot_df,\n",
    "    x = 'mean_scaled_norm',\n",
    "    y = 'pca_sumsq',\n",
    "    log_y = True,\n",
    "    log_x = True\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What % of hidden states is explained by PCA?\n",
    "\"\"\"\n",
    "# 1) Gather some data\n",
    "clear_all_cuda_memory()\n",
    "layer_hs = cupy.asarray(all_pre_mlp_hs[0:200_000, 5, :].to(torch.float16).detach().cpu())\n",
    "\n",
    "# 2) Fit PCA\n",
    "pca_model = cuml.PCA(iterated_power = 20, n_components = 10, verbose = True)\n",
    "pca_model.fit(layer_hs)\n",
    "\n",
    "print(\"Explained variance ratio:\", pca_model.explained_variance_ratio_)\n",
    "print(\"Cumulative ratio:\", np.cumsum(pca_model.explained_variance_ratio_.get())[-1])\n",
    "\n",
    "# 3) Retrieve components & variance ratio\n",
    "components = pca_model.components_.get()  # shape = (10, D)\n",
    "expl_ratios = pca_model.explained_variance_ratio_.get()  # shape = (10,)\n",
    "\n",
    "# 4) Compute dimension-level importance\n",
    "sq_loadings = components**2        # shape (10, D)\n",
    "dim_importance = sq_loadings.T @ expl_ratios   # shape (D,)\n",
    "\n",
    "# 5) Identify top 20 dims\n",
    "top_k = 10\n",
    "idx_sorted = np.argsort(dim_importance)[::-1]\n",
    "top_dims = idx_sorted[:top_k]\n",
    "sum_top = dim_importance[top_dims].sum()\n",
    "sum_all = dim_importance.sum()\n",
    "frac_top = sum_top / sum_all\n",
    "\n",
    "print(f\"Top {top_k} dims by PCA-based importance: {top_dims}\")\n",
    "print(f\"Sum of their importances: {sum_top:.4f}\")\n",
    "print(f\"Fraction of total importance: {frac_top:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_layers = np.array(sorted(list(set(topk_df['layer_ix']))))\n",
    "unique_experts = np.array(sorted(list(set(topk_df['expert'])))) \n",
    "\n",
    "topk_grouped_0 =\\\n",
    "    topk_df.groupby(['layer_ix', 'expert'], as_index = False)\\\n",
    "    .agg(\n",
    "        token_count = ('sample_ix', 'nunique'), # count distinct tokens\n",
    "        weight_sum = ('weight', 'sum') # sum of gating weights\n",
    "    )\n",
    "\n",
    "pd.merge(\n",
    "    pd.DataFrame({'layer_ix': unique_layers}),\n",
    "    pd.DataFrame({'expert': unique_experts}),\n",
    "    how = 'cross'\n",
    ")\\\n",
    ".merge(topk_grouped_0, how = 'left', on = ['layer_ix', 'expert'])\\\n",
    ".assign(\n",
    "    token_count = lambda df: df['token_count'].fillna(0),\n",
    "    weight_sum = lambda df: df['weight_sum'].fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate load balancing metrics\n",
    "\"\"\"\n",
    "topk_grouped_0 =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    .groupby(['layer_ix', 'expert'], as_index = False)\\\n",
    "    .agg(\n",
    "        token_count = ('sample_ix', 'nunique'), # count distinct tokens\n",
    "        weight_sum = ('weight', 'sum') # sum of gating weights\n",
    "    )\n",
    "\n",
    "unique_layers = np.array(sorted(list(set(topk_df['layer_ix']))))\n",
    "unique_experts = np.array(sorted(list(set(topk_df['expert'])))) \n",
    "\n",
    "# Fill in missing expert/layers\n",
    "topk_grouped =\\\n",
    "    pd.merge(\n",
    "        pd.DataFrame({'layer_ix': unique_layers}),\n",
    "        pd.DataFrame({'expert': unique_experts}),\n",
    "        how = 'cross'\n",
    "    )\\\n",
    "    .merge(topk_grouped_0, how = 'left', on = ['layer_ix', 'expert'])\\\n",
    "    .assign(\n",
    "        token_count = lambda df: df['token_count'].fillna(0),\n",
    "        weight_sum = lambda df: df['weight_sum'].fillna(0)\n",
    "    )\\\n",
    "    .assign(\n",
    "        layer_token_sums = lambda df: df.groupby('layer_ix')['token_count'].transform('sum'), # fraction of tokens that pick (layer, expert)\n",
    "        layer_weight_sums = lambda df: df.groupby('layer_ix')['weight_sum'].transform('sum'),\n",
    "        token_frac = lambda df: df['token_count'] / df['layer_token_sums'],\n",
    "        weight_frac = lambda df: df['weight_sum'] / df['layer_weight_sums']\n",
    "    )\n",
    "\n",
    "def shannon_entropy(probs):\n",
    "    # Avoid log(0)\n",
    "    probs = probs[probs > 0]\n",
    "    return -np.sum(probs * np.log2(probs))\n",
    "\n",
    "entropies = []\n",
    "for layer, layer_df in topk_grouped.groupby('layer_ix'):\n",
    "    token_entropy = shannon_entropy(layer_df['token_frac'].values)\n",
    "    weight_entropy = shannon_entropy(layer_df['weight_frac'].values)\n",
    "    entropies.append({\n",
    "        'layer_ix': layer,\n",
    "        'token_entropy': token_entropy,\n",
    "        'weight_entropy': weight_entropy\n",
    "    })\n",
    "entropy_df = pd.DataFrame(entropies)\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    mask = (p > 0) & (q > 0)\n",
    "    return np.sum(p[mask] * np.log2(p[mask]/q[mask]))\n",
    "\n",
    "kl_list = []\n",
    "for layer, layer_df in topk_grouped.groupby('layer_ix'):\n",
    "    p_token = layer_df['token_frac'].values    \n",
    "    q = np.full_like(p_token, 1/len(p_token))\n",
    "    \n",
    "    token_kl = kl_divergence(p_token, q)\n",
    "    weight_kl = kl_divergence(layer_df['weight_frac'].values, q)\n",
    "    \n",
    "    kl_list.append({\n",
    "        'layer_ix': layer,\n",
    "        'token_kl': token_kl,\n",
    "        'weight_kl': weight_kl\n",
    "    })\n",
    "kl_df = pd.DataFrame(kl_list)\n",
    "\n",
    "px.line(\n",
    "    kl_df,\n",
    "    x = 'layer_ix', y = ['weight_kl', 'token_kl'],\n",
    "    title = 'KL Divergence from Uniform'\n",
    ").update_layout(autosize = False, width = 800, height = 400).show()\n",
    "\n",
    "px.line(\n",
    "    entropy_df,\n",
    "    x = 'layer_ix', y = ['weight_entropy', 'token_entropy'],\n",
    "    title = 'Shannon Entropy'\n",
    ").update_layout(autosize = False, width = 800, height = 400).show()\n",
    "\n",
    "px.line(\n",
    "    topk_grouped.pipe(lambda df: df[df['expert'].isin(list(range(0, 100)))]),\n",
    "    x = 'layer_ix',\n",
    "    y = 'token_count',\n",
    "    color = 'expert'\n",
    ").update_layout(autosize = False, width = 800, height = 400).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Cross-layer Topk = 1 Clusters\n",
    "\"\"\"\n",
    "def print_samples(df, grouping_cols):\n",
    "    \"\"\"\n",
    "    Takes a wide dataframe and groups it, then prints random groups\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        df\\\n",
    "        .groupby(grouping_cols, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .sample(35)\n",
    "    display(res)\n",
    "\n",
    "topk_wide =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    .merge(sample_df[['sample_ix', 'token']], on = 'sample_ix', how = 'inner')\\\n",
    "    .pivot(index = ['sample_ix', 'token'], columns = 'layer_ix', values = 'expert')\\\n",
    "    .rename(columns = lambda c: f'layer_{c}_id')\\\n",
    "    .reset_index()\n",
    "\n",
    "display(topk_wide.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "print_samples(topk_wide, ['layer_4_id', 'layer_5_id', 'layer_6_id', 'layer_7_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Within layer clusters\n",
    "\"\"\"\n",
    "# Pivot by layer and topk to get expert_l4_k1, etc.\n",
    "layer_topk_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'].isin([5, 7])])\\\n",
    "    .merge(sample_df[['sample_ix', 'token']], on = 'sample_ix', how = 'inner')\\\n",
    "    .assign(layer_topk_ix = lambda df: 'l' + df['layer_ix'].astype(str) + '_k' + df['topk_ix'].astype(str))\\\n",
    "    .pivot(index = ['sample_ix', 'token'], columns = ['layer_topk_ix'], values = 'expert')\\\n",
    "    .rename(columns = lambda c: f'expert_{c}')\\\n",
    "    .merge(sample_df[['sample_ix', 'token']], on = 'sample_ix', how = 'inner')\\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "print_samples(layer_topk_df, ['expert_l5_k1', 'expert_l5_k2', 'expert_l5_k3', 'expert_l5_k4'])\n",
    "print_samples(layer_topk_df, ['expert_l7_k1', 'expert_l7_k2', 'expert_l7_k3', 'expert_l7_k4'])\n",
    "print_samples(layer_topk_df, ['expert_l5_k1', 'expert_l5_k2', 'expert_l7_k1', 'expert_l7_k2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Base K-Means (note - returns imbalanced clusters)\n",
    "\"\"\" \n",
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 64):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        @layer_hs: A n_token_samples x D tensor for a single layer\n",
    "        @n_clusters: The number of clusters to return\n",
    "\n",
    "    Returns:\n",
    "        A list of length n_token_samples of cluster ids\n",
    "    \"\"\"\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float16))\n",
    "    kmeans_model = cuml.cluster.KMeans(\n",
    "        n_clusters = n_clusters,\n",
    "        max_iter = 1000,\n",
    "        random_state = 123,\n",
    "        verbose = True\n",
    "    )\n",
    "    kmeans_model.fit(hs_cupy)\n",
    "    cluster_labels = kmeans_model.labels_ # shape = (n_samples,)\n",
    "    # cluster_centers = kmeans_model.cluster_centers_ # shape = (num_clusters, D)\n",
    "    clear_all_cuda_memory()\n",
    "    return cluster_labels.tolist()\n",
    "\n",
    "kmeans_res = [\n",
    "    {'layer_ix': layer_ix, 'cluster_ids': cluster_kmeans(layer_hs, 64)}\n",
    "    for layer_ix, layer_hs in tqdm(enumerate(all_pre_mlp_hs.unbind(dim = 1)))\n",
    "]\n",
    "\n",
    "kmeans_df =\\\n",
    "    pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in kmeans_res], axis = 1)\\\n",
    "    .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "\n",
    "display(kmeans_df.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "display(kmeans_df.groupby('layer_3_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "clear_all_cuda_memory()\n",
    "\n",
    "print_samples(kmeans_df, ['layer_2_id', 'layer_3_id', 'layer_4_id', 'layer_5_id', 'layer_6_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Test decomp methods\n",
    "\"\"\"\n",
    "def reduce_pca(layer_hs: torch.Tensor, n_components = 2):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#principal-component-analysis\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float16))\n",
    "    model = cuml.PCA(\n",
    "        iterated_power = 20,\n",
    "        n_components = n_components,\n",
    "        verbose = True\n",
    "    )\n",
    "    model.fit(hs_cupy)\n",
    "    # print(f'Explained variance ratio: {model.explained_variance_ratio_}')\n",
    "    # print(f'Cumulative variance ratio: {np.cumsum(model.explained_variance_ratio_)[-1]}')\n",
    "    # print(f'Means by feature: {model.mean_}')\n",
    "    # print(f'Max feature mean: {np.max(model.mean_)} | Min feature mean: {np.min(model.mean_)}')\n",
    "    pred = cupy.asnumpy(model.transform(hs_cupy))\n",
    "    clear_all_cuda_memory()\n",
    "    return pred\n",
    "\n",
    "pca_test = reduce_pca(all_pre_mlp_hs.unbind(dim = 1)[0], 100)\n",
    "px.scatter(\n",
    "    pd.concat([pd.DataFrame({'d1': pca_test[:, 0], 'd2': pca_test[:, 1]}), sample_df.head(pca_test.shape[0])], axis = 1)\\\n",
    "        .sample(5000)\n",
    "        .assign(is_of = lambda df: np.where(df['token'] == ' of', 1, 0)),\n",
    "    x = 'd1', y = 'd2', color = 'is_of', hover_data = ['token']\n",
    ").show()\n",
    "\n",
    "pca_10 = [reduce_pca(layer_hs, 10) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]\n",
    "pca_100 = [reduce_pca(layer_hs, 100) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_umap(layer_hs: torch.Tensor, n_components = 2, metric = 'cosine'):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#umap\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float16))\n",
    "\n",
    "    model = cuml.UMAP(\n",
    "        n_components = n_components, \n",
    "        n_neighbors = 15, # 15 for default, smaller = more local data preserved [2 - 100]\n",
    "        metric = metric, # euclidean, cosine, manhattan, l2, hamming\n",
    "        min_dist = 0.1, # 0.1 by default, effective distance between embedded points\n",
    "        n_epochs = 200, # 200 by default for large datasets\n",
    "        random_state = None, # Allow parallelism\n",
    "        verbose = False\n",
    "    )\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory()\n",
    "    return pred\n",
    "\n",
    "umap_test = reduce_umap(all_pre_mlp_hs.unbind(dim = 1)[0], 2, 'cosine') # 300k = 2min\n",
    "px.scatter(\n",
    "    pd.concat([pd.DataFrame({'d1': umap_test[:, 0], 'd2': umap_test[:, 1]}), sample_df.head(umap_test.shape[0])], axis = 1)\\\n",
    "        .sample(5000)\n",
    "        .assign(is_of = lambda df: np.where(df['token'] == ' of', 1, 0)),\n",
    "    x = 'd1', y = 'd2', color = 'is_of', hover_data = ['token']\n",
    ").show()\n",
    "\n",
    "# umap_euc_10 = [reduce_umap(layer_hs, 10, 'euclidean') for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]\n",
    "# umap_euc_100 = [reduce_umap(layer_hs, 100, 'euclidean') for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]\n",
    "umap_cos_10 = [reduce_umap(layer_hs, 10, 'cosine') for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))] # Cosine most closely maps to router (dot product)\n",
    "umap_cos_100 = [reduce_umap(layer_hs, 100, 'cosine') for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Kmeans\n",
    "\"\"\"\n",
    "def cluster_kmeans(layer_hs_np: np.ndarray):\n",
    "    \"\"\"\n",
    "    Cluster a layer using Kmeans\n",
    "\n",
    "    Params:\n",
    "        @layer_hs_np: An np array of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension.\n",
    "    \"\"\"\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#kmeans\n",
    "    hs_cupy = cupy.asarray(layer_hs_np)\n",
    "\n",
    "    model = cuml.KMeans(\n",
    "        n_clusters = 100,\n",
    "        max_iter = 500\n",
    "    )\n",
    "    cluster_labels = model.fit_predict(hs_cupy).tolist()\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def test_kmeans(layer_hs_list, layers_to_group):\n",
    "    \"\"\"\n",
    "    Cluster multiple layers and print diagnostics, then print cross-layer groups.\n",
    "    \n",
    "    Params:\n",
    "        @layer_hs_list: A list of np arrays, each of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension\n",
    "        @layers_to_group: The indices of layer_hs_list (0-indexed) to be used for grouping clusters across layers.\n",
    "    \"\"\"\n",
    "    cl_res = [{'layer_ix': l, 'cluster_ids': cluster_kmeans(layer_hs_list[l])} for l in tqdm(layers_to_group)]\n",
    "    \n",
    "    cl_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cl_res], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df.head(layer_hs_list[0].shape[0])], axis = 1))\n",
    "\n",
    "    display(cl_df.groupby(f\"layer_{str(layers_to_group[0])}_id\", as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "    print('Cross-layer clusters:')\n",
    "    print_samples(cl_df, [f\"layer_{str(l)}_id\"  for l in layers_to_group])\n",
    "\n",
    "    return cl_df\n",
    "\n",
    "kmeans_path_1 = test_kmeans(umap_cos_100, [2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agglomerative\n",
    "\"\"\"\n",
    "def cluster_aggc(layer_hs_np: np.ndarray):\n",
    "    \"\"\"\n",
    "    Cluster a layer using Kmeans\n",
    "\n",
    "    Params:\n",
    "        @layer_hs_np: An np array of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension.\n",
    "    \"\"\"\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#dbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs_np)\n",
    "\n",
    "    model = cuml.AgglomerativeClustering(\n",
    "        n_clusters = 100,\n",
    "        metric = 'cosine'\n",
    "    )\n",
    "    cluster_labels = model.fit_predict(hs_cupy).tolist()\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def test_aggc(layer_hs_list, layers_to_group):\n",
    "    \"\"\"\n",
    "    Cluster multiple layers and print diagnostics, then print cross-layer groups.\n",
    "    \n",
    "    Params:\n",
    "        @layer_hs_list: A list of np arrays, each of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension\n",
    "        @layers_to_group: The indices of layer_hs_list (0-indexed) to be used for grouping clusters across layers.\n",
    "    \"\"\"\n",
    "    cl_res = [{'layer_ix': l, 'cluster_ids': cluster_aggc(layer_hs_list[l])} for l in tqdm(layers_to_group)]\n",
    "    \n",
    "    cl_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cl_res], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df.head(layer_hs_list[0].shape[0])], axis = 1))\n",
    "\n",
    "    display(cl_df.groupby(f\"layer_{str(layers_to_group[0])}_id\", as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "    print('Cross-layer clusters:')\n",
    "    print_samples(cl_df, [f\"layer_{str(l)}_id\"  for l in layers_to_group])\n",
    "\n",
    "    return cl_df\n",
    "\n",
    "aggc_path_1 = test_aggc(umap_cos_100, [2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DBScan\n",
    "\"\"\"\n",
    "def cluster_dbscan(layer_hs_np: np.ndarray, metric = 'euclidean'):\n",
    "    \"\"\"\n",
    "    Cluster a layer using DBScan\n",
    "\n",
    "    Params:\n",
    "        @layer_hs_np: An np array of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension.\n",
    "        @metric: The distance metric to use. Either \"euclidean\" or \"cosine\" are reasonable.\n",
    "    \"\"\"\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#dbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs_np)\n",
    "\n",
    "    model = cuml.DBSCAN(\n",
    "        metric = metric, # Or cosine\n",
    "        min_samples = 10, # Number of samples st the group can be considered a core point\n",
    "        verbose = False\n",
    "    )\n",
    "    cluster_labels = model.fit_predict(hs_cupy).tolist()\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def test_dbscan(layer_hs_list, metric, layers_to_group):\n",
    "    \"\"\"\n",
    "    Cluster multiple layers and print diagnostics, then print cross-layer groups.\n",
    "    \n",
    "    Params:\n",
    "        @layer_hs_list: A list of np arrays, each of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension\n",
    "        @metric: The distance metric to use. Either \"euclidean\" or \"cosine\" are reasonable.\n",
    "        @layers_to_group: The indices of layer_hs_list (0-indexed) to be used for grouping clusters across layers.\n",
    "    \"\"\"\n",
    "    cl_res = [{'layer_ix': l, 'cluster_ids': cluster_dbscan(layer_hs_list[l] , metric)} for l in tqdm(layers_to_group)]\n",
    "    \n",
    "    for r in cl_res:\n",
    "        print(f\"Clusters {len(set(r['cluster_ids'])):,} | Unassigned to clusters: {len([x for x in r['cluster_ids'] if x == -1]):,}/{len(r['cluster_ids']):,}\")\n",
    "\n",
    "    cl_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cl_res], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df.head(layer_hs_list[0].shape[0])], axis = 1))\n",
    "\n",
    "    display(cl_df.groupby(f\"layer_{str(layers_to_group[0])}_id\", as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "    print('Cross-layer clusters:')\n",
    "    print_samples(cl_df, [f\"layer_{str(l)}_id\"  for l in layers_to_group])\n",
    "\n",
    "    return cl_df\n",
    "\n",
    "dbscan_paths_1 = test_dbscan(umap_cos_10, 'euclidean', [2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HDBSCAN\n",
    "\"\"\"\n",
    "\n",
    "def cluster_hdbscan(layer_hs_np: np.ndarray, metric = 'euclidean'):\n",
    "    \"\"\"\n",
    "    Cluster a layer using HDBScan\n",
    "\n",
    "    Params:\n",
    "        @layer_hs_np: An np array of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension.\n",
    "        @metric: The distance metric to use. Either \"euclidean\" or \"cosine\" are reasonable.\n",
    "    \"\"\"\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#dbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs_np)\n",
    "\n",
    "    model = cuml.HDBSCAN(\n",
    "        min_cluster_size = len(hs_cupy) // (64 * 50), # Min 1/20 of the uniform dist value\n",
    "        max_cluster_size = len(hs_cupy) // (64 * 1/50), # Max 20x the uniform dist values \n",
    "        metric = metric,\n",
    "        min_samples = 1,\n",
    "    )\n",
    "    cluster_labels = model.fit_predict(hs_cupy).tolist()\n",
    "    return cluster_labels\n",
    "\n",
    "def test_hdbscan(layer_hs_list, metric, layers_to_group):\n",
    "    \"\"\"\n",
    "    Cluster multiple layers and print diagnostics, then print cross-layer groups.\n",
    "    \n",
    "    Params:\n",
    "        @layer_hs_list: A list of np arrays, each of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension\n",
    "        @metric: The distance metric to use. Either \"euclidean\" or \"cosine\" are reasonable.\n",
    "        @layers_to_group: The indices of layer_hs_list (0-indexed) to be used for grouping clusters across layers.\n",
    "    \"\"\"\n",
    "    cl_res = [{'layer_ix': l, 'cluster_ids': cluster_hdbscan(layer_hs_list[l] , metric)} for l in tqdm(layers_to_group)]\n",
    "    \n",
    "    for r in cl_res:\n",
    "        print(f\"Clusters {len(set(r['cluster_ids'])):,} | Unassigned to clusters: {len([x for x in r['cluster_ids'] if x == -1]):,}/{len(r['cluster_ids']):,}\")\n",
    "\n",
    "    cl_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cl_res], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df.head(layer_hs_list[0].shape[0])], axis = 1))\n",
    "\n",
    "    display(cl_df.groupby(f\"layer_{str(layers_to_group[0])}_id\", as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "    print('Cross-layer clusters:')\n",
    "    print_samples(cl_df, [f\"layer_{str(l)}_id\"  for l in layers_to_group])\n",
    "\n",
    "    return cl_df\n",
    "\n",
    "hdbscan_paths_1 = test_hdbscan(umap_cos_10, 'euclidean', [2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The goal now is to split things by SVD\n",
    "\"\"\"\n",
    "\n",
    "def decompose_orthogonal(hidden_states: torch.Tensor, router_weights: torch.Tensor, method: str = 'svd', svd_tol: float = 1e-6) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Decomposes hidden states into components parallel and orthogonal to the row space of the router weights.\n",
    "\n",
    "    The component parallel to the row space ('h_row') contains the information seen by the linear router mechanism (logits = W_g @ h).\n",
    "\n",
    "    The component orthogonal to the row space ('h_orth') contains information ignored by the linear router mechanism, but potentially used by the non-linear expert MLP or downstream layers.\n",
    "\n",
    "    Params:\n",
    "        @hidden_states: Tensor of shape (n_samples, D) representing the pre-routing hidden states.\n",
    "        @router_weights: Tensor of shape (n_experts, D) representing the linear router gate weights for the layer.\n",
    "        @method: Decomposition method, 'svd' (default) or 'qr'.\n",
    "        @svd_tol: Tolerance for determining non-zero singular values in SVD to establish the matrix rank.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - h_row (torch.Tensor): Projection onto the row space (\"used\" by router). Shape (n_samples, D).\n",
    "        - h_orth (torch.Tensor): Projection onto the orthogonal complement (\"unused\" by router). Shape (n_samples, D).\n",
    "\n",
    "    Example:\n",
    "        h_row, h_orth = decompose_orthogonal(all_pre_mlp_hs[0:10_000, 1, :].to(torch.float32), model.model.layers[1].mlp.gate.weight.to(torch.float32).detach().cpu(), 'svd')\n",
    "        dot_products_svd = torch.sum(h_row * h_orth, dim=1)\n",
    "        print(f\"Mean dot product (SVD): {torch.mean(dot_products_svd).item():.4e}\")\n",
    "        print(f\"Max absolute dot product (SVD): {torch.max(torch.abs(dot_products_svd)).item():.4e}\")\n",
    "\n",
    "        reconstruction_diff_svd = torch.norm(all_pre_mlp_hs[0:10_000, 1, :].to(torch.float32) - (h_row + h_orth), dim=1)\n",
    "        print(f\"Mean reconstruction norm diff (SVD): {torch.mean(reconstruction_diff_svd).item():.4e}\")\n",
    "\n",
    "        # Can also verify that QR orthogonality/reconstruction is close to 0, and also that SVD and QR results shoudl be close torch.norm(h_svd = h_qr)\n",
    "    \"\"\"\n",
    "    _, D = hidden_states.shape\n",
    "\n",
    "    assert D == router_weights.shape[1], 'Hidden state dim != router gate dim'\n",
    "\n",
    "    if method == 'svd':\n",
    "        # Compute SVD: W_g = U S V^T\n",
    "        # V^T (Vt) has shape (k, D), where k = min(n_experts, D)\n",
    "        # The rows of V^T are the right singular vectors (orthonormal)\n",
    "        # The first 'rank' rows of V^T span the row space of W_g\n",
    "        U, S, Vt = torch.linalg.svd(router_weights, full_matrices = False) # Use full_matrices = False for efficiency if D > n_experts\n",
    "\n",
    "        # Determine rank based on tolerance\n",
    "        rank = torch.sum(S > svd_tol)\n",
    "        if rank == 0:\n",
    "             raise Exception('Router weights matrix has rank 0 according to tolerance.')\n",
    "\n",
    "        # Basis for the row space (columns of Vr)\n",
    "        # Vt[:rank] selects the first 'rank' rows (shape rank x D)\n",
    "        # .T makes it (D x rank) - columns are the orthonormal basis vectors\n",
    "        Vr = Vt[:rank, :].T\n",
    "\n",
    "        # Project hidden_states onto the row space (Vr)\n",
    "        # Formula: h_row = Vr @ Vr^T @ h\n",
    "        # Batched: H_row = (H @ Vr) @ Vr^T\n",
    "        # (n_samples, D) @ (D, rank) -> (n_samples, rank)\n",
    "        h_projected_coeffs = hidden_states @ Vr\n",
    "        # (n_samples, rank) @ (rank, D) -> (n_samples, D)\n",
    "        h_row = h_projected_coeffs @ Vr.T\n",
    "\n",
    "    elif method == 'qr':\n",
    "        # Compute QR decomposition of W_g^T: W_g^T = Q R\n",
    "        # Q will have shape (D, k), where k = min(D, n_experts)\n",
    "        # Columns of Q form an orthonormal basis for column space of W_g^T, which is the row space of W_g.\n",
    "        Q, R = torch.linalg.qr(router_weights.T, mode = 'reduced') # Use 'reduced' mode for efficiency\n",
    "\n",
    "        # Q's columns are the orthonormal basis (shape D x k)\n",
    "        # Need to consider rank deficiency if applicable, but QR handles it implicitly by the shape of Q returned by 'reduced' mode.\n",
    "\n",
    "        # Project hidden_states onto the column space of Q\n",
    "        # Formula: h_row = Q @ Q^T @ h\n",
    "        # Batched: H_row = (H @ Q) @ Q^T\n",
    "        # (n_samples, D) @ (D, k) -> (n_samples, k)\n",
    "        h_projected_coeffs = hidden_states @ Q\n",
    "        # (n_samples, k) @ (k, D) -> (n_samples, D)\n",
    "        h_row = h_projected_coeffs @ Q.T\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Method must be svd or qr')\n",
    "\n",
    "    # The orthogonal component is the residual\n",
    "    h_orth = hidden_states - h_row\n",
    "\n",
    "    return h_row, h_orth\n",
    "\n",
    "test_layers = list(range(0, 6))\n",
    "\n",
    "res = [\n",
    "    decompose_orthogonal(all_pre_mlp_hs[:, layer_ix, :].to(torch.float32), model.model.layers[layer_ix].mlp.gate.weight.to(torch.float32).detach().cpu(), 'svd')\n",
    "    for layer_ix in tqdm(test_layers)\n",
    "]\n",
    "\n",
    "h_row_by_layer = [x[0] for x in res]\n",
    "h_orth_by_layer = [x[1] for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_samples(df, grouping_cols):\n",
    "    \"\"\"\n",
    "    Takes a wide dataframe and groups it, then prints random groups\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        df\\\n",
    "        .groupby(grouping_cols, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .sample(35)\n",
    "    display(res)\n",
    "\n",
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 64):\n",
    "    kmeans_model = cuml.cluster.KMeans(n_clusters = n_clusters, max_iter = 1000, random_state = 123)\n",
    "    kmeans_model.fit(cupy.asarray(layer_hs.to(torch.float32)))\n",
    "    clear_all_cuda_memory()\n",
    "    return kmeans_model.labels_.tolist()\n",
    "\n",
    "par_kmeans_res = [{'layer_ix': layer_ix, 'cluster_ids': cluster_kmeans(layer_hs, 64)} for layer_ix, layer_hs in tqdm(enumerate(h_row_by_layer))]\n",
    "\n",
    "par_kmeans_df =\\\n",
    "    pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in par_kmeans_res], axis = 1)\\\n",
    "    .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "\n",
    "display(par_kmeans_df.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "\n",
    "print_samples(par_kmeans_df, ['layer_1_id', 'layer_2_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orth_kmeans_res = [{'layer_ix': layer_ix, 'cluster_ids': cluster_kmeans(layer_hs, 64)} for layer_ix, layer_hs in tqdm(enumerate(h_orth_by_layer))]\n",
    "\n",
    "orth_kmeans_df =\\\n",
    "    pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in orth_kmeans_res], axis = 1)\\\n",
    "    .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "\n",
    "display(orth_kmeans_df.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "\n",
    "print_samples(orth_kmeans_df, ['layer_1_id', 'layer_2_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orth_kmeans_df\\\n",
    "    .groupby(['layer_1_id'], as_index = False)\\\n",
    "    .agg(\n",
    "        n_samples = ('token', 'size'),\n",
    "        samples = ('token', lambda s: s.sample(n = min(len(s), 20)).tolist())\n",
    "    )\\\n",
    "    .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "    .assign(is_eq = lambda df: df.samples.apply(lambda s: 1 if len(set(s)) == 1 else 0))\\\n",
    "    .groupby('is_eq', as_index = False)\\\n",
    "    .agg(count = ('is_eq', 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_kmeans_df\\\n",
    "    .groupby(['layer_1_id'], as_index = False)\\\n",
    "    .agg(\n",
    "        n_samples = ('token', 'size'),\n",
    "        samples = ('token', lambda s: s.sample(n = min(len(s), 20)).tolist())\n",
    "    )\\\n",
    "    .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "    .assign(is_eq = lambda df: df.samples.apply(lambda s: 1 if len(set(s)) == 1 else 0))\\\n",
    "    .groupby('is_eq', as_index = False)\\\n",
    "    .agg(count = ('is_eq', 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_samples(par_kmeans_df, ['layer_3_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_kmeans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_samples(orth_kmeans_df, ['layer_3_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_kmeans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic regression - predict topk using h_orth?\n",
    "\"\"\"\n",
    "# Test layer \n",
    "test_layer = 5\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "\n",
    "lr_model = cuml.linear_model.LogisticRegression(\n",
    "    penalty = 'l2', \n",
    "    max_iter = 10000,\n",
    "    fit_intercept = False\n",
    ")\n",
    "layer_hs = cupy.asarray(h_row_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "lr_model.fit(layer_hs, expert_ids_cp)\n",
    "accuracy = lr_model.score(layer_hs, expert_ids_cp)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "layer_hs = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "lr_model.fit(layer_hs, expert_ids_cp)\n",
    "accuracy = lr_model.score(layer_hs, expert_ids_cp)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.groupby('token').agg(n = ('token', 'count')).sort_values(by = 'n', ascending = False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layer = 5\n",
    "\n",
    "def run_lr(x_cp, y_cp):\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.1, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 10000, fit_intercept = False)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "y_df =\\\n",
    "    sample_df\\\n",
    "    .assign(is_sample = lambda df: np.where(df['token'].isin(['.', '_', ',', ':']), 1, 0))\\\n",
    "    ['is_sample'].tolist()\n",
    "\n",
    "y_cp = cupy.asarray(y_df)\n",
    "x_cp_para = cupy.asarray(h_row_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, y_cp)\n",
    "run_lr(x_cp_orth, y_cp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
