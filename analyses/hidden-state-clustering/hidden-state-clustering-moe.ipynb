{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to test MoE hidden states & understand routing weights.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "\n",
    "# https://docs.rapids.ai/install/\n",
    "import cupy\n",
    "import cuml\n",
    "\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_id = 'Qwen/Qwen1.5-MoE-A2.7B-Chat'\n",
    "model_prefix = 'qwen1.5moe'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix):\n",
    "    all_pre_mlp_hs = torch.load(f'data/{model_prefix}-all-pre-mlp-hidden-states.pt')\n",
    "    with open(f'data/{model_prefix}-metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    return all_pre_mlp_hs, metadata['sample_df'], metadata['topk_df']\n",
    "\n",
    "all_pre_mlp_hs, sample_df_import, topk_df_import = load_data(model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze routing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Norms by expert and layer\n",
    "\"\"\"\n",
    "norms_by_expert_layer = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        'layer_ix': layer_ix,\n",
    "        'norm': torch.linalg.norm(layer.mlp.gate.weight, dim = 1, ord = 1).to(torch.float16).cpu().detach().numpy(),\n",
    "        'expert': list(range(1, layer.mlp.gate.weight.shape[0] + 1))\n",
    "    })\n",
    "    for layer_ix, layer in enumerate(model.model.layers)\n",
    "])\n",
    "\n",
    "plot_df = norms_by_expert_layer.pivot(index = 'layer_ix', columns = 'expert', values = 'norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Norm by Expert and Layer\"\n",
    ").update_layout(autosize = False, width = 800).show()\n",
    "\n",
    "scaled_df =\\\n",
    "    norms_by_expert_layer\\\n",
    "    .assign(layer_mean = lambda df: df.groupby('layer_ix')['norm'].transform('mean'))\\\n",
    "    .assign(norm_scaled = lambda df: df['norm'] / df['layer_mean'] - 1)\n",
    "\n",
    "scaled_plot_df = scaled_df.pivot(index = 'layer_ix', columns = 'expert', values = 'norm_scaled')\n",
    "px.imshow(\n",
    "    scaled_plot_df,\n",
    "    x = scaled_plot_df.columns, y = scaled_plot_df.index,\n",
    "    color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Norm by Expert and Layer\"\n",
    ").update_layout(autosize = False, width = 800).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For a single layer, what do the weights and RMSnorms look like?\n",
    "\"\"\"\n",
    "plot_layer_ix = 9\n",
    "show_dims = list(range(0, 400))\n",
    "\n",
    "# RMSNorm\n",
    "rms_tensor = model.model.layers[plot_layer_ix].post_attention_layernorm.weight\n",
    "rms_df = pd.DataFrame({\n",
    "    'gamma': rms_tensor.to(torch.float32).cpu().detach().numpy(),\n",
    "    'coef': 1,\n",
    "    'dimension': list(range(0, rms_tensor.shape[0]))\n",
    "})\n",
    "plot_df = rms_df.pipe(lambda df: df[df['dimension'].isin(show_dims)]).pivot(index = 'coef', columns = 'dimension', values = 'gamma')\n",
    "\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"RMSNorm Scaling Values\"\n",
    ").update_layout(autosize = False, width = 1400, height = 400).show()\n",
    "\n",
    "\n",
    "# Weights\n",
    "wt_tensor = model.model.layers[plot_layer_ix].mlp.gate.weight\n",
    "wt_df = pd.DataFrame({\n",
    "    'value': wt_tensor.view(-1).to(torch.float32).cpu().detach().numpy(),\n",
    "    'expert': [i // wt_tensor.shape[1] for i in range(wt_tensor.view(-1).shape[0])],\n",
    "    'dimension': [i % wt_tensor.shape[1] for i in range(wt_tensor.view(-1).shape[0])]\n",
    "})\n",
    "\n",
    "plot_df = wt_df.pipe(lambda df: df[df['dimension'].isin(show_dims)]).pivot(index = 'expert', columns = 'dimension', values = 'value')\n",
    "\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Routing Weights\"\n",
    ").update_layout(autosize = False, width = 1400).show()\n",
    "\n",
    "# Scale weights by RMSNorm\n",
    "scaled_df = wt_df.merge(rms_df, on = 'dimension', how = 'inner').assign(gamma_scaled_value = lambda df: df['gamma'] * df['value'])\n",
    "plot_df = scaled_df.pipe(lambda df: df[df['dimension'].isin(show_dims)]).pivot(index = 'expert', columns = 'dimension', values = 'gamma_scaled_value')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Scaled Routing Weights\"\n",
    ").update_layout(autosize = False, width = 1400).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mean norms across layers and dimension (averaged across experts)\n",
    "\"\"\"\n",
    "dfs_list = []\n",
    "for layer_ix, layer in enumerate(model.model.layers):\n",
    "    wt_tensor = layer.mlp.gate.weight.to(torch.float32).cpu().detach()\n",
    "    rms_tensor = layer.post_attention_layernorm.weight.to(torch.float32).cpu().detach()\n",
    "    scaled = (wt_tensor * rms_tensor) # Mltiply by RMS norm\n",
    "    scaled = scaled.abs().mean(dim = 0) # Take mean L1 norm\n",
    "    dfs_list.append(pd.DataFrame({\n",
    "        'mean_norm': scaled.numpy(),\n",
    "        'layer_ix': layer_ix,\n",
    "        'dim': list(range(1, scaled.shape[0] + 1))\n",
    "    }))\n",
    "\n",
    "my_df = pd.concat(dfs_list)\n",
    "# Additionally scale by layer average\n",
    "my_df_ex_scale =\\\n",
    "    my_df\\\n",
    "    .assign(layer_mean = lambda df: df.groupby('layer_ix')['mean_norm'].transform('mean'))\\\n",
    "    .assign(mean_norm = lambda df: df['mean_norm'] / df['layer_mean'])\n",
    "\n",
    "plot_df = my_df_ex_scale.pipe(lambda df: df[df['dim']  <= 200]).pivot(index = 'layer_ix', columns = 'dim', values = 'mean_norm')\n",
    "\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    zmin = 0, zmax = 8,\n",
    "    aspect = 'auto', # Allow non-square boxes\n",
    "    color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "At dimension x layer-level, analyze activations (averaged across samples) versus routing weights (averaged across experts).\n",
    "\"\"\"\n",
    "show_dims = list(range(0, 800))\n",
    "\n",
    "pre_mlp_for_layer_norms_all = all_pre_mlp_hs[:, :, :].abs().mean(dim = 0) # Collapse to n_layers x D dimensional\n",
    "\n",
    "dfs_list = []\n",
    "for layer_ix, pre_mlp_for_layer_norm in enumerate(pre_mlp_for_layer_norms_all.unbind(dim = 0)):\n",
    "    wt_tensor = model.model.layers[layer_ix].mlp.gate.weight.to(torch.float32).cpu().detach() # (n_experts, D)\n",
    "    act_tensor = pre_mlp_for_layer_norm.to(torch.float32).cpu().detach() # D-dimensional\n",
    "    scaled = (wt_tensor * act_tensor) # Multiply by activation tensor\n",
    "    scaled = scaled.abs().mean(dim = 0) # Take mean L1 norm\n",
    "    dfs_list.append(pd.DataFrame({\n",
    "        'act_norm': act_tensor.numpy(),\n",
    "        'wt_norm': wt_tensor.abs().mean(dim = 0),\n",
    "        'mean_scaled_norm': scaled.numpy(),\n",
    "        'layer_ix': layer_ix,\n",
    "        'dim': list(range(1, scaled.shape[0] + 1))\n",
    "    }))\n",
    "\n",
    "pre_mlp_df = pd.concat(dfs_list)\n",
    "\n",
    "plot_df = pre_mlp_df.pipe(lambda df: df[df['dim'].isin(show_dims)]).pivot(index = 'layer_ix', columns = 'dim', values = 'mean_scaled_norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    zmin = 0,\n",
    "    zmax = .2,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean scaled wt * activation norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "plot_df = pre_mlp_df.pipe(lambda df: df[df['dim'].isin(show_dims)]).pivot(index = 'layer_ix', columns = 'dim', values = 'act_norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    zmin = 0, zmax = 8,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean activation norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "plot_df = pre_mlp_df.pipe(lambda df: df[df['dim'].isin(show_dims)]).pivot(index = 'layer_ix', columns = 'dim', values = 'wt_norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean weight norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "# Plot activation vs routing weight norms\n",
    "px.scatter(\n",
    "    pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == 6]),\n",
    "    x = 'wt_norm', y = 'act_norm', color = 'wt_norm',\n",
    "    log_y = True,\n",
    "    log_x = True,\n",
    "    color_continuous_scale = 'viridis', title = 'Per-Dimension Plot - Activation L1 Norm versus Routing Weight L1 Norm'\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "import scipy\n",
    "scipy.stats.kurtosis(pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == 6]['act_norm'].tolist() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linear regression - test ability to reconstruct topk expert id\n",
    "\"\"\"\n",
    "layer_to_test = 5\n",
    "\n",
    "lr_model = cuml.linear_model.LogisticRegression(\n",
    "    penalty = 'l2', \n",
    "    max_iter = 10000,\n",
    "    fit_intercept = False\n",
    "    )\n",
    "\n",
    "dims = [\n",
    "    x - 1 \n",
    "    for x in pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == layer_to_test]).sort_values(by = 'mean_scaled_norm', ascending = False)['dim'].tolist()\n",
    "]\n",
    "\n",
    "layer_hs = cupy.asarray(all_pre_mlp_hs[:, layer_to_test, dims[0:2048//50]].to(torch.float32).detach().cpu())\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == layer_to_test])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "\n",
    "lr_model.fit(layer_hs, expert_ids_cp)\n",
    "accuracy = lr_model.score(layer_hs, expert_ids_cp)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compare PCA top dimensinos versus scaled activation top dimensions\n",
    "\"\"\"\n",
    "layer_hs = cupy.asarray(all_pre_mlp_hs[0:200_000, layer_to_test, :].to(torch.float32).detach().cpu())\n",
    "mean_vals = cupy.mean(layer_hs, axis=0)\n",
    "std_vals = cupy.std(layer_hs, axis=0)\n",
    "std_vals = cupy.where(std_vals == 0, cupy.asarray(1e-7), std_vals)\n",
    "layer_hs_std = (layer_hs - mean_vals)/std_vals\n",
    "\n",
    "pca = cuml.decomposition.PCA(n_components = 10, random_state = 123)\n",
    "pca.fit(layer_hs_std)\n",
    "\n",
    "pc_loadings = pca.components_\n",
    "sumsq = (pc_loadings ** 2).sum(axis=0)\n",
    "\n",
    "ranking = cupy.argsort(-sumsq)  # descending order\n",
    "pca_top_dims = ranking.tolist()\n",
    "\n",
    "plot_df =\\\n",
    "    pd.DataFrame({'pca_sumsq': cupy.asarray(sumsq).tolist(), 'dim': list(range(1, len(sumsq) + 1))})\\\n",
    "    .merge(\n",
    "        pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == layer_to_test])[['dim', 'mean_scaled_norm']],\n",
    "        on = 'dim',\n",
    "        how = 'inner'\n",
    "    )\n",
    "\n",
    "px.scatter(\n",
    "    plot_df,\n",
    "    x = 'mean_scaled_norm',\n",
    "    y = 'pca_sumsq',\n",
    "    log_y = True,\n",
    "    log_x = True\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Cross-layer Topk = 1 Clusters\n",
    "\"\"\"\n",
    "def print_samples(df, grouping_cols):\n",
    "    \"\"\"\n",
    "    Takes a wide dataframe and groups it, then prints random groups\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        df\\\n",
    "        .groupby(grouping_cols, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .sample(35)\n",
    "    display(res)\n",
    "\n",
    "topk_wide =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    .merge(sample_df[['sample_ix', 'token']], on = 'sample_ix', how = 'inner')\\\n",
    "    .pivot(index = ['sample_ix', 'token'], columns = 'layer_ix', values = 'expert')\\\n",
    "    .rename(columns = lambda c: f'layer_{c}_id')\\\n",
    "    .reset_index()\n",
    "\n",
    "display(topk_wide.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "print_samples(topk_wide, ['layer_4_id', 'layer_5_id', 'layer_6_id', 'layer_7_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Within layer clusters\n",
    "\"\"\"\n",
    "# Pivot by layer and topk to get expert_l4_k1, etc.\n",
    "layer_topk_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'].isin([5, 7])])\\\n",
    "    .merge(sample_df[['sample_ix', 'token']], on = 'sample_ix', how = 'inner')\\\n",
    "    .assign(layer_topk_ix = lambda df: 'l' + df['layer_ix'].astype(str) + '_k' + df['topk_ix'].astype(str))\\\n",
    "    .pivot(index = ['sample_ix', 'token'], columns = ['layer_topk_ix'], values = 'expert')\\\n",
    "    .rename(columns = lambda c: f'expert_{c}')\\\n",
    "    .merge(sample_df[['sample_ix', 'token']], on = 'sample_ix', how = 'inner')\\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "print_samples(layer_topk_df, ['expert_l5_k1', 'expert_l5_k2', 'expert_l5_k3', 'expert_l5_k4'])\n",
    "print_samples(layer_topk_df, ['expert_l7_k1', 'expert_l7_k2', 'expert_l7_k3', 'expert_l7_k4'])\n",
    "print_samples(layer_topk_df, ['expert_l5_k1', 'expert_l5_k2', 'expert_l7_k1', 'expert_l7_k2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Base K-Means (note - returns imbalanced clusters)\n",
    "\"\"\" \n",
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 64):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        @layer_hs: A n_token_samples x D tensor for a single layer\n",
    "        @n_clusters: The number of clusters to return\n",
    "\n",
    "    Returns:\n",
    "        A list of length n_token_samples of cluster ids\n",
    "    \"\"\"\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float16))\n",
    "    kmeans_model = cuml.cluster.KMeans(\n",
    "        n_clusters = n_clusters,\n",
    "        max_iter = 1000,\n",
    "        random_state = 123,\n",
    "        verbose = True\n",
    "    )\n",
    "    kmeans_model.fit(hs_cupy)\n",
    "    cluster_labels = kmeans_model.labels_ # shape = (n_samples,)\n",
    "    # cluster_centers = kmeans_model.cluster_centers_ # shape = (num_clusters, D)\n",
    "    clear_all_cuda_memory()\n",
    "    return cluster_labels.tolist()\n",
    "\n",
    "kmeans_res = [\n",
    "    {'layer_ix': layer_ix, 'cluster_ids': cluster_kmeans(layer_hs, 64)}\n",
    "    for layer_ix, layer_hs in tqdm(enumerate(all_pre_mlp_hs.unbind(dim = 1)))\n",
    "]\n",
    "\n",
    "kmeans_df =\\\n",
    "    pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in kmeans_res], axis = 1)\\\n",
    "    .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "\n",
    "display(kmeans_df.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "display(kmeans_df.groupby('layer_3_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "clear_all_cuda_memory()\n",
    "\n",
    "print_samples(kmeans_df, ['layer_2_id', 'layer_3_id', 'layer_4_id', 'layer_5_id', 'layer_6_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Test decomp methods\n",
    "\"\"\"\n",
    "def reduce_pca(layer_hs: torch.Tensor, n_components = 2):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#principal-component-analysis\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "    model = cuml.PCA(\n",
    "        iterated_power = 20,\n",
    "        n_components = n_components,\n",
    "        verbose = True\n",
    "    )\n",
    "    model.fit(hs_cupy)\n",
    "    # print(f'Explained variance ratio: {model.explained_variance_ratio_}')\n",
    "    # print(f'Cumulative variance ratio: {np.cumsum(model.explained_variance_ratio_)[-1]}')\n",
    "    # print(f'Means by feature: {model.mean_}')\n",
    "    # print(f'Max feature mean: {np.max(model.mean_)} | Min feature mean: {np.min(model.mean_)}')\n",
    "    pred = cupy.asnumpy(model.transform(hs_cupy))\n",
    "    clear_all_cuda_memory()\n",
    "    return pred\n",
    "\n",
    "pca_test = reduce_pca(all_pre_mlp_hs.unbind(dim = 1)[0], 100)\n",
    "px.scatter(\n",
    "    pd.concat([pd.DataFrame({'d1': pca_test[:, 0], 'd2': pca_test[:, 1]}), sample_df.head(pca_test.shape[0])], axis = 1)\\\n",
    "        .sample(5000)\n",
    "        .assign(is_of = lambda df: np.where(df['token'] == ' of', 1, 0)),\n",
    "    x = 'd1', y = 'd2', color = 'is_of', hover_data = ['token']\n",
    ").show()\n",
    "\n",
    "pca_10 = [reduce_pca(layer_hs, 10) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]\n",
    "pca_100 = [reduce_pca(layer_hs, 100) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_umap(layer_hs: torch.Tensor, n_components = 2, metric = 'cosine'):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#umap\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float16))\n",
    "\n",
    "    model = cuml.UMAP(\n",
    "        n_components = n_components, \n",
    "        n_neighbors = 15, # 15 for default, smaller = more local data preserved [2 - 100]\n",
    "        metric = metric, # euclidean, cosine, manhattan, l2, hamming\n",
    "        min_dist = 0.1, # 0.1 by default, effective distance between embedded points\n",
    "        n_epochs = 200, # 200 by default for large datasets\n",
    "        random_state = None, # Allow parallelism\n",
    "        verbose = False\n",
    "    )\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory()\n",
    "    return pred\n",
    "\n",
    "umap_test = reduce_umap(all_pre_mlp_hs.unbind(dim = 1)[0], 2, 'cosine') # 300k = 2min\n",
    "px.scatter(\n",
    "    pd.concat([pd.DataFrame({'d1': umap_test[:, 0], 'd2': umap_test[:, 1]}), sample_df.head(umap_test.shape[0])], axis = 1)\\\n",
    "        .sample(5000)\n",
    "        .assign(is_of = lambda df: np.where(df['token'] == ' of', 1, 0)),\n",
    "    x = 'd1', y = 'd2', color = 'is_of', hover_data = ['token']\n",
    ").show()\n",
    "\n",
    "# umap_euc_10 = [reduce_umap(layer_hs, 10, 'euclidean') for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]\n",
    "# umap_euc_100 = [reduce_umap(layer_hs, 100, 'euclidean') for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]\n",
    "umap_cos_10 = [reduce_umap(layer_hs, 10, 'cosine') for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))] # Cosine most closely maps to router (dot product)\n",
    "umap_cos_100 = [reduce_umap(layer_hs, 100, 'cosine') for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Kmeans\n",
    "\"\"\"\n",
    "def cluster_kmeans(layer_hs_np: np.ndarray):\n",
    "    \"\"\"\n",
    "    Cluster a layer using Kmeans\n",
    "\n",
    "    Params:\n",
    "        @layer_hs_np: An np array of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension.\n",
    "    \"\"\"\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#kmeans\n",
    "    hs_cupy = cupy.asarray(layer_hs_np)\n",
    "\n",
    "    model = cuml.KMeans(\n",
    "        n_clusters = 100,\n",
    "        max_iter = 500\n",
    "    )\n",
    "    cluster_labels = model.fit_predict(hs_cupy).tolist()\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def test_kmeans(layer_hs_list, layers_to_group):\n",
    "    \"\"\"\n",
    "    Cluster multiple layers and print diagnostics, then print cross-layer groups.\n",
    "    \n",
    "    Params:\n",
    "        @layer_hs_list: A list of np arrays, each of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension\n",
    "        @layers_to_group: The indices of layer_hs_list (0-indexed) to be used for grouping clusters across layers.\n",
    "    \"\"\"\n",
    "    cl_res = [{'layer_ix': l, 'cluster_ids': cluster_kmeans(layer_hs_list[l])} for l in tqdm(layers_to_group)]\n",
    "    \n",
    "    cl_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cl_res], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df.head(layer_hs_list[0].shape[0])], axis = 1))\n",
    "\n",
    "    display(cl_df.groupby(f\"layer_{str(layers_to_group[0])}_id\", as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "    print('Cross-layer clusters:')\n",
    "    print_samples(cl_df, [f\"layer_{str(l)}_id\"  for l in layers_to_group])\n",
    "\n",
    "    return cl_df\n",
    "\n",
    "kmeans_path_1 = test_kmeans(umap_cos_100, [2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agglomerative\n",
    "\"\"\"\n",
    "def cluster_aggc(layer_hs_np: np.ndarray):\n",
    "    \"\"\"\n",
    "    Cluster a layer using Kmeans\n",
    "\n",
    "    Params:\n",
    "        @layer_hs_np: An np array of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension.\n",
    "    \"\"\"\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#dbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs_np)\n",
    "\n",
    "    model = cuml.AgglomerativeClustering(\n",
    "        n_clusters = 100,\n",
    "        metric = 'cosine'\n",
    "    )\n",
    "    cluster_labels = model.fit_predict(hs_cupy).tolist()\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def test_aggc(layer_hs_list, layers_to_group):\n",
    "    \"\"\"\n",
    "    Cluster multiple layers and print diagnostics, then print cross-layer groups.\n",
    "    \n",
    "    Params:\n",
    "        @layer_hs_list: A list of np arrays, each of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension\n",
    "        @layers_to_group: The indices of layer_hs_list (0-indexed) to be used for grouping clusters across layers.\n",
    "    \"\"\"\n",
    "    cl_res = [{'layer_ix': l, 'cluster_ids': cluster_aggc(layer_hs_list[l])} for l in tqdm(layers_to_group)]\n",
    "    \n",
    "    cl_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cl_res], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df.head(layer_hs_list[0].shape[0])], axis = 1))\n",
    "\n",
    "    display(cl_df.groupby(f\"layer_{str(layers_to_group[0])}_id\", as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "    print('Cross-layer clusters:')\n",
    "    print_samples(cl_df, [f\"layer_{str(l)}_id\"  for l in layers_to_group])\n",
    "\n",
    "    return cl_df\n",
    "\n",
    "aggc_path_1 = test_aggc(umap_cos_100, [2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DBScan\n",
    "\"\"\"\n",
    "def cluster_dbscan(layer_hs_np: np.ndarray, metric = 'euclidean'):\n",
    "    \"\"\"\n",
    "    Cluster a layer using DBScan\n",
    "\n",
    "    Params:\n",
    "        @layer_hs_np: An np array of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension.\n",
    "        @metric: The distance metric to use. Either \"euclidean\" or \"cosine\" are reasonable.\n",
    "    \"\"\"\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#dbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs_np)\n",
    "\n",
    "    model = cuml.DBSCAN(\n",
    "        metric = metric, # Or cosine\n",
    "        min_samples = 10, # Number of samples st the group can be considered a core point\n",
    "        verbose = False\n",
    "    )\n",
    "    cluster_labels = model.fit_predict(hs_cupy).tolist()\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def test_dbscan(layer_hs_list, metric, layers_to_group):\n",
    "    \"\"\"\n",
    "    Cluster multiple layers and print diagnostics, then print cross-layer groups.\n",
    "    \n",
    "    Params:\n",
    "        @layer_hs_list: A list of np arrays, each of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension\n",
    "        @metric: The distance metric to use. Either \"euclidean\" or \"cosine\" are reasonable.\n",
    "        @layers_to_group: The indices of layer_hs_list (0-indexed) to be used for grouping clusters across layers.\n",
    "    \"\"\"\n",
    "    cl_res = [{'layer_ix': l, 'cluster_ids': cluster_dbscan(layer_hs_list[l] , metric)} for l in tqdm(layers_to_group)]\n",
    "    \n",
    "    for r in cl_res:\n",
    "        print(f\"Clusters {len(set(r['cluster_ids'])):,} | Unassigned to clusters: {len([x for x in r['cluster_ids'] if x == -1]):,}/{len(r['cluster_ids']):,}\")\n",
    "\n",
    "    cl_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cl_res], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df.head(layer_hs_list[0].shape[0])], axis = 1))\n",
    "\n",
    "    display(cl_df.groupby(f\"layer_{str(layers_to_group[0])}_id\", as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "    print('Cross-layer clusters:')\n",
    "    print_samples(cl_df, [f\"layer_{str(l)}_id\"  for l in layers_to_group])\n",
    "\n",
    "    return cl_df\n",
    "\n",
    "dbscan_paths_1 = test_dbscan(umap_cos_10, 'euclidean', [2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HDBSCAN\n",
    "\"\"\"\n",
    "\n",
    "def cluster_hdbscan(layer_hs_np: np.ndarray, metric = 'euclidean'):\n",
    "    \"\"\"\n",
    "    Cluster a layer using HDBScan\n",
    "\n",
    "    Params:\n",
    "        @layer_hs_np: An np array of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension.\n",
    "        @metric: The distance metric to use. Either \"euclidean\" or \"cosine\" are reasonable.\n",
    "    \"\"\"\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#dbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs_np)\n",
    "\n",
    "    model = cuml.HDBSCAN(\n",
    "        min_cluster_size = len(hs_cupy) // (64 * 50), # Min 1/20 of the uniform dist value\n",
    "        max_cluster_size = len(hs_cupy) // (64 * 1/50), # Max 20x the uniform dist values \n",
    "        metric = metric,\n",
    "        min_samples = 1,\n",
    "    )\n",
    "    cluster_labels = model.fit_predict(hs_cupy).tolist()\n",
    "    return cluster_labels\n",
    "\n",
    "def test_hdbscan(layer_hs_list, metric, layers_to_group):\n",
    "    \"\"\"\n",
    "    Cluster multiple layers and print diagnostics, then print cross-layer groups.\n",
    "    \n",
    "    Params:\n",
    "        @layer_hs_list: A list of np arrays, each of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension\n",
    "        @metric: The distance metric to use. Either \"euclidean\" or \"cosine\" are reasonable.\n",
    "        @layers_to_group: The indices of layer_hs_list (0-indexed) to be used for grouping clusters across layers.\n",
    "    \"\"\"\n",
    "    cl_res = [{'layer_ix': l, 'cluster_ids': cluster_hdbscan(layer_hs_list[l] , metric)} for l in tqdm(layers_to_group)]\n",
    "    \n",
    "    for r in cl_res:\n",
    "        print(f\"Clusters {len(set(r['cluster_ids'])):,} | Unassigned to clusters: {len([x for x in r['cluster_ids'] if x == -1]):,}/{len(r['cluster_ids']):,}\")\n",
    "\n",
    "    cl_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cl_res], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df.head(layer_hs_list[0].shape[0])], axis = 1))\n",
    "\n",
    "    display(cl_df.groupby(f\"layer_{str(layers_to_group[0])}_id\", as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "    print('Cross-layer clusters:')\n",
    "    print_samples(cl_df, [f\"layer_{str(l)}_id\"  for l in layers_to_group])\n",
    "\n",
    "    return cl_df\n",
    "\n",
    "hdbscan_paths_1 = test_hdbscan(umap_cos_10, 'euclidean', [2, 3, 4, 5, 6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
