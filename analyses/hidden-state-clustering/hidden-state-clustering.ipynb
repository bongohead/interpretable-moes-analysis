{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is similar to `./../path-analysis/store-pretrained-model-paths.ipynb`, but also exports hidden states. \n",
    "- Use for clustering analysis to compare clusters of hidden states versus expert IDs.\n",
    "- Compares dense models to MoE clusters.\n",
    "- Due to the huge dataset cost of storing activations, it's better to simply create them at runtime.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib \n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.store_outputs import convert_outputs_to_df\n",
    "from utils import pretrained_models\n",
    "\n",
    "# https://docs.rapids.ai/install/\n",
    "import umap\n",
    "import cupy\n",
    "import cudf\n",
    "import cuml\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\n",
    "Architectures supported currently:\n",
    "- OlMoE architecture, includes OLMoE-1B-7B-0125-Instruct (1B/7B)\n",
    "- Qwen2MoE architecture, inclues Qwen1.5-MoE-A2.7B-Chat (2.7B/14.3B), Qwen2-57B-A14B (14B/57B)\n",
    "- Deepseek v2 architecture, includes Deepseek-v2-Lite (2.4B/15.7B), Deepseek-v2 (21B/236B)\n",
    "- Deepseek v3 architecture, includes Deepseek-v3 (37B/671B), Deepseek-R1 (37B/671B), Moonlight-16B-A3B (3B/16B)\n",
    "\"\"\"\n",
    "selected_model_index = 1\n",
    "\n",
    "def get_model(index):\n",
    "    model = [\n",
    "        ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 'olmoe'),\n",
    "        ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 'qwen2moe'),\n",
    "        ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 'dsv2'),\n",
    "        ('moonshotai/Moonlight-16B-A3B', 'moonlight', 'dsv3')\n",
    "    ][index]\n",
    "\n",
    "    return model[0], model[1], model[2]\n",
    "\n",
    "model_id, model_prefix, model_architecture = get_model(selected_model_index)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load reverse-engineered forward pass functions that return topk expert IDs and weights\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "    original_results = model(**inputs)\n",
    "    custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = True)\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    assert len(custom_results['all_topk_experts']) == len(custom_results['all_topk_weights']), 'Length of topk IDs and weights not equal'\n",
    "    print(f\"Length of topk: {len(custom_results['all_topk_experts'])}\")\n",
    "    print(f\"Topk size: {custom_results['all_topk_experts'][0].shape}\")\n",
    "    print(f\"First token topk IDs: {custom_results['all_topk_experts'][0][1,]}\")\n",
    "    print(f\"First token topk weights: {custom_results['all_topk_weights'][0][1,]}\")\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), model.config.vocab_size).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "    print(f\"Hidden states layers (pre-mlp, post-layer): {len(custom_results['all_pre_mlp_hidden_states'])} | {len(custom_results['all_hidden_states'])}\")\n",
    "    print(f\"Hidden state size (pre-mlp, post-layer): {(custom_results['all_pre_mlp_hidden_states'][0].shape)} | {(custom_results['all_hidden_states'][0].shape)}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset - C4 mix (en/zh/es)\n",
    "\"\"\"\n",
    "def load_raw_ds():\n",
    "   \n",
    "    ds_en = load_dataset('allenai/c4', 'en', split = 'validation', streaming = True).shuffle(seed = 123, buffer_size = 100_000)\n",
    "    ds_zh = load_dataset('allenai/c4', 'zh', split = 'validation', streaming = True).shuffle(seed = 123, buffer_size = 100_000)\n",
    "    ds_es = load_dataset('allenai/c4', 'es', split = 'validation', streaming = True).shuffle(seed = 123, buffer_size = 100_000)\n",
    "    \n",
    "    def get_data(ds, n_samples):\n",
    "        raw_data = []\n",
    "        ds_iter = iter(ds)\n",
    "        for _ in range(0, n_samples):\n",
    "            sample = next(ds_iter, None)\n",
    "            if sample is None:\n",
    "                break\n",
    "            raw_data.append(sample['text'])\n",
    "        \n",
    "        return raw_data\n",
    "    \n",
    "    return get_data(ds_en, 500) + get_data(ds_zh, 100) + get_data(ds_es, 100)\n",
    "\n",
    "\n",
    "raw_data = load_raw_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader. The dataloader returns the original tokens - this is important for BPE tokenizers as otherwise it's difficult to reconstruct the correct string later!\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ReconstructableTextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text_dataset, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        Creates a dataset object that also returns a B x N list of the original tokens in the same position as the input ids.\n",
    "\n",
    "        Params:\n",
    "            @text_dataset: A list of B samples of text dataset.\n",
    "            @tokenizer: A HF tokenizer object.\n",
    "        \"\"\"\n",
    "        tokenized = tokenizer(text_dataset, add_special_tokens = False, max_length = max_length, padding = 'max_length', truncation = True, return_offsets_mapping = True, return_tensors = 'pt')\n",
    "\n",
    "        self.input_ids = tokenized['input_ids']\n",
    "        self.attention_mask = tokenized['attention_mask']\n",
    "        self.offset_mapping = tokenized['offset_mapping']\n",
    "        self.original_tokens = self.get_original_tokens(text_dataset)\n",
    "\n",
    "    def get_original_tokens(self, text_dataset):\n",
    "        \"\"\"\n",
    "        Return the original tokens associated with each B x N position. This is important for reconstructing the original text when BPE tokenizers are used.\n",
    "        \n",
    "        Params:\n",
    "            @input_ids: A B x N tensor of input ids.\n",
    "            @offset_mapping: A B x N x 2 tensor of offset mappings. Get from `tokenizer(..., return_offsets_mapping = True)`.\n",
    "\n",
    "        Returns:\n",
    "            A list of length B, each with length N, containing the corresponding original tokens corresponding to the token ID at the same position of input_ids.\n",
    "        \"\"\"\n",
    "        all_token_substrings = []\n",
    "        for i in range(0, self.input_ids.shape[0]):\n",
    "            token_substrings = []\n",
    "            for j in range(self.input_ids.shape[1]): \n",
    "                start_char, end_char = self.offset_mapping[i][j].tolist()\n",
    "                if start_char == 0 and end_char == 0: # When pads, offset_mapping might be [0, 0], so let's store an empty string for those positions.\n",
    "                    token_substrings.append(\"\")\n",
    "                else:\n",
    "                    original_substring = text_dataset[i][start_char:end_char]\n",
    "                    token_substrings.append(original_substring)\n",
    "            \n",
    "            all_token_substrings.append(token_substrings)\n",
    "\n",
    "        return all_token_substrings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx], 'original_tokens': self.original_tokens[idx]}\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function; necessary to return original_tokens in the correct shape \n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([b['input_ids'] for b in batch], dim = 0)\n",
    "    attention_mask = torch.stack([b['attention_mask'] for b in batch], dim = 0)        \n",
    "    original_tokens = [b['original_tokens'] for b in batch]\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'original_tokens': original_tokens}\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    ReconstructableTextDataset(raw_data, tokenizer, max_length = 1024),\n",
    "    batch_size = 8,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get expert selections + export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Run forward passes + export data\n",
    "\n",
    "Note the bulk of compute time will be spent on exporting the CSV, not handling the forward passes.\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_and_export_topk(model, dl: ReconstructableTextDataset, layers_to_keep: list[int], max_batches: None | int = None):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the intermediate hidden layers as well as topks\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on. Should return a dict with keys `logits`, `all_topk_experts`, `all_topk_weights`, and\n",
    "          `all_pre_mlp_hidden_states`.\n",
    "        @dl: The dataloader which returns `input_Ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layers_to_keep: A list of layers for which to filter `topk_df` and `all_pre_mlp_hidden_states` (see returned object description).\n",
    "        @max_batches: The max number of batches to run.\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A sample (token)-level dataframe with corresponding input token ID, output token ID, and input token text (removes masked tokens)\n",
    "        - `topk_df`: A sample (token) x layer_ix x topk_ix level dataframe that gives the expert ID selected at each sample-layer-topk (removes masked_tokens)\n",
    "        - `all_pre_mlp_hidden_states`: A tensor of size n_samples x layers_to_keep x D return the hidden state for each retained layers. Each \n",
    "            n_sample corresponds to a row of sample_df.\n",
    "    \"\"\"\n",
    "    b_count = 0\n",
    "    all_pre_mlp_hidden_states = []\n",
    "    sample_dfs = []\n",
    "    topk_dfs = []\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "        original_tokens = batch['original_tokens']\n",
    "\n",
    "        output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "        # Check no bugs by validating output/perplexity\n",
    "        if batch_ix == 0:\n",
    "            loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), model.config.vocab_size).detach().cpu().item()\n",
    "            for i in range(min(2, input_ids.size(0))):\n",
    "                decoded_input = tokenizer.decode(input_ids[i, :attention_mask[i].sum()], skip_special_tokens = True)\n",
    "                next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                print(decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = True), 'green'))\n",
    "            print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "        \n",
    "        original_tokens_df = pd.DataFrame(\n",
    "            [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "            columns = ['sequence_ix', 'token_ix', 'token']\n",
    "        )\n",
    "\n",
    "        # Create sample (token) level dataframe\n",
    "        sample_df =\\\n",
    "            convert_outputs_to_df(input_ids, attention_mask, output['logits'])\\\n",
    "            .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "            .assign(batch_ix = batch_ix)\n",
    "\n",
    "        # Create topk x layer_ix x sample level dataframe\n",
    "        topk_df =\\\n",
    "            convert_topk_to_df(input_ids, attention_mask, output['all_topk_experts'], output['all_topk_weights'])\\\n",
    "            .assign(batch_ix = batch_ix, weight = lambda df: df['weight'])\\\n",
    "            .drop(columns = 'token_id')\\\n",
    "            .pipe(lambda df: df[df['layer_ix'].isin(layers_to_keep)])\n",
    "        \n",
    "        sample_dfs.append(sample_df)\n",
    "        topk_dfs.append(topk_df)\n",
    "\n",
    "        # Store pre-MLP hidden states - the fwd pass as n_layers list as BN x D, collapse to BN x n_layers x D, with BN filtering out masked items\n",
    "        valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "        all_pre_mlp_hidden_states.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layers_to_keep, :])\n",
    "\n",
    "        b_count += 1\n",
    "        if max_batches is not None and b_count >= max_batches:\n",
    "            break\n",
    "\n",
    "    return {'sample_df': pd.concat(sample_dfs), 'topk_df': pd.concat(topk_dfs), 'all_pre_mlp_hidden_states': torch.cat(all_pre_mlp_hidden_states, dim = 0)}\n",
    "\n",
    "res = run_and_export_topk(model, test_dl, layers_to_keep = list(range(0, 6)), max_batches = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    res['sample_df']\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    res['topk_df']\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pre_mlp_hs = res['all_pre_mlp_hidden_states']\n",
    "all_pre_mlp_hs.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Cross-layer Topk = 1 Clusters\n",
    "\"\"\"\n",
    "topk_wide =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    .merge(sample_df[['sample_ix', 'token']], on = 'sample_ix', how = 'inner')\\\n",
    "    .pivot(index = ['sample_ix', 'token'], columns = 'layer_ix', values = 'expert')\\\n",
    "    .rename(columns = lambda c: f'layer_{c}_id')\\\n",
    "    .reset_index()\n",
    "\n",
    "display(topk_wide.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "\n",
    "topk_wide\\\n",
    "    .groupby(['layer_2_id', 'layer_3_id', 'layer_4_id', 'layer_5_id'], as_index = False)\\\n",
    "    .agg(\n",
    "        n_samples = ('token', 'size'),\n",
    "        samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "    )\\\n",
    "    .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "    .sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Within layer clusters\n",
    "\"\"\"\n",
    "# Test single layer\n",
    "sl_wide =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == 4])\\\n",
    "    .merge(sample_df[['sample_ix', 'token']], on = 'sample_ix', how = 'inner')\\\n",
    "    .pivot(index = ['sample_ix', 'token'], columns = 'topk_ix', values = 'expert')\\\n",
    "    .rename(columns = lambda c: f'topk_l1_{c}_id')\\\n",
    "    .reset_index()\n",
    "\n",
    "display(sl_wide\\\n",
    "    .groupby(['topk_l1_1_id', 'topk_l1_2_id', 'topk_l1_3_id', 'topk_l1_4_id'], as_index = False)\\\n",
    "    .agg(\n",
    "        n_samples = ('token', 'size'),\n",
    "        samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "    )\\\n",
    "    .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "    .sample(25))\n",
    "\n",
    "sl2_wide =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == 5])\\\n",
    "    .merge(sample_df[['sample_ix', 'token']], on = 'sample_ix', how = 'inner')\\\n",
    "    .pivot(index = ['sample_ix', 'token'], columns = 'topk_ix', values = 'expert')\\\n",
    "    .rename(columns = lambda c: f'topk_l2_{c}_id')\\\n",
    "    .reset_index()\n",
    "\n",
    "display(sl2_wide\\\n",
    "    .groupby(['topk_l2_1_id', 'topk_l2_2_id', 'topk_l2_3_id', 'topk_l2_4_id'], as_index = False)\\\n",
    "    .agg(\n",
    "        n_samples = ('token', 'size'),\n",
    "        samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "    )\\\n",
    "    .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "    .sample(25))\n",
    "\n",
    "\n",
    "display(sl_wide.merge(sl2_wide, on = 'sample_ix', how = 'inner')\\\n",
    "    .groupby(['topk_l1_1_id', 'topk_l1_2_id', 'topk_l2_1_id', 'topk_l2_2_id'], as_index = False)\\\n",
    "    .agg(\n",
    "        n_samples = ('token_x', 'size'),\n",
    "        samples = ('token_x', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "    )\\\n",
    "    .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "    .sample(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "K-Means (note - returns imbalanced clusters)\n",
    "\"\"\" \n",
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 64):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        @layer_hs: A n_token_samples x D tensor for a single layer\n",
    "        @n_clusters: The number of clusters to return\n",
    "\n",
    "    Returns:\n",
    "        A list of length n_token_samples of cluster ids\n",
    "    \"\"\"\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "    kmeans_model = cuml.cluster.KMeans(\n",
    "        n_clusters = n_clusters,\n",
    "        max_iter = 1000,\n",
    "        random_state = 123,\n",
    "        verbose = True\n",
    "    )\n",
    "    kmeans_model.fit(hs_cupy)\n",
    "    cluster_labels = kmeans_model.labels_ # shape = (n_samples,)\n",
    "    # cluster_centers = kmeans_model.cluster_centers_ # shape = (num_clusters, D)\n",
    "    return cluster_labels.tolist()\n",
    "\n",
    "kmeans_res = [\n",
    "    {'cluster_method': 'kmeans', 'layer_ix': layer_ix, 'cluster_ids': cluster_kmeans(layer_hs, 64)}\n",
    "    for layer_ix, layer_hs in tqdm(enumerate(all_pre_mlp_hs.unbind(dim = 1)))\n",
    "]\n",
    "\n",
    "joined_df =\\\n",
    "    pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in kmeans_res], axis = 1)\\\n",
    "    .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "\n",
    "display(joined_df.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "display(joined_df.groupby('layer_3_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "\n",
    "routes =\\\n",
    "    joined_df\\\n",
    "    .groupby(['layer_2_id', 'layer_3_id', 'layer_4_id', 'layer_5_id'], as_index = False)\\\n",
    "    .agg(n_samples = ('token', 'size'), samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist()))\\\n",
    "    .pipe(lambda df: df[df['n_samples'] >= 5])\n",
    "\n",
    "display(routes.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Test decomp methods\n",
    "\"\"\"\n",
    "def reduce_pca(layer_hs: torch.Tensor, n_components = 2, fit_samples: None | int = 10_000):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#principal-component-analysis\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "    if fit_samples:\n",
    "        subset_indices = np.random.default_rng(123).choice(hs_cupy.shape[0], min(hs_cupy.shape[0], fit_samples), replace = False)\n",
    "    else:\n",
    "        subset_indices = list(range(0, hs_cupy.shape[0]))\n",
    "\n",
    "    model = cuml.PCA(\n",
    "        iterated_power = 20,\n",
    "        n_components = n_components,\n",
    "        verbose = True\n",
    "    )\n",
    "    model.fit(hs_cupy[subset_indices, :])\n",
    "    # print(f'Explained variance ratio: {model.explained_variance_ratio_}')\n",
    "    print(f'Cumulative variance ratio: {np.cumsum(model.explained_variance_ratio_)[-1]}')\n",
    "    # print(f'Means by feature: {model.mean_}')\n",
    "    print(f'Max feature mean: {np.max(model.mean_)} | Min feature mean: {np.min(model.mean_)}')\n",
    "    pred = model.transform(hs_cupy)\n",
    "    \n",
    "    return cupy.asnumpy(pred)\n",
    "\n",
    "pca_test = reduce_pca(all_pre_mlp_hs.unbind(dim = 1)[0], 100, 500_000)\n",
    "px.scatter(\n",
    "    pd.concat([pd.DataFrame({'d1': pca_test[:, 0], 'd2': pca_test[:, 1]}), sample_df.head(pca_test.shape[0])], axis = 1)\\\n",
    "        .sample(5000)\n",
    "        .assign(is_of = lambda df: np.where(df['token'] == ' of', 1, 0)),\n",
    "    x = 'd1', y = 'd2', color = 'is_of', hover_data = ['token']\n",
    ").show()\n",
    "clear_all_cuda_memory()\n",
    "\n",
    "pca_100 = [reduce_pca(layer_hs, 100, 500_000) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_umap(layer_hs: torch.Tensor, n_components = 2, metric = 'euclidean', fit_samples: None | int = 10_000):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#umap\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "    if fit_samples:\n",
    "        subset_indices = np.random.default_rng(123).choice(hs_cupy.shape[0], min(hs_cupy.shape[0], fit_samples), replace = False)\n",
    "    else:\n",
    "        subset_indices = list(range(0, hs_cupy.shape[0]))\n",
    "\n",
    "    model = cuml.UMAP(\n",
    "        n_components = n_components, \n",
    "        n_neighbors = 15, # 15 for default, smaller = more local data preserved [2 - 100]\n",
    "        metric = metric, # euclidean, cosine, manhattan, l2, hamming\n",
    "        min_dist = 0.1, # 0.1 by default, effective distance between embedded points\n",
    "        n_epochs = 250, # 200 by default for large datasets\n",
    "        random_state = None, # Allow parallelism\n",
    "        verbose = True\n",
    "    )\n",
    "    model.fit(hs_cupy[subset_indices, :])\n",
    "    pred = model.transform(hs_cupy)\n",
    "    \n",
    "    return cupy.asnumpy(pred)\n",
    "\n",
    "umap_test = reduce_umap(all_pre_mlp_hs.unbind(dim = 1)[0], 100, 'euclidean', 500_000) # 300k = 2min\n",
    "px.scatter(\n",
    "    pd.concat([pd.DataFrame({'d1': umap_test[:, 0], 'd2': umap_test[:, 1]}), sample_df.head(umap_test.shape[0])], axis = 1)\\\n",
    "        .sample(5000)\n",
    "        .assign(is_of = lambda df: np.where(df['token'] == ' of', 1, 0)),\n",
    "    x = 'd1', y = 'd2', color = 'is_of', hover_data = ['token']\n",
    ").show()\n",
    "clear_all_cuda_memory()\n",
    "\n",
    "umap_euc_100 = [reduce_umap(layer_hs, 100, 'euclidean', 500_000) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]\n",
    "umap_cos_100 = [reduce_umap(layer_hs, 100, 'cosine', 500_000) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DBScan\n",
    "\"\"\"\n",
    "def cluster_dbscan(layer_hs: torch.Tensor, fit_samples: None | int = 10_000):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#dbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "\n",
    "    if fit_samples:\n",
    "        subset_indices = np.random.randint(0, hs_cupy.shape[0], size = max(hs_cupy.shape[0], fit_samples)) # 50k = 3min\n",
    "    else:\n",
    "        subset_indices = list(range(0, hs_cupy.shape[0]))\n",
    "\n",
    "    dbscan_model = cuml.cluster.DBSCAN(\n",
    "        metric = 'euclidean', # Or cosine\n",
    "        min_samples = 5, # Number of samples st the group can be considered a core point\n",
    "        verbose = True\n",
    "    )\n",
    "    dbscan_model.fit(hs_cupy[subset_indices, :])\n",
    "    pred = dbscan_model.transform(hs_cupy)\n",
    "    cluster_labels = pred.labels_ # shape = (n_samples,)\n",
    "    \n",
    "    print(f\"Values unassigned to clusters: {len([l for l in cluster_labels.tolist() if l == -1])}/{len(cluster_labels)}\")\n",
    "\n",
    "    return cluster_labels.tolist()\n",
    "\n",
    "cluster_dbscan(all_pre_mlp_hs.unbind(dim = 1)[0], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdgasdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UMAP Testing\n",
    "\"\"\"\n",
    "\n",
    "def reduce_umap(layer_hs: torch.Tensor)\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#umap\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "    subset_indicies = np.random.choice(hs_cupy.shape[0], size = 50_000, replace = False) # 50k = 3min\n",
    "    reducer = umap.UMAP(\n",
    "        n_components = 2, \n",
    "        n_neighbors = 15, # 15 for default, smaller = more local data preserved [2 - 100]\n",
    "        metric = 'euclidean', # cosine, manhattan, l2, hamming\n",
    "        min_dist = 0.1, # 0.1 by default, effective distance between embedded points\n",
    "        n_epochs = 200, # 200 by default for large datasets\n",
    "        random_state = None # Allow parallelism\n",
    "        )\n",
    "\n",
    "    embed = reducer.fit(hs_layer_np[subset_indicies])  # shape = (n_samples, n_components)\n",
    "\n",
    "\n",
    "px.scatter(\n",
    "    pd.concat(\n",
    "        [pd.DataFrame({'d1': embed[:, 0], 'd2': embed[:, 1]}), sample_df.head(embed.shape[0])],\n",
    "        axis = 1\n",
    "        )\\\n",
    "        .sample(5000)\n",
    "        .assign(\n",
    "            is_of = lambda df: np.where(df['token'] == ' of', 1, 0)\n",
    "        ),\n",
    "    x = 'd1',\n",
    "    y = 'd2',\n",
    "    color = 'is_of',\n",
    "    hover_data = ['token']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UMAP -> 100 + HDBSCAN\n",
    "\"\"\"\n",
    "import umap\n",
    "\n",
    "def cluster_umap_to_hdbscan(layer_hs: torch.Tensor, umap_dim: int = 100, fit_samples: None | int = 10_000):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#hdbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "\n",
    "    if fit_samples:\n",
    "        subset_indices = np.random.randint(0, hs_cupy.shape[0], size = max(hs_cupy.shape[0], fit_samples)) # 50k = 3min\n",
    "    else:\n",
    "        subset_indices = list(range(0, hs_cupy.shape[0]))\n",
    "\n",
    "    hdbscan_model = cuml.cluster.HDBSCAN(\n",
    "        min_cluster_size = len(hs_cupy) // (64 * 100), # Min 1/20 of the uniform dist value\n",
    "        max_cluster_size = len(hs_cupy) // (64 * 1/100), # Max 20x the uniform dist values \n",
    "        metric = 'euclidean',\n",
    "        min_samples = 1,\n",
    "        verbose = True\n",
    "    )\n",
    "    hdbscan_model.fit(hs_cupy[subset_indices, :])\n",
    "    pred = hdbscan_model.transform(hs_cupy)\n",
    "    cluster_labels = pred.labels_ # shape = (n_samples,)\n",
    "    \n",
    "    print(f\"Values unassigned to clusters: {len([l for l in cluster_labels.tolist() if l == -1])}/{len(cluster_labels)}\")\n",
    "\n",
    "    return cluster_labels.tolist()\n",
    "\n",
    "cluster_hdbscan(all_pre_mlp_hs.unbind(dim = 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.manifold.umap import UMAP\n",
    "\n",
    "hs_for_layer = hs_by_layer[0]\n",
    "\n",
    "hs_layer_np = hs_for_layer.to(torch.float32).numpy()\n",
    "subset_indicies = np.random.choice(list(range(0, hs_layer_np.shape[0])), size = 50_000, replace = False) # 50k = 3min\n",
    "reducer = umap.UMAP(\n",
    "    n_components = 10, \n",
    "    n_neighbors = 15, \n",
    "    min_dist = 0.1, \n",
    "    n_epochs = 100,\n",
    "    data_on_host = True\n",
    "    )\n",
    "\n",
    "embed = reducer.fit(hs_layer_np[subset_indicies])  # shape = (n_samples, n_components)\n",
    "embed_all = reducer.transform(hs_layer_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size = 500,\n",
    "    min_samples = 100,\n",
    "    metric = 'euclidean' # cosine\n",
    "    ) # https://stackoverflow.com/questions/67898039/hdbscan-difference-between-parameters\n",
    "labels = clusterer.fit_predict(embed_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.scatter(\n",
    "    pd.concat(\n",
    "        [\n",
    "            pd.DataFrame({'d1': embedding[:, 0], 'd2': embedding[:, 1]}),\n",
    "            sample_df.head(embedding.shape[0])\n",
    "        ],\n",
    "        axis = 1\n",
    "    ).sample(5000)\n",
    "    .assign(\n",
    "        is_of = lambda df: np.where(df['token'] == ' of', 1, 0)\n",
    "    ),\n",
    "    x = 'd1',\n",
    "    y = 'd2',\n",
    "    color = 'is_of',\n",
    "    hover_data = ['token']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.DataFrame({'d1': embedding[:, 0], 'd2': embedding[:, 1]}),\n",
    "        sample_df.head(embedding.shape[0])\n",
    "    ],\n",
    "    axis = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Dense Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
