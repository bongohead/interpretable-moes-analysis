{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test clustering\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib \n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "import gc\n",
    "\n",
    "# https://docs.rapids.ai/install/\n",
    "import cupy\n",
    "import cuml\n",
    "\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\n",
    "Architectures supported currently:\n",
    "- OlMoE architecture, includes OLMoE-1B-7B-0125-Instruct (1B/7B)\n",
    "- Qwen2MoE architecture, inclues Qwen1.5-MoE-A2.7B-Chat (2.7B/14.3B), Qwen2-57B-A14B (14B/57B)\n",
    "- Deepseek v2 architecture, includes Deepseek-v2-Lite (2.4B/15.7B), Deepseek-v2 (21B/236B)\n",
    "- Deepseek v3 architecture, includes Deepseek-v3 (37B/671B), Deepseek-R1 (37B/671B), Moonlight-16B-A3B (3B/16B)\n",
    "\"\"\"\n",
    "selected_model_index = 1\n",
    "\n",
    "def get_model(index):\n",
    "    model = [\n",
    "        ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 'olmoe'),\n",
    "        ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 'qwen2moe'),\n",
    "        ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 'dsv2'),\n",
    "        ('moonshotai/Moonlight-16B-A3B', 'moonlight', 'dsv3')\n",
    "    ][index]\n",
    "\n",
    "    return model[0], model[1], model[2]\n",
    "\n",
    "model_id, model_prefix, model_architecture = get_model(selected_model_index)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix):\n",
    "    all_pre_mlp_hs = torch.load(f'data/{model_prefix}-all-pre-mlp-hidden-states.pt')\n",
    "    with open(f'data/{model_prefix}-metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    return all_pre_mlp_hs, metadata['sample_df'], metadata['topk_df']\n",
    "\n",
    "all_pre_mlp_hs, sample_df_import, topk_df_import = load_data(model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Cross-layer Topk = 1 Clusters\n",
    "\"\"\"\n",
    "def print_samples(df, grouping_cols):\n",
    "    \"\"\"\n",
    "    Takes a wide dataframe and groups it, then prints random groups\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        df\\\n",
    "        .groupby(grouping_cols, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .sample(25)\n",
    "    display(res)\n",
    "\n",
    "topk_wide =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    .merge(sample_df[['sample_ix', 'token']], on = 'sample_ix', how = 'inner')\\\n",
    "    .pivot(index = ['sample_ix', 'token'], columns = 'layer_ix', values = 'expert')\\\n",
    "    .rename(columns = lambda c: f'layer_{c}_id')\\\n",
    "    .reset_index()\n",
    "\n",
    "display(topk_wide.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "print_samples(topk_wide, ['layer_2_id', 'layer_3_id', 'layer_4_id', 'layer_5_id', 'layer_6_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Within layer clusters\n",
    "\"\"\"\n",
    "# Pivot by layer and topk to get expert_l4_k1, etc.\n",
    "layer_topk_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'].isin([4, 6])])\\\n",
    "    .merge(sample_df[['sample_ix', 'token']], on = 'sample_ix', how = 'inner')\\\n",
    "    .assign(layer_topk_ix = lambda df: 'l' + df['layer_ix'].astype(str) + '_k' + df['topk_ix'].astype(str))\\\n",
    "    .pivot(index = ['sample_ix', 'token'], columns = ['layer_topk_ix'], values = 'expert')\\\n",
    "    .rename(columns = lambda c: f'expert_{c}')\\\n",
    "    .merge(sample_df[['sample_ix', 'token']], on = 'sample_ix', how = 'inner')\\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "print_samples(layer_topk_df, ['expert_l4_k1', 'expert_l4_k2', 'expert_l4_k3', 'expert_l4_k4'])\n",
    "print_samples(layer_topk_df, ['expert_l6_k1', 'expert_l6_k2', 'expert_l6_k3', 'expert_l6_k4'])\n",
    "print_samples(layer_topk_df, ['expert_l4_k1', 'expert_l4_k2', 'expert_l6_k1', 'expert_l6_k2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "K-Means (note - returns imbalanced clusters)\n",
    "\"\"\" \n",
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 64):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        @layer_hs: A n_token_samples x D tensor for a single layer\n",
    "        @n_clusters: The number of clusters to return\n",
    "\n",
    "    Returns:\n",
    "        A list of length n_token_samples of cluster ids\n",
    "    \"\"\"\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "    kmeans_model = cuml.cluster.KMeans(\n",
    "        n_clusters = n_clusters,\n",
    "        max_iter = 1000,\n",
    "        random_state = 123,\n",
    "        verbose = True\n",
    "    )\n",
    "    kmeans_model.fit(hs_cupy)\n",
    "    cluster_labels = kmeans_model.labels_ # shape = (n_samples,)\n",
    "    # cluster_centers = kmeans_model.cluster_centers_ # shape = (num_clusters, D)\n",
    "    return cluster_labels.tolist()\n",
    "\n",
    "kmeans_res = [\n",
    "    {'layer_ix': layer_ix, 'cluster_ids': cluster_kmeans(layer_hs, 64)}\n",
    "    for layer_ix, layer_hs in tqdm(enumerate(all_pre_mlp_hs.unbind(dim = 1)))\n",
    "]\n",
    "\n",
    "kmeans_df =\\\n",
    "    pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in kmeans_res], axis = 1)\\\n",
    "    .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "\n",
    "display(kmeans_df.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "display(kmeans_df.groupby('layer_3_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "clear_all_cuda_memory()\n",
    "\n",
    "print_samples(kmeans_df, ['layer_2_id', 'layer_3_id', 'layer_4_id', 'layer_5_id', 'layer_6_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Test decomp methods\n",
    "\"\"\"\n",
    "def reduce_pca(layer_hs: torch.Tensor, n_components = 2, fit_samples: None | int = 10_000):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#principal-component-analysis\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "    if fit_samples:\n",
    "        subset_indices = np.random.default_rng(123).choice(hs_cupy.shape[0], min(hs_cupy.shape[0], fit_samples), replace = False)\n",
    "    else:\n",
    "        subset_indices = list(range(0, hs_cupy.shape[0]))\n",
    "\n",
    "    model = cuml.PCA(\n",
    "        iterated_power = 20,\n",
    "        n_components = n_components,\n",
    "        verbose = True\n",
    "    )\n",
    "    model.fit(hs_cupy[subset_indices, :])\n",
    "    # print(f'Explained variance ratio: {model.explained_variance_ratio_}')\n",
    "    # print(f'Cumulative variance ratio: {np.cumsum(model.explained_variance_ratio_)[-1]}')\n",
    "    # print(f'Means by feature: {model.mean_}')\n",
    "    # print(f'Max feature mean: {np.max(model.mean_)} | Min feature mean: {np.min(model.mean_)}')\n",
    "    pred = cupy.asnumpy(model.transform(hs_cupy)) \n",
    "    clear_all_cuda_memory()\n",
    "    return pred\n",
    "\n",
    "pca_test = reduce_pca(all_pre_mlp_hs.unbind(dim = 1)[0], 100, 100_000)\n",
    "px.scatter(\n",
    "    pd.concat([pd.DataFrame({'d1': pca_test[:, 0], 'd2': pca_test[:, 1]}), sample_df.head(pca_test.shape[0])], axis = 1)\\\n",
    "        .sample(5000)\n",
    "        .assign(is_of = lambda df: np.where(df['token'] == ' of', 1, 0)),\n",
    "    x = 'd1', y = 'd2', color = 'is_of', hover_data = ['token']\n",
    ").show()\n",
    "\n",
    "pca_10 = [reduce_pca(layer_hs, 10, 1_000_000) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]\n",
    "pca_100 = [reduce_pca(layer_hs, 100, 1_000_000) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_umap(layer_hs: torch.Tensor, n_components = 2, metric = 'euclidean', fit_samples: None | int = 10_000):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#umap\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "    if fit_samples:\n",
    "        subset_indices = np.random.default_rng(123).choice(hs_cupy.shape[0], min(hs_cupy.shape[0], fit_samples), replace = False)\n",
    "    else:\n",
    "        subset_indices = list(range(0, hs_cupy.shape[0]))\n",
    "\n",
    "    model = cuml.UMAP(\n",
    "        n_components = n_components, \n",
    "        n_neighbors = 15, # 15 for default, smaller = more local data preserved [2 - 100]\n",
    "        # metric = metric, # euclidean, cosine, manhattan, l2, hamming\n",
    "        # min_dist = 0.1, # 0.1 by default, effective distance between embedded points\n",
    "        # n_epochs = 200, # 200 by default for large datasets\n",
    "        # random_state = None, # Allow parallelism\n",
    "        # verbose = True,\n",
    "        build_algo = 'nn_descent',\n",
    "        build_kwds = {'nnd_n_clusters': 10, 'nnd_do_batch': True}\n",
    "    )\n",
    "    model.fit(hs_cupy[subset_indices, :], data_on_host = True)\n",
    "    pred = cupy.asnumpy(model.transform(hs_cupy)) \n",
    "    clear_all_cuda_memory()\n",
    "    return pred\n",
    "\n",
    "umap_test = reduce_umap(all_pre_mlp_hs.unbind(dim = 1)[0], 100, 'cosine', 10_000) # 300k = 2min\n",
    "clear_all_cuda_memory()\n",
    "px.scatter(\n",
    "    pd.concat([pd.DataFrame({'d1': umap_test[:, 0], 'd2': umap_test[:, 1]}), sample_df.head(umap_test.shape[0])], axis = 1)\\\n",
    "        .sample(5000)\n",
    "        .assign(is_of = lambda df: np.where(df['token'] == ' of', 1, 0)),\n",
    "    x = 'd1', y = 'd2', color = 'is_of', hover_data = ['token']\n",
    ").show()\n",
    "\n",
    "# umap_euc_10 = [reduce_umap(layer_hs, 10, 'euclidean', 1_000_000) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]\n",
    "# clear_all_cuda_memory()\n",
    "# umap_cos_10 = [reduce_umap(layer_hs, 10, 'cosine', 1_000_000) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]\n",
    "# clear_all_cuda_memory()\n",
    "# umap_euc_100 = [reduce_umap(layer_hs, 100, 'euclidean', 1_000_000) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]\n",
    "# clear_all_cuda_memory()\n",
    "# umap_cos_100 = [reduce_umap(layer_hs, 100, 'cosine', 1_000_000) for layer_hs in tqdm(all_pre_mlp_hs.unbind(dim = 1))]\n",
    "# clear_all_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DBScan\n",
    "\"\"\"\n",
    "def cluster_dbscan(layer_hs: torch.Tensor, fit_samples: None | int = 10_000):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#dbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "    if fit_samples:\n",
    "        subset_indices = np.random.default_rng(123).choice(hs_cupy.shape[0], min(hs_cupy.shape[0], fit_samples), replace = False)\n",
    "    else:\n",
    "        subset_indices = list(range(0, hs_cupy.shape[0]))\n",
    "\n",
    "    dbscan_model = cuml.cluster.DBSCAN(\n",
    "        metric = 'euclidean', # Or cosine\n",
    "        min_samples = 5, # Number of samples st the group can be considered a core point\n",
    "        verbose = True\n",
    "    )\n",
    "    dbscan_model.fit(hs_cupy[subset_indices, :])\n",
    "    pred = cupy.asnumpy(model.transform(hs_cupy).labels_).tolist() # shape = n_samples\n",
    "    print(f\"Values unassigned to clusters: {len([l for l in pred if l == -1])}/{len(pred)}\")\n",
    "    del model, hs_cupy\n",
    "    gc.collect()\n",
    "    return pred\n",
    "\n",
    "\n",
    "dbscan_cls = cluster_dbscan(umap_euc_10[0], 100)\n",
    "\n",
    "# kmeans_res = [{'layer_ix': layer_ix, 'cluster_ids': cluster_dbscan(layer_hs, 64)} for layer_ix, layer_hs in tqdm(enumerate(all_pre_mlp_hs.unbind(dim = 1)))]\n",
    "\n",
    "# kmeans_df =\\\n",
    "#     pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in kmeans_res], axis = 1)\\\n",
    "#     .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UMAP -> 100 + HDBSCAN\n",
    "\"\"\"\n",
    "import umap\n",
    "\n",
    "def cluster_umap_to_hdbscan(layer_hs: torch.Tensor, umap_dim: int = 100, fit_samples: None | int = 10_000):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#hdbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "\n",
    "    if fit_samples:\n",
    "        subset_indices = np.random.randint(0, hs_cupy.shape[0], size = max(hs_cupy.shape[0], fit_samples)) # 50k = 3min\n",
    "    else:\n",
    "        subset_indices = list(range(0, hs_cupy.shape[0]))\n",
    "\n",
    "    hdbscan_model = cuml.cluster.HDBSCAN(\n",
    "        min_cluster_size = len(hs_cupy) // (64 * 100), # Min 1/20 of the uniform dist value\n",
    "        max_cluster_size = len(hs_cupy) // (64 * 1/100), # Max 20x the uniform dist values \n",
    "        metric = 'euclidean',\n",
    "        min_samples = 1,\n",
    "        verbose = True\n",
    "    )\n",
    "    hdbscan_model.fit(hs_cupy[subset_indices, :])\n",
    "    pred = hdbscan_model.transform(hs_cupy)\n",
    "    cluster_labels = pred.labels_ # shape = (n_samples,)\n",
    "    \n",
    "    print(f\"Values unassigned to clusters: {len([l for l in cluster_labels.tolist() if l == -1])}/{len(cluster_labels)}\")\n",
    "\n",
    "    return cluster_labels.tolist()\n",
    "\n",
    "cluster_hdbscan(all_pre_mlp_hs.unbind(dim = 1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Dense Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
