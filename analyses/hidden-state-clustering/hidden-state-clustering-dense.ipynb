{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test clustering\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "\n",
    "# https://docs.rapids.ai/install/\n",
    "import cupy\n",
    "import cuml\n",
    "\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_prefix = 'qwen2.5-3b'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix):\n",
    "    all_post_layer_hs = torch.load(f'data/{model_prefix}-all-post-layer-hidden-states.pt')\n",
    "    with open(f'data/{model_prefix}-metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    return all_post_layer_hs, metadata['sample_df']\n",
    "\n",
    "all_post_layer_hs, sample_df_import = load_data(model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_samples(df, grouping_cols):\n",
    "    \"\"\"\n",
    "    Takes a wide dataframe and groups it, then prints random groups\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        df\\\n",
    "        .groupby(grouping_cols, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .sample(25)\n",
    "\n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Base K-Means (note - returns imbalanced clusters)\n",
    "\"\"\" \n",
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 64):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        @layer_hs: A n_token_samples x D tensor for a single layer\n",
    "        @n_clusters: The number of clusters to return\n",
    "\n",
    "    Returns:\n",
    "        A list of length n_token_samples of cluster ids\n",
    "    \"\"\"\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "    kmeans_model = cuml.cluster.KMeans(\n",
    "        n_clusters = n_clusters,\n",
    "        max_iter = 1000,\n",
    "        random_state = 123,\n",
    "        verbose = True\n",
    "    )\n",
    "    kmeans_model.fit(hs_cupy)\n",
    "    cluster_labels = kmeans_model.labels_ # shape = (n_samples,)\n",
    "    # cluster_centers = kmeans_model.cluster_centers_ # shape = (num_clusters, D)\n",
    "    return cluster_labels.tolist()\n",
    "\n",
    "kmeans_res = [\n",
    "    {'layer_ix': layer_ix, 'cluster_ids': cluster_kmeans(layer_hs, 64)}\n",
    "    for layer_ix, layer_hs in tqdm(enumerate(all_post_layer_hs.unbind(dim = 1)))\n",
    "]\n",
    "\n",
    "kmeans_df =\\\n",
    "    pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in kmeans_res], axis = 1)\\\n",
    "    .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "\n",
    "display(kmeans_df.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "display(kmeans_df.groupby('layer_3_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "clear_all_cuda_memory()\n",
    "\n",
    "print_samples(kmeans_df, ['layer_2_id', 'layer_3_id', 'layer_4_id', 'layer_5_id', 'layer_6_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Test decomp methods\n",
    "\"\"\"\n",
    "def reduce_pca(layer_hs: torch.Tensor, n_components = 2):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#principal-component-analysis\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float32))\n",
    "    model = cuml.PCA(\n",
    "        iterated_power = 20,\n",
    "        n_components = n_components,\n",
    "        verbose = True\n",
    "    )\n",
    "    model.fit(hs_cupy)\n",
    "    # print(f'Explained variance ratio: {model.explained_variance_ratio_}')\n",
    "    print(f'Cumulative variance ratio: {np.cumsum(model.explained_variance_ratio_)[-1]}')\n",
    "    # print(f'Means by feature: {model.mean_}')\n",
    "    # print(f'Max feature mean: {np.max(model.mean_)} | Min feature mean: {np.min(model.mean_)}')\n",
    "    pred = cupy.asnumpy(model.transform(hs_cupy))\n",
    "    clear_all_cuda_memory()\n",
    "    return pred\n",
    "\n",
    "pca_test = reduce_pca(all_post_layer_hs.unbind(dim = 1)[0], 2)\n",
    "px.scatter(\n",
    "    pd.concat([pd.DataFrame({'d1': pca_test[:, 0], 'd2': pca_test[:, 1]}), sample_df.head(pca_test.shape[0])], axis = 1)\\\n",
    "        .sample(5000)\n",
    "        .assign(is_of = lambda df: np.where(df['token'] == ' of', 1, 0)),\n",
    "    x = 'd1', y = 'd2', color = 'is_of', hover_data = ['token']\n",
    ").show()\n",
    "\n",
    "pca_10 = [reduce_pca(layer_hs, 10) for layer_hs in tqdm(all_post_layer_hs.unbind(dim = 1))]\n",
    "pca_100 = [reduce_pca(layer_hs, 100) for layer_hs in tqdm(all_post_layer_hs.unbind(dim = 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_umap(layer_hs: torch.Tensor, n_components = 2, metric = 'cosine'):\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#umap\n",
    "    hs_cupy = cupy.asarray(layer_hs.to(torch.float16))\n",
    "\n",
    "    model = cuml.UMAP(\n",
    "        n_components = n_components, \n",
    "        n_neighbors = 15, # 15 for default, smaller = more local data preserved [2 - 100]\n",
    "        metric = metric, # euclidean, cosine, manhattan, l2, hamming\n",
    "        min_dist = 0.1, # 0.1 by default, effective distance between embedded points\n",
    "        n_epochs = 200, # 200 by default for large datasets\n",
    "        random_state = None, # Allow parallelism\n",
    "        verbose = False\n",
    "    )\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory()\n",
    "    return pred\n",
    "\n",
    "umap_test = reduce_umap(all_post_layer_hs.unbind(dim = 1)[0], 2, 'cosine') # 300k = 2min\n",
    "px.scatter(\n",
    "    pd.concat([pd.DataFrame({'d1': umap_test[:, 0], 'd2': umap_test[:, 1]}), sample_df.head(umap_test.shape[0])], axis = 1)\\\n",
    "        .sample(5000)\n",
    "        .assign(is_of = lambda df: np.where(df['token'] == ' of', 1, 0)),\n",
    "    x = 'd1', y = 'd2', color = 'is_of', hover_data = ['token']\n",
    ").show()\n",
    "\n",
    "# umap_euc_10 = [reduce_umap(layer_hs, 10, 'euclidean') for layer_hs in tqdm(all_post_layer_hs.unbind(dim = 1))]\n",
    "# umap_euc_100 = [reduce_umap(layer_hs, 100, 'euclidean') for layer_hs in tqdm(all_post_layer_hs.unbind(dim = 1))]\n",
    "umap_cos_10 = [reduce_umap(layer_hs, 10, 'cosine') for layer_hs in tqdm(all_post_layer_hs.unbind(dim = 1))] # Cosine most closely maps to router (dot product)\n",
    "umap_cos_100 = [reduce_umap(layer_hs, 100, 'cosine') for layer_hs in tqdm(all_post_layer_hs.unbind(dim = 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Kmeans\n",
    "\"\"\"\n",
    "def cluster_kmeans(layer_hs_np: np.ndarray):\n",
    "    \"\"\"\n",
    "    Cluster a layer using Kmeans\n",
    "\n",
    "    Params:\n",
    "        @layer_hs_np: An np array of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension.\n",
    "    \"\"\"\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#kmeans\n",
    "    hs_cupy = cupy.asarray(layer_hs_np)\n",
    "\n",
    "    model = cuml.KMeans(\n",
    "        n_clusters = 100,\n",
    "        max_iter = 500\n",
    "    )\n",
    "    cluster_labels = model.fit_predict(hs_cupy).tolist()\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def test_kmeans(layer_hs_list, layers_to_group):\n",
    "    \"\"\"\n",
    "    Cluster multiple layers and print diagnostics, then print cross-layer groups.\n",
    "    \n",
    "    Params:\n",
    "        @layer_hs_list: A list of np arrays, each of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension\n",
    "        @layers_to_group: The indices of layer_hs_list (0-indexed) to be used for grouping clusters across layers.\n",
    "    \"\"\"\n",
    "    cl_res = [{'layer_ix': l, 'cluster_ids': cluster_kmeans(layer_hs_list[l])} for l in tqdm(layers_to_group)]\n",
    "    \n",
    "    cl_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cl_res], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df.head(layer_hs_list[0].shape[0])], axis = 1))\n",
    "\n",
    "    display(cl_df.groupby(f\"layer_{str(layers_to_group[0])}_id\", as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "    print('Cross-layer clusters:')\n",
    "    print_samples(cl_df, [f\"layer_{str(l)}_id\"  for l in layers_to_group])\n",
    "\n",
    "    return cl_df\n",
    "\n",
    "kmeans_path_1 = test_kmeans(umap_cos_100, [2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Kmeans\n",
    "\"\"\"\n",
    "def cluster_aggc(layer_hs_np: np.ndarray):\n",
    "    \"\"\"\n",
    "    Cluster a layer using Kmeans\n",
    "\n",
    "    Params:\n",
    "        @layer_hs_np: An np array of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension.\n",
    "    \"\"\"\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#dbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs_np)\n",
    "\n",
    "    model = cuml.AgglomerativeClustering(\n",
    "        n_clusters = 100,\n",
    "        metric = 'cosine'\n",
    "    )\n",
    "    cluster_labels = model.fit_predict(hs_cupy).tolist()\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def test_aggc(layer_hs_list, layers_to_group):\n",
    "    \"\"\"\n",
    "    Cluster multiple layers and print diagnostics, then print cross-layer groups.\n",
    "    \n",
    "    Params:\n",
    "        @layer_hs_list: A list of np arrays, each of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension\n",
    "        @layers_to_group: The indices of layer_hs_list (0-indexed) to be used for grouping clusters across layers.\n",
    "    \"\"\"\n",
    "    cl_res = [{'layer_ix': l, 'cluster_ids': cluster_aggc(layer_hs_list[l])} for l in tqdm(layers_to_group)]\n",
    "    \n",
    "    cl_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cl_res], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df.head(layer_hs_list[0].shape[0])], axis = 1))\n",
    "\n",
    "    display(cl_df.groupby(f\"layer_{str(layers_to_group[0])}_id\", as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "    print('Cross-layer clusters:')\n",
    "    print_samples(cl_df, [f\"layer_{str(l)}_id\"  for l in layers_to_group])\n",
    "\n",
    "    return cl_df\n",
    "\n",
    "aggc_path_1 = test_aggc(umap_cos_100, [2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DBScan\n",
    "\"\"\"\n",
    "def cluster_dbscan(layer_hs_np: np.ndarray, metric = 'euclidean'):\n",
    "    \"\"\"\n",
    "    Cluster a layer using DBScan\n",
    "\n",
    "    Params:\n",
    "        @layer_hs_np: An np array of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension.\n",
    "        @metric: The distance metric to use. Either \"euclidean\" or \"cosine\" are reasonable.\n",
    "    \"\"\"\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#dbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs_np)\n",
    "\n",
    "    model = cuml.DBSCAN(\n",
    "        metric = metric, # Or cosine\n",
    "        min_samples = hs_cupy.shape[0] // 64 // 100, # Number of samples st the group can be considered a core point\n",
    "        verbose = False\n",
    "    )\n",
    "    cluster_labels = model.fit_predict(hs_cupy).tolist()\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def test_dbscan(layer_hs_list, metric, layers_to_group):\n",
    "    \"\"\"\n",
    "    Cluster multiple layers and print diagnostics, then print cross-layer groups.\n",
    "    \n",
    "    Params:\n",
    "        @layer_hs_list: A list of np arrays, each of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension\n",
    "        @metric: The distance metric to use. Either \"euclidean\" or \"cosine\" are reasonable.\n",
    "        @layers_to_group: The indices of layer_hs_list (0-indexed) to be used for grouping clusters across layers.\n",
    "    \"\"\"\n",
    "    cl_res = [{'layer_ix': l, 'cluster_ids': cluster_dbscan(layer_hs_list[l] , metric)} for l in tqdm(layers_to_group)]\n",
    "    \n",
    "    for r in cl_res:\n",
    "        print(f\"Clusters {len(set(r['cluster_ids'])):,} | Unassigned to clusters: {len([x for x in r['cluster_ids'] if x == -1]):,}/{len(r['cluster_ids']):,}\")\n",
    "\n",
    "    cl_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cl_res], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df.head(layer_hs_list[0].shape[0])], axis = 1))\n",
    "\n",
    "    display(cl_df.groupby(f\"layer_{str(layers_to_group[0])}_id\", as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "    print('Cross-layer clusters:')\n",
    "    print_samples(cl_df, [f\"layer_{str(l)}_id\"  for l in layers_to_group])\n",
    "\n",
    "    return cl_df\n",
    "\n",
    "dbscan_paths_1 = test_dbscan(umap_cos_100, 'cosine', [2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HDBSCAN\n",
    "\"\"\"\n",
    "\n",
    "def cluster_hdbscan(layer_hs_np: np.ndarray, metric = 'euclidean'):\n",
    "    \"\"\"\n",
    "    Cluster a layer using HDBScan\n",
    "\n",
    "    Params:\n",
    "        @layer_hs_np: An np array of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension.\n",
    "        @metric: The distance metric to use. Either \"euclidean\" or \"cosine\" are reasonable.\n",
    "    \"\"\"\n",
    "    # https://docs.rapids.ai/api/cuml/stable/api/#dbscan\n",
    "    hs_cupy = cupy.asarray(layer_hs_np)\n",
    "\n",
    "    model = cuml.HDBSCAN(\n",
    "        min_cluster_size = len(hs_cupy) // 64 // 20, # Min 1/20 of the uniform dist value\n",
    "        max_cluster_size = len(hs_cupy) // 64 * 20, # Max 20x the uniform dist values \n",
    "        metric = metric,\n",
    "        min_samples = 1,\n",
    "    )\n",
    "    cluster_labels = model.fit_predict(hs_cupy).tolist()\n",
    "    return cluster_labels\n",
    "\n",
    "def test_hdbscan(layer_hs_list, metric, layers_to_group):\n",
    "    \"\"\"\n",
    "    Cluster multiple layers and print diagnostics, then print cross-layer groups.\n",
    "    \n",
    "    Params:\n",
    "        @layer_hs_list: A list of np arrays, each of size n_samples x Dhat, where Dhat is some possibly compressed hidden state dimension\n",
    "        @metric: The distance metric to use. Either \"euclidean\" or \"cosine\" are reasonable.\n",
    "        @layers_to_group: The indices of layer_hs_list (0-indexed) to be used for grouping clusters across layers.\n",
    "    \"\"\"\n",
    "    cl_res = [{'layer_ix': l, 'cluster_ids': cluster_hdbscan(layer_hs_list[l] , metric)} for l in tqdm(layers_to_group)]\n",
    "    \n",
    "    for r in cl_res:\n",
    "        print(f\"Clusters {len(set(r['cluster_ids'])):,} | Unassigned to clusters: {len([x for x in r['cluster_ids'] if x == -1]):,}/{len(r['cluster_ids']):,}\")\n",
    "\n",
    "    cl_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cl_res], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df.head(layer_hs_list[0].shape[0])], axis = 1))\n",
    "\n",
    "    display(cl_df.groupby(f\"layer_{str(layers_to_group[0])}_id\", as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False))\n",
    "    print('Cross-layer clusters:')\n",
    "    print_samples(cl_df, [f\"layer_{str(l)}_id\"  for l in layers_to_group])\n",
    "\n",
    "    return cl_df\n",
    "\n",
    "hdbscan_paths_1 = test_hdbscan(umap_cos_10, 'euclidean', [2, 3, 4, 5, 6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
