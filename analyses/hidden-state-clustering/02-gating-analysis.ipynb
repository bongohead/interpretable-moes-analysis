{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to understand the behavior of routing weights.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "\n",
    "# https://docs.rapids.ai/install/\n",
    "import cupy\n",
    "import cuml\n",
    "\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_id = 'allenai/OLMoE-1B-7B-0125-Instruct'\n",
    "model_prefix = 'olmoe'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix):\n",
    "    all_pre_mlp_hs = torch.load(f'data/{model_prefix}-all-pre-mlp-hidden-states.pt')\n",
    "    with open(f'data/{model_prefix}-metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    return all_pre_mlp_hs, metadata['sample_df'], metadata['topk_df']\n",
    "\n",
    "all_pre_mlp_hs, sample_df_import, topk_df_import = load_data(model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze routing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Norms by expert and layer\n",
    "\"\"\"\n",
    "norms_by_expert_layer = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        'layer_ix': layer_ix,\n",
    "        'norm': torch.linalg.norm(layer.mlp.gate.weight, dim = 1, ord = 1).to(torch.float16).cpu().detach().numpy(),\n",
    "        'expert': list(range(1, layer.mlp.gate.weight.shape[0] + 1))\n",
    "    })\n",
    "    for layer_ix, layer in enumerate(model.model.layers)\n",
    "])\n",
    "\n",
    "plot_df = norms_by_expert_layer.pivot(index = 'layer_ix', columns = 'expert', values = 'norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Norm by Expert and Layer\"\n",
    ").update_layout(autosize = False, width = 800).show()\n",
    "\n",
    "scaled_df =\\\n",
    "    norms_by_expert_layer\\\n",
    "    .assign(layer_mean = lambda df: df.groupby('layer_ix')['norm'].transform('mean'))\\\n",
    "    .assign(norm_scaled = lambda df: df['norm'] / df['layer_mean'] - 1)\n",
    "\n",
    "scaled_plot_df = scaled_df.pivot(index = 'layer_ix', columns = 'expert', values = 'norm_scaled')\n",
    "px.imshow(\n",
    "    scaled_plot_df,\n",
    "    x = scaled_plot_df.columns, y = scaled_plot_df.index,\n",
    "    color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Norm by Expert and Layer\"\n",
    ").update_layout(autosize = False, width = 800).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For a single layer, what do the weights and RMSnorms look like?\n",
    "\"\"\"\n",
    "plot_layer_ix = 9\n",
    "show_dims = list(range(0, 400))\n",
    "\n",
    "# RMSNorm\n",
    "rms_tensor = model.model.layers[plot_layer_ix].post_attention_layernorm.weight\n",
    "rms_df = pd.DataFrame({\n",
    "    'gamma': rms_tensor.to(torch.float16).cpu().detach().numpy(),\n",
    "    'coef': 1,\n",
    "    'dimension': list(range(0, rms_tensor.shape[0]))\n",
    "})\n",
    "plot_df = rms_df.pipe(lambda df: df[df['dimension'].isin(show_dims)]).pivot(index = 'coef', columns = 'dimension', values = 'gamma')\n",
    "\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"RMSNorm Scaling Values\"\n",
    ").update_layout(autosize = False, width = 1400, height = 400).show()\n",
    "\n",
    "\n",
    "# Weights\n",
    "wt_tensor = model.model.layers[plot_layer_ix].mlp.gate.weight\n",
    "wt_df = pd.DataFrame({\n",
    "    'value': wt_tensor.view(-1).to(torch.float16).cpu().detach().numpy(),\n",
    "    'expert': [i // wt_tensor.shape[1] for i in range(wt_tensor.view(-1).shape[0])],\n",
    "    'dimension': [i % wt_tensor.shape[1] for i in range(wt_tensor.view(-1).shape[0])]\n",
    "})\n",
    "\n",
    "plot_df = wt_df.pipe(lambda df: df[df['dimension'].isin(show_dims)]).pivot(index = 'expert', columns = 'dimension', values = 'value')\n",
    "\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Routing Weights\"\n",
    ").update_layout(autosize = False, width = 1400).show()\n",
    "\n",
    "# Scale weights by RMSNorm\n",
    "scaled_df = wt_df.merge(rms_df, on = 'dimension', how = 'inner').assign(gamma_scaled_value = lambda df: df['gamma'] * df['value'])\n",
    "plot_df = scaled_df.pipe(lambda df: df[df['dimension'].isin(show_dims)]).pivot(index = 'expert', columns = 'dimension', values = 'gamma_scaled_value')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Scaled Routing Weights\"\n",
    ").update_layout(autosize = False, width = 1400).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mean norms across layers and dimension (averaged across experts)\n",
    "\"\"\"\n",
    "dfs_list = []\n",
    "for layer_ix, layer in enumerate(model.model.layers):\n",
    "    wt_tensor = layer.mlp.gate.weight.to(torch.float16).cpu().detach()\n",
    "    rms_tensor = layer.post_attention_layernorm.weight.to(torch.float16).cpu().detach()\n",
    "    scaled = (wt_tensor * rms_tensor) # Mltiply by RMS norm\n",
    "    scaled = scaled.abs().mean(dim = 0) # Take mean L1 norm\n",
    "    dfs_list.append(pd.DataFrame({\n",
    "        'mean_norm': scaled.numpy(),\n",
    "        'layer_ix': layer_ix,\n",
    "        'dim': list(range(1, scaled.shape[0] + 1))\n",
    "    }))\n",
    "\n",
    "my_df = pd.concat(dfs_list)\n",
    "# Additionally scale by layer average\n",
    "my_df_ex_scale =\\\n",
    "    my_df\\\n",
    "    .assign(layer_mean = lambda df: df.groupby('layer_ix')['mean_norm'].transform('mean'))\\\n",
    "    .assign(mean_norm = lambda df: df['mean_norm'] / df['layer_mean'])\n",
    "\n",
    "plot_df = my_df_ex_scale.pipe(lambda df: df[df['dim']  <= 200]).pivot(index = 'layer_ix', columns = 'dim', values = 'mean_norm')\n",
    "\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    zmin = 0, zmax = 8,\n",
    "    aspect = 'auto', # Allow non-square boxes\n",
    "    color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "At dimension x layer-level, analyze activations (averaged across samples) versus routing weights (averaged across experts).\n",
    "\"\"\"\n",
    "show_dims = list(range(0, 800))\n",
    "\n",
    "pre_mlp_for_layer_norms_all = all_pre_mlp_hs[0:500_000, :, :].abs().mean(dim = 0) # Collapse to n_layers x D dimensional\n",
    "\n",
    "dfs_list = []\n",
    "for layer_ix, pre_mlp_for_layer_norm in enumerate(pre_mlp_for_layer_norms_all.unbind(dim = 0)):\n",
    "    wt_tensor = model.model.layers[layer_ix].mlp.gate.weight[:, :].to(torch.float16).cpu().detach() # (n_experts, D)\n",
    "    act_tensor = pre_mlp_for_layer_norm.cpu().detach() # D-dimensional\n",
    "    scaled = (wt_tensor * act_tensor) # Multiply by activation tensor\n",
    "    scaled = scaled.abs().mean(dim = 0) # Take mean L1 norm\n",
    "    dfs_list.append(pd.DataFrame({\n",
    "        'act_norm': act_tensor.numpy(),\n",
    "        'wt_norm': wt_tensor.abs().mean(dim = 0),\n",
    "        'mean_scaled_norm': scaled.numpy(),\n",
    "        'layer_ix': layer_ix,\n",
    "        'dim': list(range(1, scaled.shape[0] + 1)) # show_dims\n",
    "    }))\n",
    "\n",
    "pre_mlp_df = pd.concat(dfs_list)\n",
    "del dfs_list, pre_mlp_for_layer_norms_all\n",
    "\n",
    "plot_df = pre_mlp_df.pipe(lambda df: df[df['dim'].isin(show_dims)]).pivot(index = 'layer_ix', columns = 'dim', values = 'mean_scaled_norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    zmin = 0,\n",
    "    zmax = .2,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean scaled wt * activation norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "plot_df = pre_mlp_df.pipe(lambda df: df[df['dim'].isin(show_dims)]).pivot(index = 'layer_ix', columns = 'dim', values = 'act_norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    zmin = 0, zmax = 8,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean activation norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "plot_df = pre_mlp_df.pipe(lambda df: df[df['dim'].isin(show_dims)]).pivot(index = 'layer_ix', columns = 'dim', values = 'wt_norm')\n",
    "px.imshow(\n",
    "    plot_df,\n",
    "    x = plot_df.columns, y = plot_df.index,\n",
    "    aspect = 'auto', color_continuous_scale = 'ylgnbu',\n",
    "    labels = {'color': 'Norm'}, title = \"Mean weight norms by dimension and layer\"\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "# Plot activation vs routing weight norms\n",
    "px.scatter(\n",
    "    pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == 6]),\n",
    "    x = 'wt_norm', y = 'act_norm', color = 'wt_norm',\n",
    "    log_y = True,\n",
    "    log_x = True,\n",
    "    color_continuous_scale = 'viridis', title = 'Per-Dimension Plot - Activation L1 Norm versus Routing Weight L1 Norm'\n",
    ").update_layout(autosize = False, width = 1400, coloraxis = dict()).show()\n",
    "\n",
    "scipy.stats.kurtosis(pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == 6]['act_norm'].tolist() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linear regression - test ability to reconstruct topk expert id\n",
    "\"\"\"\n",
    "layer_to_test = 5\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == layer_to_test])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "\n",
    "lr_model = cuml.linear_model.LogisticRegression(\n",
    "    penalty = 'l2', \n",
    "    max_iter = 10000,\n",
    "    fit_intercept = False\n",
    ")\n",
    "\n",
    "dims = [\n",
    "    x - 1\n",
    "    for x in pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == layer_to_test]).sort_values(by = 'mean_scaled_norm', ascending = False)['dim'].tolist()\n",
    "]\n",
    "\n",
    "layer_hs = cupy.asarray(all_pre_mlp_hs[:, layer_to_test, dims[0:all_pre_mlp_hs.shape[2]//50]].to(torch.float16).detach().cpu())\n",
    "lr_model.fit(layer_hs, expert_ids_cp)\n",
    "accuracy = lr_model.score(layer_hs, expert_ids_cp)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "rand_dims = [int(x - 1) for x in np.random.choice(pre_mlp_df['dim'].tolist(), size = all_pre_mlp_hs.shape[2] // 50, replace = False)]\n",
    "rand_hs = cupy.asarray(all_pre_mlp_hs[:, layer_to_test, rand_dims].to(torch.float16).detach().cpu())\n",
    "lr_model.fit(rand_hs, expert_ids_cp)\n",
    "accuracy = lr_model.score(layer_hs, expert_ids_cp)\n",
    "print(f\"Baseline accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compare PCA top dimensinos versus scaled activation top dimensions\n",
    "\"\"\"\n",
    "layer_hs = cupy.asarray(all_pre_mlp_hs[0:200_000, layer_to_test, :].to(torch.float16).detach().cpu())\n",
    "mean_vals = cupy.mean(layer_hs, axis=0)\n",
    "std_vals = cupy.std(layer_hs, axis=0)\n",
    "std_vals = cupy.where(std_vals == 0, cupy.asarray(1e-7), std_vals)\n",
    "layer_hs_std = (layer_hs - mean_vals)/std_vals\n",
    "\n",
    "pca = cuml.decomposition.PCA(n_components = 10, random_state = 123)\n",
    "pca.fit(layer_hs_std)\n",
    "\n",
    "pc_loadings = pca.components_\n",
    "sumsq = (pc_loadings ** 2).sum(axis=0)\n",
    "\n",
    "ranking = cupy.argsort(-sumsq)  # descending order\n",
    "pca_top_dims = ranking.tolist()\n",
    "\n",
    "plot_df =\\\n",
    "    pd.DataFrame({'pca_sumsq': cupy.asarray(sumsq).tolist(), 'dim': list(range(1, len(sumsq) + 1))})\\\n",
    "    .merge(\n",
    "        pre_mlp_df.pipe(lambda df: df[df['layer_ix'] == layer_to_test])[['dim', 'mean_scaled_norm']],\n",
    "        on = 'dim',\n",
    "        how = 'inner'\n",
    "    )\n",
    "\n",
    "px.scatter(\n",
    "    plot_df,\n",
    "    x = 'mean_scaled_norm',\n",
    "    y = 'pca_sumsq',\n",
    "    log_y = True,\n",
    "    log_x = True\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What % of hidden states is explained by PCA?\n",
    "\"\"\"\n",
    "# 1) Gather some data\n",
    "clear_all_cuda_memory()\n",
    "layer_hs = cupy.asarray(all_pre_mlp_hs[0:200_000, 5, :].to(torch.float16).detach().cpu())\n",
    "\n",
    "# 2) Fit PCA\n",
    "pca_model = cuml.PCA(iterated_power = 20, n_components = 10, verbose = True)\n",
    "pca_model.fit(layer_hs)\n",
    "\n",
    "print(\"Explained variance ratio:\", pca_model.explained_variance_ratio_)\n",
    "print(\"Cumulative ratio:\", np.cumsum(pca_model.explained_variance_ratio_.get())[-1])\n",
    "\n",
    "# 3) Retrieve components & variance ratio\n",
    "components = pca_model.components_.get()  # shape = (10, D)\n",
    "expl_ratios = pca_model.explained_variance_ratio_.get()  # shape = (10,)\n",
    "\n",
    "# 4) Compute dimension-level importance\n",
    "sq_loadings = components**2        # shape (10, D)\n",
    "dim_importance = sq_loadings.T @ expl_ratios   # shape (D,)\n",
    "\n",
    "# 5) Identify top 20 dims\n",
    "top_k = 10\n",
    "idx_sorted = np.argsort(dim_importance)[::-1]\n",
    "top_dims = idx_sorted[:top_k]\n",
    "sum_top = dim_importance[top_dims].sum()\n",
    "sum_all = dim_importance.sum()\n",
    "frac_top = sum_top / sum_all\n",
    "\n",
    "print(f\"Top {top_k} dims by PCA-based importance: {top_dims}\")\n",
    "print(f\"Sum of their importances: {sum_top:.4f}\")\n",
    "print(f\"Fraction of total importance: {frac_top:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_layers = np.array(sorted(list(set(topk_df['layer_ix']))))\n",
    "unique_experts = np.array(sorted(list(set(topk_df['expert'])))) \n",
    "\n",
    "topk_grouped_0 =\\\n",
    "    topk_df.groupby(['layer_ix', 'expert'], as_index = False)\\\n",
    "    .agg(\n",
    "        token_count = ('sample_ix', 'nunique'), # count distinct tokens\n",
    "        weight_sum = ('weight', 'sum') # sum of gating weights\n",
    "    )\n",
    "\n",
    "pd.merge(\n",
    "    pd.DataFrame({'layer_ix': unique_layers}),\n",
    "    pd.DataFrame({'expert': unique_experts}),\n",
    "    how = 'cross'\n",
    ")\\\n",
    ".merge(topk_grouped_0, how = 'left', on = ['layer_ix', 'expert'])\\\n",
    ".assign(\n",
    "    token_count = lambda df: df['token_count'].fillna(0),\n",
    "    weight_sum = lambda df: df['weight_sum'].fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate load balancing metrics\n",
    "\"\"\"\n",
    "topk_grouped_0 =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    .groupby(['layer_ix', 'expert'], as_index = False)\\\n",
    "    .agg(\n",
    "        token_count = ('sample_ix', 'nunique'), # count distinct tokens\n",
    "        weight_sum = ('weight', 'sum') # sum of gating weights\n",
    "    )\n",
    "\n",
    "unique_layers = np.array(sorted(list(set(topk_df['layer_ix']))))\n",
    "unique_experts = np.array(sorted(list(set(topk_df['expert'])))) \n",
    "\n",
    "# Fill in missing expert/layers\n",
    "topk_grouped =\\\n",
    "    pd.merge(\n",
    "        pd.DataFrame({'layer_ix': unique_layers}),\n",
    "        pd.DataFrame({'expert': unique_experts}),\n",
    "        how = 'cross'\n",
    "    )\\\n",
    "    .merge(topk_grouped_0, how = 'left', on = ['layer_ix', 'expert'])\\\n",
    "    .assign(\n",
    "        token_count = lambda df: df['token_count'].fillna(0),\n",
    "        weight_sum = lambda df: df['weight_sum'].fillna(0)\n",
    "    )\\\n",
    "    .assign(\n",
    "        layer_token_sums = lambda df: df.groupby('layer_ix')['token_count'].transform('sum'), # fraction of tokens that pick (layer, expert)\n",
    "        layer_weight_sums = lambda df: df.groupby('layer_ix')['weight_sum'].transform('sum'),\n",
    "        token_frac = lambda df: df['token_count'] / df['layer_token_sums'],\n",
    "        weight_frac = lambda df: df['weight_sum'] / df['layer_weight_sums']\n",
    "    )\n",
    "\n",
    "def shannon_entropy(probs):\n",
    "    # Avoid log(0)\n",
    "    probs = probs[probs > 0]\n",
    "    return -np.sum(probs * np.log2(probs))\n",
    "\n",
    "entropies = []\n",
    "for layer, layer_df in topk_grouped.groupby('layer_ix'):\n",
    "    token_entropy = shannon_entropy(layer_df['token_frac'].values)\n",
    "    weight_entropy = shannon_entropy(layer_df['weight_frac'].values)\n",
    "    entropies.append({\n",
    "        'layer_ix': layer,\n",
    "        'token_entropy': token_entropy,\n",
    "        'weight_entropy': weight_entropy\n",
    "    })\n",
    "entropy_df = pd.DataFrame(entropies)\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    mask = (p > 0) & (q > 0)\n",
    "    return np.sum(p[mask] * np.log2(p[mask]/q[mask]))\n",
    "\n",
    "kl_list = []\n",
    "for layer, layer_df in topk_grouped.groupby('layer_ix'):\n",
    "    p_token = layer_df['token_frac'].values    \n",
    "    q = np.full_like(p_token, 1/len(p_token))\n",
    "    \n",
    "    token_kl = kl_divergence(p_token, q)\n",
    "    weight_kl = kl_divergence(layer_df['weight_frac'].values, q)\n",
    "    \n",
    "    kl_list.append({\n",
    "        'layer_ix': layer,\n",
    "        'token_kl': token_kl,\n",
    "        'weight_kl': weight_kl\n",
    "    })\n",
    "kl_df = pd.DataFrame(kl_list)\n",
    "\n",
    "px.line(\n",
    "    kl_df,\n",
    "    x = 'layer_ix', y = ['weight_kl', 'token_kl'],\n",
    "    title = 'KL Divergence from Uniform'\n",
    ").update_layout(autosize = False, width = 800, height = 400).show()\n",
    "\n",
    "px.line(\n",
    "    entropy_df,\n",
    "    x = 'layer_ix', y = ['weight_entropy', 'token_entropy'],\n",
    "    title = 'Shannon Entropy'\n",
    ").update_layout(autosize = False, width = 800, height = 400).show()\n",
    "\n",
    "px.line(\n",
    "    topk_grouped.pipe(lambda df: df[df['expert'].isin(list(range(0, 100)))]),\n",
    "    x = 'layer_ix',\n",
    "    y = 'token_count',\n",
    "    color = 'expert'\n",
    ").update_layout(autosize = False, width = 800, height = 400).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
