{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to use SVD to decompose hidden states based on whether they're used by routing or not.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "\n",
    "# https://docs.rapids.ai/install/\n",
    "import cupy\n",
    "import cuml\n",
    "\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_id = 'allenai/OLMoE-1B-7B-0125-Instruct'\n",
    "model_prefix = 'olmoe'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix):\n",
    "    all_pre_mlp_hs = torch.load(f'data/{model_prefix}-all-pre-mlp-hidden-states.pt')\n",
    "    with open(f'data/{model_prefix}-metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    return all_pre_mlp_hs, metadata['sample_df'], metadata['topk_df']\n",
    "\n",
    "all_pre_mlp_hs, sample_df_import, topk_df_import = load_data(model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's take the pre-MLP hidden states and split them using SVD into parallel and orthogonal components.\n",
    "\"\"\"\n",
    "def decompose_orthogonal(hidden_states: torch.Tensor, router_weights: torch.Tensor, method: str = 'svd', svd_tol: float = 1e-6) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Decomposes hidden states into components parallel and orthogonal to the row space of the router weights.\n",
    "\n",
    "    The component parallel to the row space ('h_para') contains the information seen by the linear router mechanism (logits = W_g @ h).\n",
    "\n",
    "    The component orthogonal to the row space ('h_orth') contains information ignored by the linear router mechanism, but potentially used by the non-linear expert MLP or downstream layers.\n",
    "\n",
    "    Params:\n",
    "        @hidden_states: Tensor of shape (n_samples, D) representing the pre-routing hidden states.\n",
    "        @router_weights: Tensor of shape (n_experts, D) representing the linear router gate weights for the layer.\n",
    "        @method: Decomposition method, 'svd' (default) or 'qr'.\n",
    "        @svd_tol: Tolerance for determining non-zero singular values in SVD to establish the matrix rank.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - h_para (torch.Tensor): Projection onto the row space (\"used\" by router). Shape (n_samples, D).\n",
    "        - h_orth (torch.Tensor): Projection onto the orthogonal complement (\"unused\" by router). Shape (n_samples, D).\n",
    "\n",
    "    Example:\n",
    "        h_para, h_orth = decompose_orthogonal(all_pre_mlp_hs[0:10_000, 1, :].to(torch.float32), model.model.layers[1].mlp.gate.weight.to(torch.float32).detach().cpu(), 'svd')\n",
    "        dot_products_svd = torch.sum(h_para * h_orth, dim=1)\n",
    "        print(f\"Mean dot product (SVD): {torch.mean(dot_products_svd).item():.4e}\")\n",
    "        print(f\"Max absolute dot product (SVD): {torch.max(torch.abs(dot_products_svd)).item():.4e}\")\n",
    "\n",
    "        reconstruction_diff_svd = torch.norm(all_pre_mlp_hs[0:10_000, 1, :].to(torch.float32) - (h_para + h_orth), dim=1)\n",
    "        print(f\"Mean reconstruction norm diff (SVD): {torch.mean(reconstruction_diff_svd).item():.4e}\")\n",
    "\n",
    "        # Can also verify that QR orthogonality/reconstruction is close to 0, and also that SVD and QR results shoudl be close torch.norm(h_svd = h_qr)\n",
    "    \"\"\"\n",
    "    _, D = hidden_states.shape\n",
    "\n",
    "    assert D == router_weights.shape[1], 'Hidden state dim != router gate dim'\n",
    "\n",
    "    if method == 'svd':\n",
    "        # Compute SVD: W_g = U S V^T\n",
    "        # V^T (Vt) has shape (k, D), where k = min(n_experts, D)\n",
    "        # The rows of V^T are the right singular vectors (orthonormal)\n",
    "        # The first 'rank' rows of V^T span the row space of W_g\n",
    "        U, S, Vt = torch.linalg.svd(router_weights, full_matrices = False) # Use full_matrices = False for efficiency if D > n_experts\n",
    "\n",
    "        # Determine rank based on tolerance\n",
    "        rank = torch.sum(S > svd_tol)\n",
    "        if rank == 0:\n",
    "             raise Exception('Router weights matrix has rank 0 according to tolerance.')\n",
    "\n",
    "        # Basis for the row space (columns of Vr)\n",
    "        # Vt[:rank] selects the first 'rank' rows (shape rank x D)\n",
    "        # .T makes it (D x rank) - columns are the orthonormal basis vectors\n",
    "        Vr = Vt[:rank, :].T\n",
    "\n",
    "        # Project hidden_states onto the row space (Vr)\n",
    "        # Formula: h_para = Vr @ Vr^T @ h\n",
    "        # Batched: H_row = (H @ Vr) @ Vr^T\n",
    "        # (n_samples, D) @ (D, rank) -> (n_samples, rank)\n",
    "        h_projected_coeffs = hidden_states @ Vr\n",
    "        # (n_samples, rank) @ (rank, D) -> (n_samples, D)\n",
    "        h_para = h_projected_coeffs @ Vr.T\n",
    "\n",
    "    elif method == 'qr':\n",
    "        # Compute QR decomposition of W_g^T: W_g^T = Q R\n",
    "        # Q will have shape (D, k), where k = min(D, n_experts)\n",
    "        # Columns of Q form an orthonormal basis for column space of W_g^T, which is the row space of W_g.\n",
    "        Q, R = torch.linalg.qr(router_weights.T, mode = 'reduced') # Use 'reduced' mode for efficiency\n",
    "\n",
    "        # Q's columns are the orthonormal basis (shape D x k)\n",
    "        # Need to consider rank deficiency if applicable, but QR handles it implicitly by the shape of Q returned by 'reduced' mode.\n",
    "\n",
    "        # Project hidden_states onto the column space of Q\n",
    "        # Formula: h_para = Q @ Q^T @ h\n",
    "        # Batched: H_row = (H @ Q) @ Q^T\n",
    "        # (n_samples, D) @ (D, k) -> (n_samples, k)\n",
    "        h_projected_coeffs = hidden_states @ Q\n",
    "        # (n_samples, k) @ (k, D) -> (n_samples, D)\n",
    "        h_para = h_projected_coeffs @ Q.T\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Method must be svd or qr')\n",
    "\n",
    "    # The orthogonal component is the residual\n",
    "    h_orth = hidden_states - h_para\n",
    "\n",
    "    return h_para.to(torch.float16), h_orth.to(torch.float16)\n",
    "\n",
    "test_layers = list(range(0, 8))\n",
    "\n",
    "res = [\n",
    "    decompose_orthogonal(all_pre_mlp_hs[:, layer_ix, :].to(torch.float32), model.model.layers[layer_ix].mlp.gate.weight.to(torch.float32).detach().cpu(), 'svd')\n",
    "    for layer_ix in tqdm(test_layers)\n",
    "]\n",
    "\n",
    "h_para_by_layer = [x[0] for x in res]\n",
    "h_orth_by_layer = [x[1] for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function for grouping\n",
    "\"\"\"\n",
    "def print_samples(df, grouping_cols):\n",
    "    \"\"\"\n",
    "    Takes a wide dataframe and groups it, then prints random groups\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        df\\\n",
    "        .groupby(grouping_cols, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 10)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .sample(35)\n",
    "    \n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's cluster the para and ortho using k-means and see what clusters we get\n",
    "\"\"\"\n",
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 512):\n",
    "    \"\"\"\n",
    "    K-means clustering\n",
    "    \"\"\"\n",
    "    kmeans_model = cuml.cluster.KMeans(n_clusters = n_clusters, max_iter = 1000, random_state = 123)\n",
    "    kmeans_model.fit(cupy.asarray(layer_hs.to(torch.float32)))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return kmeans_model.labels_.tolist()\n",
    "\n",
    "def get_cluster(sample_df, hidden_states_by_layer, n_clusters = 512):\n",
    "    \"\"\"\n",
    "    Get k-means clusters across hidden state layers\n",
    "    \"\"\"\n",
    "    cluster_ids_by_layer = [\n",
    "        {'layer_ix': layer_ix, 'cluster_ids': cluster_kmeans(layer_hs, n_clusters)} \n",
    "        for layer_ix, layer_hs in tqdm(enumerate(hidden_states_by_layer))\n",
    "    ]\n",
    "\n",
    "    cluster_ids_df =\\\n",
    "        pd.concat([pd.DataFrame({'layer_' + str(x['layer_ix']) + '_id': x['cluster_ids']}) for x in cluster_ids_by_layer], axis = 1)\\\n",
    "        .pipe(lambda df: pd.concat([df, sample_df], axis = 1))\n",
    "    \n",
    "    display(\n",
    "        cluster_ids_df.groupby('layer_1_id', as_index = False).agg(n_samples = ('token', 'size')).sort_values(by = 'n_samples', ascending = False)\n",
    "    )\n",
    "\n",
    "    return cluster_ids_df\n",
    "\n",
    "para_clusters_df = get_cluster(sample_df, h_para_by_layer)\n",
    "orth_clusters_df = get_cluster(sample_df, h_orth_by_layer)\n",
    "\n",
    "print_samples(para_clusters_df, ['layer_1_id', 'layer_2_id'])\n",
    "print_samples(orth_clusters_df, ['layer_1_id', 'layer_2_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_samples(para_clusters_df, ['layer_6_id', 'layer_7_id'])\n",
    "print_samples(orth_clusters_df, ['layer_6_id', 'layer_7_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count how many clusters are token-specific\n",
    "\"\"\"\n",
    "def get_single_token_cluster_counts(cluster_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Get how many tokens belong to a single cluster\n",
    "    \"\"\"\n",
    "    res =\\\n",
    "        cluster_df\\\n",
    "        .groupby([f'layer_{str(layer_ix)}_id'], as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples = ('token', 'size'),\n",
    "            samples = ('token', lambda s: s.sample(n = min(len(s), 20)).tolist())\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= 5])\\\n",
    "        .assign(is_eq = lambda df: df.samples.apply(lambda s: 1 if len(set(s)) == 1 else 0))\\\n",
    "        .groupby('is_eq', as_index = False)\\\n",
    "        .agg(count = ('is_eq', 'count'))\n",
    "\n",
    "    return(res)\n",
    "\n",
    "display(get_single_token_cluster_counts(para_clusters_df, 7))\n",
    "display(get_single_token_cluster_counts(orth_clusters_df, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count entropy distribution\n",
    "\"\"\"\n",
    "def get_entropy_distribution(cluster_df, layer_ix, min_cluster_size = 1):\n",
    "    cluster_id_col = f'layer_{str(layer_ix)}_id'\n",
    "\n",
    "    def calculate_dominance(series):\n",
    "        \"\"\"Calculates the proportion of the most frequent item.\"\"\"\n",
    "        if series.empty:\n",
    "            return np.nan\n",
    "        counts = series.value_counts()\n",
    "        return counts.iloc[0] / counts.sum()\n",
    "\n",
    "    def calculate_normalized_entropy(series):\n",
    "        \"\"\"Calculates entropy normalized by log2(n_unique_tokens).\"\"\"\n",
    "        if series.empty:\n",
    "            return np.nan\n",
    "        counts = series.value_counts()\n",
    "        n_unique = len(counts)\n",
    "        \n",
    "        if n_unique <= 1:\n",
    "            return 0.0 # Perfectly pure cluster has zero entropy\n",
    "\n",
    "        ent = scipy.stats.entropy(counts, base=2)\n",
    "        \n",
    "        # Normalize by log2 of the number of unique elements\n",
    "        return ent / np.log2(n_unique)\n",
    "\n",
    "    # Perform aggregation\n",
    "    agg_metrics =\\\n",
    "        cluster_df\\\n",
    "        .groupby(cluster_id_col, as_index = False)\\\n",
    "        .agg(\n",
    "            n_samples=('token', 'size'),\n",
    "            n_unique_tokens=('token', 'nunique'),\n",
    "            dominance=('token', calculate_dominance),\n",
    "            normalized_entropy=('token', calculate_normalized_entropy)\n",
    "        )\\\n",
    "        .pipe(lambda df: df[df['n_samples'] >= min_cluster_size])\n",
    "\n",
    "    return agg_metrics\n",
    "\n",
    "para_entropy = get_entropy_distribution(para_clusters_df, 1)\n",
    "orth_entropy = get_entropy_distribution(orth_clusters_df, 1)\n",
    "\n",
    "print(f\"Para entropy: {para_entropy['normalized_entropy'].mean()}\")\n",
    "print(f\"Orth entropy: {orth_entropy['normalized_entropy'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction/probing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic regression - predict topk using h_orth?\n",
    "\"\"\"\n",
    "# Test layer \n",
    "test_layer = 0\n",
    "\n",
    "def run_lr(x_cp, y_cp):\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.1, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 10000, fit_intercept = False)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, expert_ids_cp)\n",
    "run_lr(x_cp_orth, expert_ids_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layer = 2\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, expert_ids_cp)\n",
    "run_lr(x_cp_orth, expert_ids_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict NEXT layer expert ids\n",
    "\"\"\"\n",
    "test_layer = 1\n",
    "\n",
    "expert_ids =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == test_layer + 1])\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "    ['expert'].tolist()\n",
    "\n",
    "expert_ids_cp = cupy.asarray(expert_ids)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "# x_cp_ccat = cupy.asarray(torch.cat(\n",
    "#     [h_para_by_layer[test_layer].to(torch.float16).detach().cpu(), h_orth_by_layer[test_layer].to(torch.float16).detach().cpu()],\n",
    "#     dim = 1\n",
    "#     ))\n",
    "\n",
    "run_lr(x_cp_para, expert_ids_cp)\n",
    "run_lr(x_cp_orth, expert_ids_cp)\n",
    "# run_lr(x_cp_ccat, expert_ids_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict token ID\n",
    "\"\"\"\n",
    "display(\n",
    "    sample_df.groupby('token', as_index = False).agg(n = ('token', 'count')).sort_values(by = 'n', ascending = False).head(30)\n",
    ")\n",
    "\n",
    "test_layer = 0\n",
    "\n",
    "y_df =\\\n",
    "    sample_df\\\n",
    "    .assign(is_sample = lambda df: np.where(df['token'].isin([' the']), 1, 0))\\\n",
    "    ['is_sample'].tolist()\n",
    "\n",
    "y_cp = cupy.asarray(y_df)\n",
    "x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "run_lr(x_cp_para, y_cp)\n",
    "run_lr(x_cp_orth, y_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analyze stability over layers\n",
    "\"\"\"\n",
    "def calculate_layer_transition_stability(h_orth_layers: dict, h_para_layers: dict, layer_l: int):\n",
    "    \"\"\"\n",
    "    Calculates the stability of h_orth and h_para representations between layer_l and layer_l+1 using cosine similarity and Euclidean distance.\n",
    "\n",
    "    Params:\n",
    "        @h_orth_layers: Dictionary where keys are layer indices (int) and values are (n_samples, D) tensors for h_orth.\n",
    "        @h_para_layers: Dictionary where keys are layer indices (int) andvalues are (n_samples, D) tensors for h_para.\n",
    "        layer_l: The starting layer index for the transition (e.g., 6 for L6->L7).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing:\n",
    "        - 'cosine_similarity_orth': (n_samples,) tensor of cosine similarities for h_orth.\n",
    "        - 'cosine_similarity_para': (n_samples,) tensor of cosine similarities for h_para.\n",
    "        - 'euclidean_distance_orth': (n_samples,) tensor of L2 distances for h_orth.\n",
    "        - 'euclidean_distance_para': (n_samples,) tensor of L2 distances for h_para.\n",
    "    \"\"\"\n",
    "    layer_lp1 = layer_l + 1\n",
    "\n",
    "    # Get tensors for the specified layers\n",
    "    h_orth_l = h_orth_layers[layer_l]\n",
    "    h_orth_lp1 = h_orth_layers[layer_lp1]\n",
    "    h_para_l = h_para_layers[layer_l]\n",
    "    h_para_lp1 = h_para_layers[layer_lp1]\n",
    "\n",
    "    # --- Calculate Cosine Similarities (Higher is more stable) ---\n",
    "    # dim = 1 calculates similarity row-wise\n",
    "    sim_orth = torch.nn.functional.cosine_similarity(h_orth_l, h_orth_lp1, dim = 1)\n",
    "    sim_para = torch.nn.functional.cosine_similarity(h_para_l, h_para_lp1, dim = 1)\n",
    "\n",
    "    # --- Calculate Euclidean Distances (Lower is more stable) ---\n",
    "    # torch.linalg.norm computes norms. ord=2 is L2 norm.\n",
    "    dist_orth = torch.linalg.norm(h_orth_l - h_orth_lp1, ord = 2, dim = 1)\n",
    "    dist_para = torch.linalg.norm(h_para_l - h_para_lp1, ord = 2, dim = 1)\n",
    "\n",
    "    return {\n",
    "        'cosine_similarity_orth': sim_orth.to(torch.float16),\n",
    "        'cosine_similarity_para': sim_para.to(torch.float16),\n",
    "        'euclidean_distance_orth': dist_orth.to(torch.float16),\n",
    "        'euclidean_distance_para': dist_para.to(torch.float16),\n",
    "    }\n",
    "\n",
    "stability_results = calculate_layer_transition_stability(\n",
    "    h_orth_layers = {\n",
    "        0: h_orth_by_layer[0].to(torch.float32).detach().cpu(),\n",
    "        1: h_orth_by_layer[1].to(torch.float32).detach().cpu()\n",
    "    },\n",
    "    h_para_layers = {\n",
    "        0: h_para_by_layer[0].to(torch.float32).detach().cpu(), \n",
    "        1: h_para_by_layer[1].to(torch.float32).detach().cpu() \n",
    "    },\n",
    "    layer_l = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking sim_orth for non-finite values:\", torch.isfinite(sim_orth).all())\n",
    "print(\"Checking dist_orth for non-finite values:\", torch.isfinite(dist_orth).all())\n",
    "    \n",
    "\n",
    "print(\"Zero vectors in h_orth_l:\", torch.where(torch.linalg.norm(h_orth_l, dim=1) == 0))\n",
    "print(\"Zero vectors in h_orth_lp1:\", torch.where(torch.linalg.norm(h_orth_lp1, dim=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_orth_np = stability_results['cosine_similarity_orth'].numpy()\n",
    "sim_para_np = stability_results['cosine_similarity_para'].numpy()\n",
    "dist_orth_np = stability_results['euclidean_distance_orth'].numpy()\n",
    "dist_para_np = stability_results['euclidean_distance_para'].numpy()\n",
    "\n",
    "# (Keep the print statements for descriptive statistics as before)\n",
    "\n",
    "print(\"\\nCosine Similarity (Higher is More Stable):\")\n",
    "print(f\"  h_orth: Mean={np.mean(sim_orth_np):.4f}, Median={np.median(sim_orth_np):.4f}, Std={np.std(sim_orth_np):.4f}\")\n",
    "print(f\"  h_para: Mean={np.mean(sim_para_np):.4f}, Median={np.median(sim_para_np):.4f}, Std={np.std(sim_para_np):.4f}\")\n",
    "print(\"\\nEuclidean Distance (Lower is More Stable):\")\n",
    "print(f\"  h_orth: Mean={np.mean(dist_orth_np):.4f}, Median={np.median(dist_orth_np):.4f}, Std={np.std(dist_orth_np):.4f}\")\n",
    "print(f\"  h_para: Mean={np.mean(dist_para_np):.4f}, Median={np.median(dist_para_np):.4f}, Std={np.std(dist_para_np):.4f}\")\n",
    "\n",
    "layer_idx = 0\n",
    "\n",
    "data_sim = pd.DataFrame({\n",
    "    'value': np.concatenate([sim_orth_np, sim_para_np]),\n",
    "    'component': ['h_orth'] * len(sim_orth_np) + ['h_para'] * len(sim_para_np),\n",
    "    'metric': 'cos'\n",
    "})\n",
    "\n",
    "data_dist = pd.DataFrame({\n",
    "    'value': np.concatenate([dist_orth_np, dist_para_np]),\n",
    "    'component': ['h_orth'] * len(dist_orth_np) + ['h_para'] * len(dist_para_np),\n",
    "    'metric': 'euc'\n",
    "})\n",
    "\n",
    "df_plot = pd.concat([data_sim, data_dist], ignore_index=True)\n",
    "\n",
    "fig = px.histogram(\n",
    "    df_plot,\n",
    "    x = 'value', # Values for the histogram\n",
    "    color = 'component', # Creates separate histograms for 'h_orth' and 'h_para'\n",
    "    facet_col = 'metric', # Creates separate subplots (columns) for each metric type\n",
    "    histnorm = 'probability density', # Normalize histograms\n",
    "    barmode = 'overlay', # Overlay histograms within each subplot\n",
    "    opacity = 0.75,\n",
    "    title = f'Stability Comparison: Layer {layer_idx} to {layer_idx+1}',\n",
    "    labels = {'component': 'Component Type'} \n",
    "    )\\\n",
    "    .update_xaxes(title_text = \"Cosine Similarity\", col = 1)\\\n",
    "    .update_xaxes(title_text = \"Euclidean Distance\", col = 2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# --- Statistical Test (Example: Mann-Whitney U test) ---\n",
    "u_stat_sim, p_value_sim = scipy.stats.mannwhitneyu(sim_orth_np, sim_para_np, alternative='greater') # Test if orth > para\n",
    "u_stat_dist, p_value_dist = scipy.stats.mannwhitneyu(dist_para_np, dist_orth_np, alternative='greater') # Test if para > orth\n",
    "print(\"\\nMann-Whitney U Test Results:\")\n",
    "print(f\"  Cosine Similarity (H1: orth > para): p-value = {p_value_sim:.2e} | rejection = h_orth sim is higher\")\n",
    "print(f\"  Euclidean Distance (H1: para > orth): p-value = {p_value_dist:.2e} | rejection = h_orth distance is lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Logit lens - take a single prompt and see what the different hidden states are predicting \n",
    "\"\"\"\n",
    "\n",
    "sample_ix = []\n",
    "\n",
    "pre_mlp_hidden_states = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
