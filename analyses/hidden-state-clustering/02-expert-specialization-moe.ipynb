{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to test orthogonality of expert specialization.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18084712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cupy\n",
    "import cuml\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10fc64",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_id = 'allenai/OLMoE-1B-7B-0125-Instruct'\n",
    "model_prefix = 'olmoe'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix):\n",
    "    all_pre_mlp_hs = torch.load(f'data/{model_prefix}/all-pre-mlp-hidden-states.pt')\n",
    "    all_expert_outputs = torch.load(f'data/{model_prefix}/all-expert-outputs.pt')\n",
    "    with open(f'data/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    return all_pre_mlp_hs, all_expert_outputs, metadata['sample_df'], metadata['topk_df'], metadata['all_pre_mlp_hidden_states_layers'], metadata['all_expert_outputs_layers']\n",
    "\n",
    "all_pre_mlp_hs_import, all_expert_outputs_import, sample_df_import, topk_df_import, act_map, expert_map = load_data(model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "del sample_df_import, topk_df_import\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dccba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatability) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {layer_ix: all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "all_expert_outputs = all_expert_outputs_import.to(torch.float16)\n",
    "compare_bf16_fp16_batched(all_expert_outputs_import, all_expert_outputs)\n",
    "del all_expert_outputs_import\n",
    "all_expert_outputs = {layer_ix: all_expert_outputs[:, save_ix, :, :] for save_ix, layer_ix in enumerate(expert_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04e099",
   "metadata": {},
   "source": [
    "## Compare activation clusters of single expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize clusters\n",
    "\"\"\"\n",
    "\n",
    "layer_ix = 10\n",
    "expert_id = 24\n",
    "\n",
    "topk_df\\\n",
    "    .pipe(lambda df: df[(df['layer_ix'] == layer_ix) & (df[df['expert_id'] == expert_id])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae135d00",
   "metadata": {},
   "source": [
    "## Compare outputs of single expert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
