{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to test orthogonality of expert specialization.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18084712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "from utils.svd import decompose_orthogonal, decompose_sideways\n",
    "from utils.vis import combine_plots\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10fc64",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_id = 'allenai/OLMoE-1B-7B-0125-Instruct'\n",
    "model_prefix = 'olmoe'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix):\n",
    "    all_pre_mlp_hs = torch.load(f'data/{model_prefix}/all-pre-mlp-hidden-states.pt')\n",
    "    all_expert_outputs = torch.load(f'data/{model_prefix}/all-expert-outputs.pt')\n",
    "    with open(f'data/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    return all_pre_mlp_hs, all_expert_outputs, metadata['sample_df'], metadata['topk_df'], metadata['all_pre_mlp_hidden_states_layers'], metadata['all_expert_outputs_layers']\n",
    "\n",
    "all_pre_mlp_hs_import, all_expert_outputs_import, sample_df_import, topk_df_import, act_map, expert_map = load_data(model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\n",
    "\n",
    "topk1_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "del sample_df_import, topk_df_import\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dccba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {layer_ix: all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "all_expert_outputs = all_expert_outputs_import.to(torch.float16)\n",
    "compare_bf16_fp16_batched(all_expert_outputs_import, all_expert_outputs)\n",
    "del all_expert_outputs_import\n",
    "all_expert_outputs = {layer_ix: all_expert_outputs[:, save_ix, :, :] for save_ix, layer_ix in enumerate(expert_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04e099",
   "metadata": {},
   "source": [
    "## Visualize activation clusters of single (layer, expert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize clusters\n",
    "\"\"\"\n",
    "layer_ix = 9\n",
    "expert_id = 1\n",
    "\n",
    "relevant_sample_ids =\\\n",
    "    topk1_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == layer_ix])\\\n",
    "    .pipe(lambda df: df[df['expert'] == expert_id])\\\n",
    "    .sort_values(by = 'sample_ix', ascending = True)\\\n",
    "    ['sample_ix']\\\n",
    "    .tolist()\n",
    "\n",
    "# Get expert IDs of previous layer\n",
    "prev_experts_df =\\\n",
    "    topk1_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == layer_ix - 1])\\\n",
    "    .pipe(lambda df: df[df['sample_ix'].isin(relevant_sample_ids)])\\\n",
    "    .rename(columns = {'expert': 'prev_expert'})\\\n",
    "    [['sample_ix', 'prev_expert']]\n",
    "\n",
    "# Get sample dfs of relevant sample IDs, include expert IDs of previous layer\n",
    "relevant_samples_df =\\\n",
    "    sample_df[sample_df['sample_ix'].isin(relevant_sample_ids)]\\\n",
    "    .merge(prev_experts_df, on = 'sample_ix', how = 'inner')\n",
    "\n",
    "display(relevant_samples_df)\n",
    "\n",
    "relevant_pre_mlp_hs = all_pre_mlp_hs[layer_ix][relevant_sample_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d67fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helpers\n",
    "\"\"\"\n",
    "def reduce_pca(input_tensor: torch.Tensor, n_components = 2):\n",
    "    hs_cupy = cupy.asarray(input_tensor.to(torch.float32))\n",
    "    model = cuml.PCA(\n",
    "        iterated_power = 100,\n",
    "        n_components = n_components,\n",
    "        verbose = True\n",
    "    )\n",
    "    model.fit(hs_cupy)\n",
    "    # print(f'Explained variance ratio: {model.explained_variance_ratio_}')\n",
    "    print(f'Cumulative variance ratio: {np.cumsum(model.explained_variance_ratio_)[-1]}')\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return pred\n",
    "\n",
    "def reduce_umap(input_tensor: torch.Tensor, n_components = 2, metric = 'cosine', n_epochs = 200):\n",
    "    hs_cupy = cupy.asarray(input_tensor.to(torch.float32))\n",
    "    model = cuml.UMAP(\n",
    "        n_components = n_components, \n",
    "        n_neighbors = 20, # 15 for default, smaller = more local data preserved [2 - 100]\n",
    "        metric = metric, # euclidean, cosine, manhattan, l2, hamming\n",
    "        min_dist = 0.5, # 0.1 by default, effective distance between embedded points\n",
    "        n_epochs = n_epochs, # 200 by default for large datasets\n",
    "        random_state = 123, # Allow parallelism\n",
    "        verbose = False\n",
    "    )\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return pred\n",
    "\n",
    "def plot_manifold(plot_df, color_col, hover_col, title = None):\n",
    "    plot = px.scatter(\n",
    "        plot_df,\n",
    "        x = 'd1', y = 'd2', color = color_col, hover_data = [hover_col],\n",
    "        title = title\n",
    "    ).update_layout(autosize = False, height = 400)\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot PCA + UMAP, color by previous expert ID\n",
    "\"\"\"\n",
    "pca_res = reduce_pca(relevant_pre_mlp_hs, 2)\n",
    "pca_plot_df =\\\n",
    "    pd.concat([pd.DataFrame({'d1': pca_res[:, 0], 'd2': pca_res[:, 1]}), relevant_samples_df], axis = 1)\\\n",
    "    .sample(5000)\\\n",
    "    .assign(prev_expert = lambda df: df['prev_expert'].astype(str))\n",
    "plot_manifold(pca_plot_df, 'prev_expert', 'token')\n",
    "plot_manifold(pca_plot_df, 'source', 'token')\n",
    "\n",
    "ump_res = reduce_umap(relevant_pre_mlp_hs, 2, 'cosine')\n",
    "ump_plot_df =\\\n",
    "    pd.concat([pd.DataFrame({'d1': pca_res[:, 0], 'd2': pca_res[:, 1]}), relevant_samples_df], axis = 1)\\\n",
    "    .sample(5000)\\\n",
    "    .assign(prev_expert = lambda df: df['prev_expert'].astype(str))\n",
    "plot_manifold(ump_plot_df, 'prev_expert', 'token')\n",
    "plot_manifold(ump_plot_df, 'source', 'token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8fae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot PCA + UMAP, but this time include token samples from outside this expert\n",
    "\"\"\"\n",
    "nonrelevant_sample_ids =\\\n",
    "    topk1_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == layer_ix])\\\n",
    "    .pipe(lambda df: df[df['expert'] != expert_id])\\\n",
    "    .sort_values(by = 'sample_ix', ascending = True)\\\n",
    "    ['sample_ix']\\\n",
    "    .tolist()\n",
    "\n",
    "# Get sample dfs of relevant sample IDs, include expert IDs of previous layer\n",
    "nonrelevant_samples_df = sample_df[sample_df['sample_ix'].isin(nonrelevant_sample_ids)]\n",
    "nonrelevant_pre_mlp_hs = all_pre_mlp_hs[layer_ix][nonrelevant_sample_ids]\n",
    "\n",
    "# Prep all_samples_df samples df\n",
    "all_samples_df = pd.concat([\n",
    "    relevant_samples_df.assign(prev_expert = lambda df: df['prev_expert'].astype(str)), \n",
    "    nonrelevant_samples_df.assign(prev_expert = 'NA')\n",
    "]).reset_index(drop = True)\n",
    "\n",
    "pca_res = reduce_pca(torch.concat([relevant_pre_mlp_hs, nonrelevant_pre_mlp_hs], dim = 0), 2)\n",
    "pca_plot_df =\\\n",
    "    pd.concat([pd.DataFrame({'d1': pca_res[:, 0], 'd2': pca_res[:, 1]}), all_samples_df], axis = 1)\\\n",
    "    .pipe(lambda df: pd.concat([\n",
    "        df[df['prev_expert'] == 'NA'].sample(10_000),\n",
    "        df[df['prev_expert'] != 'NA'].sample(10_000)\n",
    "    ]))\n",
    "\n",
    "plot_manifold(pca_plot_df, 'prev_expert', 'token')\n",
    "plot_manifold(pca_plot_df, 'source', 'token')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347bb496",
   "metadata": {},
   "source": [
    "## Cross-layer source mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot cross-layer source mappings\n",
    "\"\"\"\n",
    "layers_to_test = [0, 2, 7, 13, 15]\n",
    "\n",
    "def get_sample_df_for_layer(sample_df, topk1_df, layer_ix):\n",
    "    layer_df =\\\n",
    "        sample_df\\\n",
    "        .merge(\n",
    "            topk1_df.pipe(lambda df: df[df['layer_ix'] == layer_ix])[['expert', 'sample_ix']],\n",
    "            how = 'inner',\n",
    "            on = 'sample_ix'\n",
    "        )\\\n",
    "        .merge(\n",
    "            topk1_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']],\n",
    "            how = 'left',\n",
    "            on = 'sample_ix'\n",
    "        )\n",
    "    return layer_df\n",
    "\n",
    "\n",
    "test_layers = [\n",
    "    {'layer_ix': layer_ix, 'sample_df': get_sample_df_for_layer(sample_df, topk1_df, layer_ix)}\n",
    "    for layer_ix in layers_to_test\n",
    "]\n",
    "\n",
    "test_layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "H by language\n",
    "\"\"\"\n",
    "for test_layer in test_layers:\n",
    "    pca_res = reduce_pca(all_pre_mlp_hs[test_layer['layer_ix']], 2)\n",
    "    pca_plot_df =\\\n",
    "        pd.concat([pd.DataFrame({'d1': pca_res[:, 0], 'd2': pca_res[:, 1]}), test_layer['sample_df']], axis = 1)\\\n",
    "        .sample(2500)\n",
    "    plot_manifold(pca_plot_df, 'source', 'token', f\"<em>H<sub>{str(test_layer['layer_ix'])}</sub></em>\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3888b451-56f0-4e8b-b4dd-3bab653cec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "H by expert\n",
    "\"\"\"\n",
    "for test_layer in test_layers:\n",
    "    pca_res = reduce_pca(all_pre_mlp_hs[test_layer['layer_ix']], 2)\n",
    "    pca_plot_df =\\\n",
    "        pd.concat([pd.DataFrame({'d1': pca_res[:, 0], 'd2': pca_res[:, 1]}), test_layer['sample_df']], axis = 1)\\\n",
    "        .pipe(lambda df: df[df['expert'].isin(list(range(0, 5)))])\\\n",
    "        .assign(expert = lambda df: df['expert'].astype(str))\\\n",
    "        .sample(2500)\\\n",
    "        .sort_values(by = 'expert')\n",
    "    plot_manifold(pca_plot_df, 'expert', 'token', f\"<em>H<sub>{str(test_layer['layer_ix'])}</sub></em>\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1564a7-8c70-418e-8116-af517fcfed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "H_orth vs H_para by expert\n",
    "\"\"\"\n",
    "for test_layer in test_layers:\n",
    "\n",
    "    h_para, h_orth = decompose_orthogonal(\n",
    "        all_pre_mlp_hs[test_layer['layer_ix']].to(torch.float32),\n",
    "        model.model.layers[test_layer['layer_ix']].mlp.gate.weight.to(torch.float32).detach().cpu(),\n",
    "        method = 'svd'\n",
    "    )\n",
    "    h_para = h_para.to(torch.float32)\n",
    "    h_orth = h_orth.to(torch.float32)\n",
    "    \n",
    "    h_para_pca_res = reduce_pca(h_para, 2)\n",
    "    h_orth_pca_res = reduce_pca(h_orth, 2)\n",
    "    \n",
    "    h_para_plot_df =\\\n",
    "        pd.concat([pd.DataFrame({'d1': h_para_pca_res[:, 0], 'd2': h_para_pca_res[:, 1]}), test_layer['sample_df']], axis = 1)\\\n",
    "        .sample(2500)\\\n",
    "        .sort_values(by = 'expert')\n",
    "\n",
    "    h_orth_plot_df =\\\n",
    "        pd.concat([pd.DataFrame({'d1': h_orth_pca_res[:, 0], 'd2': h_orth_pca_res[:, 1]}), test_layer['sample_df']], axis = 1)\\\n",
    "        .sample(2500)\\\n",
    "        .sort_values(by = 'expert')\n",
    "    \n",
    "    combine_plots([\n",
    "        plot_manifold(h_para_plot_df, 'source', 'token', f\"<em>H<sub>para</sub>({str(test_layer['layer_ix'])})</em>\"),\n",
    "        plot_manifold(h_orth_plot_df, 'source', 'token', f\"<em>H<sub>orth</sub>({str(test_layer['layer_ix'])})</em>\")\n",
    "    ]).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eccd73-0e82-4d4d-af48-ebf19f86cce7",
   "metadata": {},
   "source": [
    "## Functional Specialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a07a4f-3b15-43cb-ac21-ff893b0adb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Print available layers\n",
    "\"\"\"\n",
    "list(all_expert_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f658114-dd97-4cc0-92aa-7472cfdf4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_t1_exp_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593986d-0613-4dd6-9186-847f2335dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get test samples (pre and post-MLP) for experts where expert was top-1\n",
    "\"\"\"\n",
    "test_layer_ix = 11\n",
    "\n",
    "test_sample_df = get_sample_df_for_layer(sample_df, topk1_df, test_layer_ix)\n",
    "\n",
    "test_exp_inputs = all_pre_mlp_hs[test_layer_ix][:, :]\n",
    "test_t1_exp_outputs = all_expert_outputs[test_layer_ix][:, 0, :]\n",
    "\n",
    "test_deltas = test_t1_exp_outputs - test_exp_inputs\n",
    "print(f\"{test_exp_inputs.shape} | {test_t1_exp_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d8dd1-bc5f-4e5a-b115-434bbd445cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Show PCA of functional transformation\n",
    "\"\"\"\n",
    "layer_pca = reduce_pca(test_deltas, 2)\n",
    "test_sample_df_with_pca = test_sample_df.assign(d1 = layer_pca[:, 0], d2 = layer_pca[:, 1])\n",
    "\n",
    "# Experts to test\n",
    "test_experts =\\\n",
    "    test_sample_df\\\n",
    "    .groupby('expert', as_index = False).agg(n = ('expert', 'count'))\\\n",
    "    .sort_values('n', ascending = False)\\\n",
    "    .head(5)['expert'].tolist()\n",
    "\n",
    "# Plot across experts\n",
    "pca_plot_df =\\\n",
    "    test_sample_df_with_pca.pipe(lambda df: df[df['expert'].isin(test_experts)])\\\n",
    "    .sample(5_000)\\\n",
    "    .assign(expert = lambda df: df['expert'].astype(str))\n",
    "\n",
    "combine_plots([\n",
    "    plot_manifold(pca_plot_df.sort_values('expert'), 'expert', 'token', title = 'Deltas by expert'),\n",
    "    plot_manifold(pca_plot_df.sort_values('source'), 'source', 'token', title = 'Deltas by source')\n",
    "], title = f'Layer {str(test_layer_ix)}').show()\n",
    "\n",
    "# Plot within experts\n",
    "for test_expert in sorted(test_experts):\n",
    "    \n",
    "    source_plot_df =\\\n",
    "         test_sample_df_with_pca.pipe(lambda df: df[df['expert'] == test_expert])\\\n",
    "        .sample(5_000)\\\n",
    "        .assign(prev_expert = lambda df: df['prev_expert'].astype(str))\n",
    "\n",
    "    t5_prev_experts =\\\n",
    "        test_sample_df_with_pca.pipe(lambda df: df[df['expert'] == test_expert])\\\n",
    "        .groupby('prev_expert', as_index = False).agg(n = ('prev_expert', 'count'))\\\n",
    "        .sort_values('n', ascending = False)\\\n",
    "        .head(5)['prev_expert'].tolist()\n",
    "\n",
    "    t5_plot_df =\\\n",
    "        test_sample_df_with_pca.pipe(lambda df: df[df['expert'] == test_expert])\\\n",
    "        .pipe(lambda df: df[df['prev_expert'].isin(t5_prev_experts)])\\\n",
    "        .assign(prev_expert = lambda df: df['prev_expert'].astype(str))\n",
    "    \n",
    "    combine_plots([\n",
    "        plot_manifold(t5_plot_df.sort_values('prev_expert'), 'prev_expert', 'token', title = 'Deltas by prev expert'),\n",
    "        plot_manifold(source_plot_df.sort_values('source'), 'source', 'token', title = 'Deltas by source')\n",
    "    ], title = f'Layer {str(test_layer_ix)}, Expert {str(test_expert)}').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b92cb4-40aa-4438-816f-c68b154265c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_expert_pca_res\n",
    "del test_expert_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b03d8-cfeb-4378-975b-31149e431b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_expert_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a302e0-b776-48b9-a1ce-fd3f54726b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grped_delta = []\n",
    "\n",
    "for grp_val in sorted(test_sample_df['source'].unique().tolist()):\n",
    "    this_grp_sample_indices = test_sample_df[test_sample_df['source'] == grp_val]['sample_ix'].tolist()\n",
    "    if len(this_grp_sample_indices) <= 10:\n",
    "        continue\n",
    "    this_grp_deltas = test_deltas[test_expert_indices, :]\n",
    "    grped_deltas.append({\n",
    "        'grp': grp_val,\n",
    "        'grped_vals': this_grp_deltas.mean(dim = 0)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768be9e3-8e82-41b5-ad53-c92ffd15e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_deltas[0:10,:].mean(dim = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22834d-ac72-4def-9ef9-00f68090fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test_layer_ix = 11\n",
    "\n",
    "test_sample_df =\\\n",
    "    get_sample_df_for_layer(sample_df, topk1_df, test_layer_ix)\\\n",
    "    .pipe(lambda df: df[df['expert'] == test_expert])\n",
    "\n",
    "test_sample_indices = test_sample_df['sample_ix'].tolist()\n",
    "display(test_sample_df)\n",
    "\n",
    "test_t1_exp_outputs = all_expert_outputs[test_layer_ix][test_sample_indices, 0, :]\n",
    "test_t1_exp_inputs = all_pre_mlp_hs[test_layer_ix][test_sample_indices, :]\n",
    "\n",
    "print(f\"{test_t1_exp_inputs.shape} | {test_t1_exp_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a7bb1-92c4-4956-999c-15b6f249ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Top-1 vs top-2 behavior\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866413bd",
   "metadata": {},
   "source": [
    "## SVD clustering: sideways -> h_orth\n",
    "1. Decompose the pre-mlp hidden states into h_sideways WITHIN each group of activations that route to a single expert, with respect to just the D-dimensional routing gate for that single expert. Then, still within that single expert, cluster those h_sideways activations.\n",
    "2. Repeat across all experts.\n",
    "3. After obtaining these cluters, do h_orth (the regular decomposition using all activations with respect to the entire routing gate).\n",
    "4. Extract calculate the cluster centroids from h_orth, using the cluster ids/labels extracted from h_sideways earlier. This results in n_experts * n_clusters_per_expert cluster centers.\n",
    "5. Calculate (using cosine similarity?) the within-expert against across-expert averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib, utils.svd as svd\n",
    "# decompose_sideways = importlib.reload(svd).decompose_sideways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e7b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 512):\n",
    "    kmeans_model = cuml.cluster.KMeans(n_clusters = n_clusters, max_iter = 1000, random_state = 123)\n",
    "    kmeans_model.fit(cupy.asarray(layer_hs.to(torch.float32)))\n",
    "    clear_all_cuda_memory(False)\n",
    "    cluster_ids = kmeans_model.labels_.tolist() # n_samples\n",
    "    cluster_centers = kmeans_model.cluster_centers_ # (n_clusters, D)\n",
    "    return cluster_ids, cluster_centers\n",
    "\n",
    "cluster_kmeans(relevant_pre_mlp_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare sample-level df merged with top-1 expert selections for a single test layer\n",
    "\"\"\"\n",
    "\n",
    "test_layer_ix = 1\n",
    "\n",
    "sample_df_test =\\\n",
    "    sample_df\\\n",
    "    .merge(\n",
    "        topk1_df.pipe(lambda df: df[df['layer_ix'] == test_layer_ix])[['expert', 'sample_ix']],\n",
    "        how = 'inner',\n",
    "        on = 'sample_ix'\n",
    "    )\\\n",
    "    .merge(\n",
    "        topk1_df.pipe(lambda df: df[df['layer_ix'] == test_layer_ix - 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']],\n",
    "        how = 'inner',\n",
    "        on = 'sample_ix'\n",
    "    )\n",
    "\n",
    "sample_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd21bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the sideways decomposition within activations routed to a single expert; this specifically REMOVES the part of h directly \n",
    "responsible for increasing/decreasing logit specifically for that expert; then cluster them\n",
    "\"\"\"\n",
    "cluster_ids = torch.full([all_pre_mlp_hs[test_layer_ix].shape[0]], -1, dtype = torch.int32)\n",
    "\n",
    "for this_expert in tqdm(sorted(sample_df_test['expert'].unique().tolist())):\n",
    "\n",
    "    # Extract sample indices for expert\n",
    "    this_sample_indices = sample_df_test[sample_df_test['expert'] == this_expert]['sample_ix'].tolist()\n",
    "    \n",
    "    # D-dimensional routing gate for expert route\n",
    "    this_gate = model.model.layers[test_layer_ix].mlp.gate.weight[this_expert, :].to(torch.float32).detach().cpu()\n",
    "    \n",
    "    # Remove only this expert’s axis to expose sub‑clusters\n",
    "    _, h_side = decompose_sideways(all_pre_mlp_hs[test_layer_ix][this_sample_indices], this_gate)\n",
    "\n",
    "    # Cluster within expert\n",
    "    this_cluster_ids, _ = cluster_kmeans(h_side, n_clusters = 10)\n",
    "    cluster_ids[this_sample_indices] = torch.tensor(this_cluster_ids, dtype = cluster_ids.dtype)\n",
    "\n",
    "sample_df_test_cl = sample_df_test.assign(cluster_id = cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da61ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Go back to the original decomposition to get the regular h_orth\n",
    "\"\"\"\n",
    "_, h_orth = decompose_orthogonal(\n",
    "    all_pre_mlp_hs[test_layer_ix].to(torch.float32),\n",
    "    model.model.layers[test_layer_ix].mlp.gate.weight.to(torch.float32).detach().cpu(),\n",
    "    method = 'svd'\n",
    ")\n",
    "h_orth = h_orth.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909823d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply the sub-cluster labels obtained from clustering h_sideways to the corresponding h_orth vectors\n",
    "This allows us to compare things in h_orth space; then compare cosine similarity. \n",
    "\"\"\"\n",
    "centroids = [] # List of np centroids\n",
    "tags = [] # (expert, cluster)\n",
    "\n",
    "for this_expert in sorted(sample_df_test_cl['expert'].unique().tolist()):\n",
    "    for this_cluster in [x for x in sorted(sample_df_test_cl['cluster_id'].unique().tolist()) if x != -1]: # Get clusters\n",
    "        this_e_c_sample_indices =\\\n",
    "            sample_df_test_cl\\\n",
    "            .pipe(lambda df: df[(df['expert'] == this_expert) & (df['cluster_id'] == this_cluster)])\\\n",
    "            ['sample_ix'].tolist()\n",
    "        \n",
    "        if len(this_e_c_sample_indices) <= 50: # Throw out tiny clusters\n",
    "            continue\n",
    "\n",
    "        v = h_orth[this_e_c_sample_indices].mean(0)\n",
    "        v = v / v.norm() # normalise for cosine\n",
    "        centroids.append(v)\n",
    "        tags.append((int(this_expert), int(this_cluster)))\n",
    "\n",
    "centroids = torch.stack(centroids) # this_expert * K x D\n",
    "cosine_sims = sklearn.metrics.pairwise.cosine_similarity(centroids.numpy()) # this_expert * K x this_expert * K; pairwise sim between each expert-cluster pair\n",
    "\n",
    "within, across = [], []\n",
    "for i, (e_i, _) in enumerate(tags):\n",
    "    for j, (e_j, _) in enumerate(tags):\n",
    "        if i >= j: continue # upper‑tri only\n",
    "        (within if e_i==e_j else across).append(cosine_sims[i, j])\n",
    "\n",
    "mean_cos_within  = float(np.mean(within))\n",
    "mean_cos_across  = float(np.mean(across))\n",
    "print(f\"mean cosine  (within expert):  {mean_cos_within:.3f}\")\n",
    "print(f\"mean cosine  (across experts): {mean_cos_across:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae135d00",
   "metadata": {},
   "source": [
    "## SVD clustering: h_orth vs h_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare sample-level df merged with top-1 expert selections for a single test layer\n",
    "\"\"\"\n",
    "\n",
    "test_layer_ix = 10\n",
    "\n",
    "topk1_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\n",
    "\n",
    "sample_df_test =\\\n",
    "    sample_df\\\n",
    "    .merge(\n",
    "        topk1_df.pipe(lambda df: df[df['layer_ix'] == test_layer_ix])[['expert', 'sample_ix']],\n",
    "        how = 'inner',\n",
    "        on = 'sample_ix'\n",
    "    )\\\n",
    "    .merge(\n",
    "        topk1_df.pipe(lambda df: df[df['layer_ix'] == test_layer_ix - 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']],\n",
    "        how = 'inner',\n",
    "        on = 'sample_ix'\n",
    "    )\n",
    "\n",
    "sample_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Go back to the original decomposition to get the regular h_orth\n",
    "\"\"\"\n",
    "h_para, h_orth = decompose_orthogonal(\n",
    "    all_pre_mlp_hs[test_layer_ix].to(torch.float32),\n",
    "    model.model.layers[test_layer_ix].mlp.gate.weight.to(torch.float32).detach().cpu(),\n",
    "    method = 'svd'\n",
    ")\n",
    "h_para = h_para.to(torch.float32)\n",
    "h_orth = h_orth.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot PCA + UMAP, color by expert id\n",
    "\"\"\"\n",
    "def reduce_pca(input_tensor: torch.Tensor, n_components = 2):\n",
    "    hs_cupy = cupy.asarray(input_tensor.to(torch.float32))\n",
    "    model = cuml.PCA(\n",
    "        iterated_power = 100,\n",
    "        n_components = n_components,\n",
    "        verbose = True\n",
    "    )\n",
    "    model.fit(hs_cupy)\n",
    "    # print(f'Explained variance ratio: {model.explained_variance_ratio_}')\n",
    "    print(f'Cumulative variance ratio: {np.cumsum(model.explained_variance_ratio_)[-1]}')\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return pred\n",
    "\n",
    "def reduce_umap(input_tensor: torch.Tensor, n_components = 2, metric = 'cosine', n_epochs = 200):\n",
    "    hs_cupy = cupy.asarray(input_tensor.to(torch.float32))\n",
    "    model = cuml.UMAP(\n",
    "        n_components = n_components, \n",
    "        n_neighbors = 20, # 15 for default, smaller = more local data preserved [2 - 100]\n",
    "        metric = metric, # euclidean, cosine, manhattan, l2, hamming\n",
    "        min_dist = 0.5, # 0.1 by default, effective distance between embedded points\n",
    "        n_epochs = n_epochs, # 200 by default for large datasets\n",
    "        random_state = 123, # Allow parallelism\n",
    "        verbose = False\n",
    "    )\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return pred\n",
    "\n",
    "def plot_manifold(reduced_np, sample_df):\n",
    "    px.scatter(\n",
    "        pd.concat([pd.DataFrame({'d1': reduced_np[:, 0], 'd2': reduced_np[:, 1]}), sample_df], axis = 1)\\\n",
    "            .sample(5000)\\\n",
    "            .assign(prev_expert = lambda df: df['expert'].astype(str)),\n",
    "        x = 'd1', y = 'd2', color = 'expert', hover_data = ['token']\n",
    "    ).show()\n",
    "\n",
    "#pca_res = reduce_pca(h_para, 2)\n",
    "plot_manifold(pca_res, sample_df_test)\n",
    "\n",
    "# plot_reduction(relevant_pre_mlp_hs, relevant_samples_df, reduce_umap, 2, 'cosine', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67598c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(pca_res, sample_df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
