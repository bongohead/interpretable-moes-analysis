{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to test orthogonality of expert specialization.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18084712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "from utils.svd import decompose_orthogonal, decompose_sideways\n",
    "from utils.vis import combine_plots\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10fc64",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d50e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_id = 'allenai/OLMoE-1B-7B-0125-Instruct'\n",
    "model_prefix = 'olmoe'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ebf8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix):\n",
    "    all_pre_mlp_hs = torch.load(f'data/{model_prefix}/all-pre-mlp-hidden-states.pt')\n",
    "    all_expert_outputs = torch.load(f'data/{model_prefix}/all-expert-outputs.pt')\n",
    "    with open(f'data/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    return all_pre_mlp_hs, all_expert_outputs, metadata['sample_df'], metadata['topk_df'], metadata['all_pre_mlp_hidden_states_layers'], metadata['all_expert_outputs_layers']\n",
    "\n",
    "all_pre_mlp_hs_import, all_expert_outputs_import, sample_df_import, topk_df_import, act_map, expert_map = load_data(model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6db4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\n",
    "\n",
    "topk1_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "def get_sample_df_for_layer(sample_df, topk_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Helper to take the sample df and merge layer-level expert selection information\n",
    "    \"\"\"\n",
    "    topk_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix])\n",
    "    topk_last_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 1])\n",
    "    layer_df =\\\n",
    "        sample_df\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 1])[['sample_ix', 'expert']], how = 'inner', on = 'sample_ix')\\\n",
    "        .merge(topk_last_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 2]).rename(columns = {'expert': 'expert2'})[['sample_ix', 'expert2']], how = 'left', on = 'sample_ix')\n",
    "    \n",
    "    return layer_df\n",
    "\n",
    "del sample_df_import, topk_df_import\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16dccba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {layer_ix: all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "all_expert_outputs = all_expert_outputs_import.to(torch.float16)\n",
    "compare_bf16_fp16_batched(all_expert_outputs_import, all_expert_outputs)\n",
    "del all_expert_outputs_import\n",
    "all_expert_outputs = {layer_ix: all_expert_outputs[:, save_ix, :, :] for save_ix, layer_ix in enumerate(expert_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dcd749d-f001-453a-88bc-09284f76603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Misc visualization helpers\n",
    "\"\"\"\n",
    "def reduce_pca(input_tensor: torch.Tensor, n_components = 2):\n",
    "    hs_cupy = cupy.asarray(input_tensor.to(torch.float32))\n",
    "    model = cuml.PCA(\n",
    "        iterated_power = 100,\n",
    "        n_components = n_components,\n",
    "        verbose = True\n",
    "    )\n",
    "    model.fit(hs_cupy)\n",
    "    # print(f'Explained variance ratio: {model.explained_variance_ratio_}')\n",
    "    print(f'Cumulative variance ratio: {np.cumsum(model.explained_variance_ratio_)[-1]}')\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return pred\n",
    "\n",
    "def reduce_umap(input_tensor: torch.Tensor, n_components = 2, metric = 'cosine', n_epochs = 200):\n",
    "    hs_cupy = cupy.asarray(input_tensor.to(torch.float32))\n",
    "    model = cuml.UMAP(\n",
    "        n_components = n_components, \n",
    "        n_neighbors = 20, # 15 for default, smaller = more local data preserved [2 - 100]\n",
    "        metric = metric, # euclidean, cosine, manhattan, l2, hamming\n",
    "        min_dist = 0.2, # 0.1 by default, effective distance between embedded points\n",
    "        n_epochs = n_epochs, # 200 by default for large datasets\n",
    "        random_state = None, # Allow parallelism\n",
    "        verbose = False\n",
    "    )\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return pred\n",
    "\n",
    "def plot_manifold(plot_df, color_col, hover_col, title = None):\n",
    "    plot = px.scatter(\n",
    "        plot_df,\n",
    "        x = 'd1', y = 'd2', color = color_col, hover_data = [hover_col],\n",
    "        title = title, opacity = 0.9\n",
    "    ).update_layout(autosize = False, height = 400).update_traces(marker = dict(size = 5))\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04e099",
   "metadata": {},
   "source": [
    "## Visualize activation clusters of single (layer, expert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa5e15-0339-4bee-bad7-b989d62fe636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize clusters\n",
    "\"\"\"\n",
    "test_layer_ix = 9\n",
    "test_expert = 1\n",
    "\n",
    "test_sample_df =\\\n",
    "    get_sample_df_for_layer(sample_df, topk_df, test_layer_ix)\\\n",
    "    .pipe(lambda df: df[df['expert'] == test_expert])\n",
    "\n",
    "test_sample_indices = test_sample_df['sample_ix'].tolist()\n",
    "\n",
    "display(test_sample_df)\n",
    "\n",
    "test_pre_mlp_hs = all_pre_mlp_hs[test_layer_ix][test_sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot PCA + UMAP WITHIN test expert only, color by previous expert ID\n",
    "\"\"\"\n",
    "pca_res = reduce_pca(test_pre_mlp_hs, 2)\n",
    "pca_plot_df =\\\n",
    "    test_sample_df.assign(d1 = pca_res[:, 0], d2 = pca_res[:, 1])\\\n",
    "    .sample(5000)\\\n",
    "    .assign(prev_expert = lambda df: df['prev_expert'].astype(str))\n",
    "\n",
    "combine_plots([\n",
    "    plot_manifold(pca_plot_df, 'prev_expert', 'token', title = 'By previous expert'),\n",
    "    plot_manifold(pca_plot_df, 'source', 'token', title = 'By source')\n",
    "], f\"PCA for layer {test_layer_ix}, expert {test_expert}\").show()\n",
    "\n",
    "umap_res = reduce_umap(test_pre_mlp_hs, 2, 'cosine', n_epochs = 2000)\n",
    "umap_plot_df =\\\n",
    "    test_sample_df.assign(d1 = umap_res[:, 0], d2 = umap_res[:, 1])\\\n",
    "    .sample(5000)\\\n",
    "    .assign(prev_expert = lambda df: df['prev_expert'].astype(str))\n",
    "\n",
    "combine_plots([\n",
    "    plot_manifold(umap_plot_df, 'prev_expert', 'token', title = 'By previous expert'),\n",
    "    plot_manifold(umap_plot_df, 'source', 'token', title = 'By source')\n",
    "], f\"UMAP for layer {test_layer_ix}, expert {test_expert}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8fae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot PCA + UMAP, but this time include token samples from outside this expert\n",
    "\"\"\"\n",
    "nontest_sample_df =\\\n",
    "    get_sample_df_for_layer(sample_df, topk_df, test_layer_ix)\\\n",
    "    .pipe(lambda df: df[df['expert'] != test_expert])\n",
    "\n",
    "nontest_pre_mlp_hs = all_pre_mlp_hs[test_layer_ix][nontest_sample_df['sample_ix'].tolist()]\n",
    "\n",
    "# Prep all_samples_df samples df\n",
    "all_samples_df = pd.concat([\n",
    "    test_sample_df.assign(prev_expert = lambda df: df['prev_expert'].astype(str)), \n",
    "    nontest_sample_df.assign(prev_expert = 'NA')\n",
    "]).reset_index(drop = True)\n",
    "\n",
    "pca_res = reduce_pca(torch.concat([test_pre_mlp_hs, nontest_pre_mlp_hs], dim = 0), 2)\n",
    "pca_plot_df =\\\n",
    "    all_samples_df.assign(d1 = pca_res[:, 0], d2 = pca_res[:, 1])\\\n",
    "    .pipe(lambda df: pd.concat([\n",
    "        df[df['prev_expert'] == 'NA'].sample(10_000),\n",
    "        df[df['prev_expert'] != 'NA'].sample(10_000)\n",
    "    ]))\n",
    "\n",
    "combine_plots([\n",
    "    plot_manifold(pca_plot_df, 'prev_expert', 'token', title = 'By previous expert'),\n",
    "    plot_manifold(pca_plot_df, 'source', 'token', title = 'By source')\n",
    "], f\"PCA for layer {test_layer_ix}, expert {test_expert}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347bb496",
   "metadata": {},
   "source": [
    "## Cross-layer source mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot cross-layer source mappings\n",
    "\"\"\"\n",
    "layers_to_test = [0, 2, 7, 13, 15]\n",
    "\n",
    "test_layers = [\n",
    "    {'layer_ix': layer_ix, 'sample_df': get_sample_df_for_layer(sample_df, topk_df, layer_ix)}\n",
    "    for layer_ix in layers_to_test\n",
    "]\n",
    "\n",
    "test_layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "H groupings\n",
    "\"\"\"\n",
    "for test_layer in test_layers:\n",
    "    pca_res = reduce_pca(all_pre_mlp_hs[test_layer['layer_ix']], 2)\n",
    "\n",
    "    source_plot_df =\\\n",
    "        test_layer['sample_df'].assign(d1 = pca_res[:, 0], d2 = pca_res[:, 1])\\\n",
    "        .sample(2500)\\\n",
    "        .sort_values(by = 'source')\n",
    "\n",
    "    expert_plot_df =\\\n",
    "        pd.concat([pd.DataFrame({'d1': pca_res[:, 0], 'd2': pca_res[:, 1]}), test_layer['sample_df']], axis = 1)\\\n",
    "        .pipe(lambda df: df[df['expert'].isin(list(range(0, 5)))])\\\n",
    "        .assign(expert = lambda df: df['expert'].astype(str))\\\n",
    "        .sample(2500)\\\n",
    "        .sort_values(by = 'expert')\n",
    "    \n",
    "    combine_plots([\n",
    "        plot_manifold(expert_plot_df, 'expert', 'token', title = 'By expert'),\n",
    "        plot_manifold(source_plot_df, 'source', 'token', title = 'By source')\n",
    "    ], f\"<em>H<sub>{str(test_layer['layer_ix'])}</sub></em>\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1564a7-8c70-418e-8116-af517fcfed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "H_orth vs H_para by expert/source\n",
    "\"\"\"\n",
    "for test_layer in test_layers:\n",
    "\n",
    "    h_para, h_orth = decompose_orthogonal(\n",
    "        all_pre_mlp_hs[test_layer['layer_ix']].to(torch.float32),\n",
    "        model.model.layers[test_layer['layer_ix']].mlp.gate.weight.to(torch.float32).detach().cpu(),\n",
    "        method = 'svd'\n",
    "    )\n",
    "    h_para = h_para.to(torch.float32)\n",
    "    h_orth = h_orth.to(torch.float32)\n",
    "    \n",
    "    h_para_pca_res = reduce_pca(h_para, 2)\n",
    "    h_orth_pca_res = reduce_pca(h_orth, 2)\n",
    "    \n",
    "    h_para_plot_df =\\\n",
    "        test_layer['sample_df'].assign(d1 = h_para_pca_res[:, 0], d2 = h_para_pca_res[:, 1])\\\n",
    "        .sample(2500)\\\n",
    "        .sort_values(by = 'expert')\n",
    "\n",
    "    h_orth_plot_df =\\\n",
    "        test_layer['sample_df'].assign(d1 = h_orth_pca_res[:, 0], d2 = h_orth_pca_res[:, 1])\\\n",
    "        .sample(2500)\\\n",
    "        .sort_values(by = 'expert')\n",
    "    \n",
    "    combine_plots([\n",
    "        plot_manifold(h_para_plot_df, 'source', 'token', f\"<em>H<sub>para</sub>({str(test_layer['layer_ix'])})</em>\"),\n",
    "        plot_manifold(h_orth_plot_df, 'source', 'token', f\"<em>H<sub>orth</sub>({str(test_layer['layer_ix'])})</em>\")\n",
    "    ]).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eccd73-0e82-4d4d-af48-ebf19f86cce7",
   "metadata": {},
   "source": [
    "## Functional Specialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a07a4f-3b15-43cb-ac21-ff893b0adb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Print available layers\n",
    "\"\"\"\n",
    "list(all_expert_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c593986d-0613-4dd6-9186-847f2335dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get test samples (pre and post-MLP) for experts where expert was top-1\n",
    "\"\"\"\n",
    "test_layer_ix = 7\n",
    "\n",
    "test_sample_df = get_sample_df_for_layer(sample_df, topk_df, test_layer_ix)\n",
    "\n",
    "test_exp_inputs = all_pre_mlp_hs[test_layer_ix][:, :]\n",
    "test_t1_exp_outputs = all_expert_outputs[test_layer_ix][:, 0, :]\n",
    "\n",
    "test_deltas = (test_t1_exp_outputs - test_exp_inputs)\n",
    "print(f\"{test_exp_inputs.shape} | {test_t1_exp_outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "775d8dd1-bc5f-4e5a-b115-434bbd445cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Show PCA of functional transformations with various groupings\n",
    "\"\"\"\n",
    "layer_pca = reduce_pca(test_deltas, 2)\n",
    "test_sample_df_with_pca = test_sample_df.assign(d1 = layer_pca[:, 0], d2 = layer_pca[:, 1])\n",
    "\n",
    "# Experts to test\n",
    "test_experts =\\\n",
    "    test_sample_df\\\n",
    "    .groupby('expert', as_index = False).agg(n = ('expert', 'count'))\\\n",
    "    .sort_values('n', ascending = False)\\\n",
    "    .head(5)['expert'].tolist()\n",
    "\n",
    "# Plot across experts\n",
    "pca_plot_df =\\\n",
    "    test_sample_df_with_pca.pipe(lambda df: df[df['expert'].isin(test_experts)])\\\n",
    "    .sample(5_000)\\\n",
    "    .assign(expert = lambda df: df['expert'].astype(str))\n",
    "\n",
    "combine_plots([\n",
    "    plot_manifold(pca_plot_df.sort_values('expert'), 'expert', 'token', title = 'Deltas by expert'),\n",
    "    plot_manifold(pca_plot_df.sort_values('source'), 'source', 'token', title = 'Deltas by source')\n",
    "], title = f'Layer {str(test_layer_ix)}').show()\n",
    "\n",
    "# Plot within experts\n",
    "for test_expert in sorted(test_experts):\n",
    "\n",
    "    test_expert_sample_df = test_sample_df_with_pca.pipe(lambda df: df[df['expert'] == test_expert])\n",
    "    \n",
    "    source_plot_df =\\\n",
    "        test_expert_sample_df\\\n",
    "        .sample(5_000)\\\n",
    "        .assign(prev_expert = lambda df: df['prev_expert'].astype(str))\n",
    "\n",
    "    top_prev_experts =\\\n",
    "        test_expert_sample_df\\\n",
    "        .groupby('prev_expert', as_index = False).agg(n = ('prev_expert', 'count'))\\\n",
    "        .sort_values('n', ascending = False)\\\n",
    "        .head(5)['prev_expert'].tolist()\n",
    "\n",
    "    prev_experts_plot_df =\\\n",
    "        test_expert_sample_df\\\n",
    "        .pipe(lambda df: df[df['prev_expert'].isin(top_prev_experts)])\\\n",
    "        .sample(5_000)\\\n",
    "        .assign(prev_expert = lambda df: df['prev_expert'].astype(str))\n",
    "\n",
    "    top_expert_2s =\\\n",
    "        test_expert_sample_df\\\n",
    "        .groupby('expert2', as_index = False).agg(n = ('expert2', 'count'))\\\n",
    "        .sort_values('n', ascending = False)\\\n",
    "        .head(5)['expert2'].tolist()\n",
    "\n",
    "    expert_2s_plot_df =\\\n",
    "        test_expert_sample_df\\\n",
    "        .pipe(lambda df: df[df['expert2'].isin(top_expert_2s)])\\\n",
    "        .sample(5_000)\\\n",
    "        .assign(expert2 = lambda df: df['expert2'].astype(str))\n",
    "    \n",
    "    combine_plots([\n",
    "        plot_manifold(prev_experts_plot_df.sort_values('prev_expert'), 'prev_expert', 'token', title = 'Deltas by prev expert'),\n",
    "        plot_manifold(expert_2s_plot_df.sort_values('expert2'), 'expert2', 'token', title = 'Deltas by topk = 2 expert'),\n",
    "        plot_manifold(source_plot_df.sort_values('source'), 'source', 'token', title = 'Deltas by source'),\n",
    "    ], title = f'PCA for layer {str(test_layer_ix)}, expert {str(test_expert)}', cols = 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "02a302e0-b776-48b9-a1ce-fd3f54726b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate cosine similarity across languages\n",
    "\"\"\"\n",
    "grped_deltas = []\n",
    "\n",
    "# Iterate over groups\n",
    "for grp_val in sorted(test_sample_df['source'].unique().tolist()):\n",
    "    \n",
    "    this_grp_sample_indices = test_sample_df[test_sample_df['source'] == grp_val]['sample_ix'].tolist()\n",
    "    if len(this_grp_sample_indices) <= 10:\n",
    "        continue\n",
    "        \n",
    "    this_grp_deltas = test_deltas[this_grp_sample_indices, :]\n",
    "    \n",
    "    grped_deltas.append({\n",
    "        'grp': grp_val,\n",
    "        'grped_vals': this_grp_deltas.mean(dim = 0)\n",
    "    })\n",
    "\n",
    "cos_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    torch.stack([x['grped_vals'] for x in grped_deltas], dim = 0).numpy()\n",
    ")\n",
    "\n",
    "upper_triangle_indices = np.triu_indices(cos_sim.shape[0], k = 1)\n",
    "off_diagonal_values = cos_sim[upper_triangle_indices]\n",
    "np.mean(np.abs(off_diagonal_values)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "92ad178c-ae25-4708-bf73-dc5f9cf18728",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate cosine similarity across experts\n",
    "\"\"\"\n",
    "grped_deltas = []\n",
    "\n",
    "# Iterate over groups\n",
    "for grp_val in sorted(test_sample_df['expert'].unique().tolist()):\n",
    "    \n",
    "    this_grp_sample_indices = test_sample_df[test_sample_df['expert'] == grp_val]['sample_ix'].tolist()\n",
    "    if len(this_grp_sample_indices) <= 10:\n",
    "        continue\n",
    "        \n",
    "    this_grp_deltas = test_deltas[this_grp_sample_indices, :]\n",
    "    \n",
    "    grped_deltas.append({\n",
    "        'grp': grp_val,\n",
    "        'grped_vals': this_grp_deltas.mean(dim = 0)\n",
    "    })\n",
    "\n",
    "cos_sim = sklearn.metrics.pairwise.cosine_similarity(\n",
    "    torch.stack([x['grped_vals'] for x in grped_deltas], dim = 0).numpy()\n",
    ")\n",
    "upper_triangle_indices = np.triu_indices(cos_sim.shape[0], k = 1)\n",
    "off_diagonal_values = cos_sim[upper_triangle_indices]\n",
    "np.mean(np.abs(off_diagonal_values)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "307c37ee-dd52-4170-b2d5-0aedf9a11604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate cosine similarity across expert-source combinations\n",
    "\"\"\"\n",
    "grped_deltas = []\n",
    "\n",
    "col = 'prev_expert' # Use source, prev_expert, expert2\n",
    "\n",
    "exp_src_combinations =\\\n",
    "    test_sample_df\\\n",
    "    .groupby(['expert', col], as_index = False)\\\n",
    "    .agg(\n",
    "        size = ('sample_ix', 'count'),\n",
    "        sample_indices = ('sample_ix', list)\n",
    "    )\\\n",
    "    .pipe(lambda df: df[df['size'] > 100])\n",
    "\n",
    "means = []\n",
    "for comb in exp_src_combinations.to_dict('records'):\n",
    "    means.append(test_deltas[comb['sample_indices']].mean(dim = 0).numpy())\n",
    "\n",
    "exp_src_combinations = exp_src_combinations.assign(mean_vec = means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8fcbdb3c-01c7-41a9-8e0e-772807f2cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Across expert within lang, across lang within expert\n",
    "\"\"\"\n",
    "src_sims = {}\n",
    "for src in exp_src_combinations[col].unique().tolist():\n",
    "    all_vecs_for_src = exp_src_combinations.pipe(lambda df: df[df[col] == src])['mean_vec'].tolist()\n",
    "    if len(all_vecs_for_src) < 3:\n",
    "        continue\n",
    "    cos_sim = sklearn.metrics.pairwise_distances(np.stack(all_vecs_for_src, axis = 0), metric = 'cosine')\n",
    "    off_diag = cos_sim[np.triu_indices(cos_sim.shape[0], k = 1)]\n",
    "    src_sims[src] = 1 - np.mean(np.abs(off_diag)).item()\n",
    "# print(src_sims)\n",
    "\n",
    "exp_sims = {}\n",
    "for exp in exp_src_combinations['expert'].unique().tolist():\n",
    "    all_vecs_for_exp = exp_src_combinations.pipe(lambda df: df[df['expert'] == exp])['mean_vec'].tolist()\n",
    "    if len(all_vecs_for_exp) < 3:\n",
    "        continue\n",
    "    cos_sim = sklearn.metrics.pairwise_distances(np.stack(all_vecs_for_exp, axis = 0), metric = 'cosine')\n",
    "    off_diag = cos_sim[np.triu_indices(cos_sim.shape[0], k = 1)]\n",
    "    exp_sims[exp] = 1 - np.mean(np.abs(off_diag)).item()\n",
    "# print(exp_sims)\n",
    "\n",
    "mean_src_sim = np.mean([v for _, v in src_sims.items()]).round(4)\n",
    "mean_exp_sim = np.mean([v for _, v in exp_sims.items()]).round(4)\n",
    "\n",
    "print(f\"Average delta sim across experts within {col} {mean_src_sim}\")\n",
    "print(f\"Average delta sim across {col} within experts {mean_exp_sim}\")\n",
    "if mean_src_sim > mean_exp_sim:\n",
    "    print(f'Delta is dominated by {col} over expert')\n",
    "else:\n",
    "    print(f'Delta is dominated by expert over {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e683cef3-c078-4fc4-949b-e9a88cbb3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Assuming test_sample_df and test_deltas are pre-loaded and aligned\n",
    "# test_sample_df: DataFrame with columns 'sample_ix', 'expert', 'prev_expert', 'source', etc.\n",
    "# test_deltas: Tensor (N, D) of delta vectors corresponding to test_sample_df\n",
    "\n",
    "def get_baselines(sample_df, delta_vectors, grouping_col, n_shuffles: int = 5, min_samples_per_group: int = 100, min_groups_for_sim_calc: int = 3) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculates the average shuffled baseline similarities over multiple runs.\n",
    "\n",
    "    Params:\n",
    "        @sample_df: Dataframe linking samples to experts and the grouping_col\n",
    "        @delta_vectors: n_samples x D of tensor corresponding to sample_df\n",
    "        @grouping_col: The column name to group by ('prev_expert', 'source', 'etc').\n",
    "        @n_shuffles: Number of shuffling iterations.\n",
    "        @min_samples_per_group: Min samples required to compute a group mean.\n",
    "        @min_groups_for_sim_calc: Min number of mean vectors required to calculate an average similarity.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - avg_baseline_sim_across_experts: Baseline for \"sim across experts within col\".\n",
    "        - avg_baseline_sim_across_col: Baseline for \"sim across col within experts\".\n",
    "    \"\"\"\n",
    "    baseline_sims_across_experts = []\n",
    "    baseline_sims_across_col = []\n",
    "\n",
    "    # Ensure float32 for calculations\n",
    "    delta_vectors_np = delta_vectors.float().cpu().numpy()\n",
    "\n",
    "    print(f\"Calculating baselines for grouping col: '{grouping_col}'...\")\n",
    "    for i in tqdm(range(n_shuffles), desc=\"Shuffle Runs\"):\n",
    "        # --- Baseline 1: Shuffle Expert labels WITHIN each grouping_col group ---\n",
    "        df_shuffled_experts = sample_df.copy()\n",
    "        # Shuffle 'expert' conditionally within each group defined by 'grouping_col'\n",
    "        df_shuffled_experts['shuffled_expert'] = df_shuffled_experts.groupby(grouping_col)['expert']\\\n",
    "            .transform(lambda x: x.sample(frac=1, random_state=i).values)\n",
    "\n",
    "        # Calculate mean deltas based on (shuffled_expert, grouping_col)\n",
    "        group_means_shuffled_e = df_shuffled_experts.groupby(['shuffled_expert', grouping_col])['sample_ix']\\\n",
    "            .apply(lambda idx: delta_vectors_np[idx.tolist()].mean(axis=0) if len(idx) >= min_samples_per_group else np.nan)\\\n",
    "            .dropna()\n",
    "\n",
    "        # Calculate average similarity across SHUFFLED experts within each grouping_col\n",
    "        sims_within_col = []\n",
    "        for current_col_val in df_shuffled_experts[grouping_col].unique():\n",
    "            # Get mean vectors for this specific col value, indexed by shuffled_expert\n",
    "            means_for_col = group_means_shuffled_e.loc[:, current_col_val] # Selects multi-index level\n",
    "            means_for_col = means_for_col.dropna() # Ensure we only use valid means\n",
    "            if len(means_for_col) >= min_groups_for_sim_calc:\n",
    "                vec_stack = np.stack(means_for_col.values)\n",
    "                # Cosine distance -> Cosine similarity = 1 - distance\n",
    "                cos_sim_matrix = 1 - sklearn.metrics.pairwise_distances(vec_stack, metric='cosine')\n",
    "                # Get upper triangle (excluding diagonal) for pairwise similarities\n",
    "                upper_triangle_indices = np.triu_indices(cos_sim_matrix.shape[0], k=1)\n",
    "                if len(upper_triangle_indices[0]) > 0: # Ensure there are pairs\n",
    "                    sims_within_col.append(np.mean(np.abs(cos_sim_matrix[upper_triangle_indices])))\n",
    "\n",
    "        if sims_within_col: # Only average if we got results for at least one col group\n",
    "             baseline_sims_across_experts.append(np.mean(sims_within_col))\n",
    "\n",
    "        # --- Baseline 2: Shuffle grouping_col labels WITHIN each expert group ---\n",
    "        df_shuffled_col = sample_df.copy()\n",
    "        # Shuffle 'grouping_col' conditionally within each group defined by 'expert'\n",
    "        df_shuffled_col['shuffled_col'] = df_shuffled_col.groupby('expert')[grouping_col]\\\n",
    "            .transform(lambda x: x.sample(frac=1, random_state=i).values)\n",
    "\n",
    "        # Calculate mean deltas based on (expert, shuffled_col)\n",
    "        group_means_shuffled_c = df_shuffled_col.groupby(['expert', 'shuffled_col'])['sample_ix']\\\n",
    "            .apply(lambda idx: delta_vectors_np[idx.tolist()].mean(axis=0) if len(idx) >= min_samples_per_group else np.nan)\\\n",
    "            .dropna()\n",
    "\n",
    "        # Calculate average similarity across SHUFFLED col within each expert\n",
    "        sims_within_expert = []\n",
    "        for current_expert_val in df_shuffled_col['expert'].unique():\n",
    "             # Get mean vectors for this specific expert value, indexed by shuffled_col\n",
    "            means_for_expert = group_means_shuffled_c.loc[current_expert_val, :] # Selects multi-index level\n",
    "            means_for_expert = means_for_expert.dropna() # Ensure valid means\n",
    "            if len(means_for_expert) >= min_groups_for_sim_calc:\n",
    "                vec_stack = np.stack(means_for_expert.values)\n",
    "                cos_sim_matrix = 1 - sklearn.metrics.pairwise_distances(vec_stack, metric='cosine')\n",
    "                upper_triangle_indices = np.triu_indices(cos_sim_matrix.shape[0], k=1)\n",
    "                if len(upper_triangle_indices[0]) > 0:\n",
    "                    sims_within_expert.append(np.mean(np.abs(cos_sim_matrix[upper_triangle_indices])))\n",
    "\n",
    "        if sims_within_expert: # Only average if we got results for at least one expert group\n",
    "            baseline_sims_across_col.append(np.mean(sims_within_expert))\n",
    "\n",
    "\n",
    "    # Calculate overall averages, handling cases where shuffles might fail\n",
    "    avg_baseline_sim_across_experts = np.mean(baseline_sims_across_experts) if baseline_sims_across_experts else np.nan\n",
    "    avg_baseline_sim_across_col = np.mean(baseline_sims_across_col) if baseline_sims_across_col else np.nan\n",
    "\n",
    "    print(\"Baseline calculation finished.\")\n",
    "    return avg_baseline_sim_across_experts, avg_baseline_sim_across_col\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assume test_sample_df and test_deltas are loaded\n",
    "\n",
    "# Calculate baseline for 'prev_expert' grouping\n",
    "bl_across_experts_pe, bl_across_col_pe = get_baselines(test_sample_df, test_deltas, grouping_col = 'prev_expert', n_shuffles = 5)\n",
    "print(f\"\\nBaseline for grouping_col='prev_expert':\")\n",
    "print(f\"  Avg Baseline Sim Across Experts (within prev_expert): {bl_across_experts_pe:.4f}\")\n",
    "print(f\"  Avg Baseline Sim Across prev_expert (within expert): {bl_across_col_pe:.4f}\")\n",
    "\n",
    "# Calculate baseline for 'source' grouping\n",
    "bl_across_experts_src, bl_across_col_src = get_baselines(test_sample_df, test_deltas, grouping_col = 'source', n_shuffles = 5)\n",
    "print(f\"\\nBaseline for grouping_col='source':\")\n",
    "print(f\"  Avg Baseline Sim Across Experts (within source): {bl_across_experts_src:.4f}\")\n",
    "print(f\"  Avg Baseline Sim Across source (within expert): {bl_across_col_src:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "0677c627-847f-4fbf-93ee-5cd54e7fcf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic Regression\n",
    "\"\"\"\n",
    "def run_lr(x_cp, y_cp):\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.1, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 1000, fit_intercept = False)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    accuracy = lr_model.score(x_test, y_test)\n",
    "    proba = lr_model.predict_proba(x_test)\n",
    "    nll = -1 * cuml.metrics.log_loss(y_test, proba) # higher is better\n",
    "    print(f\"–log-loss: {float(cupy.asnumpy(nll)):.4f}\")\n",
    "\n",
    "x_cp = cupy.asarray(test_deltas)\n",
    "\n",
    "run_lr(x_cp, cupy.asarray(test_sample_df['expert'].tolist()))\n",
    "run_lr(x_cp, cupy.asarray(test_sample_df['prev_expert'].tolist()))\n",
    "_, source_codes = np.unique(test_sample_df['source'].tolist(), return_inverse = True)\n",
    "run_lr(x_cp, source_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866413bd",
   "metadata": {},
   "source": [
    "## SVD clustering: sideways -> h_orth\n",
    "1. Decompose the pre-mlp hidden states into h_sideways WITHIN each group of activations that route to a single expert, with respect to just the D-dimensional routing gate for that single expert. Then, still within that single expert, cluster those h_sideways activations.\n",
    "2. Repeat across all experts.\n",
    "3. After obtaining these cluters, do h_orth (the regular decomposition using all activations with respect to the entire routing gate).\n",
    "4. Extract calculate the cluster centroids from h_orth, using the cluster ids/labels extracted from h_sideways earlier. This results in n_experts * n_clusters_per_expert cluster centers.\n",
    "5. Calculate (using cosine similarity?) the within-expert against across-expert averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib, utils.svd as svd\n",
    "# decompose_sideways = importlib.reload(svd).decompose_sideways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e7b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 512):\n",
    "    kmeans_model = cuml.cluster.KMeans(n_clusters = n_clusters, max_iter = 1000, random_state = 123)\n",
    "    kmeans_model.fit(cupy.asarray(layer_hs.to(torch.float32)))\n",
    "    clear_all_cuda_memory(False)\n",
    "    cluster_ids = kmeans_model.labels_.tolist() # n_samples\n",
    "    cluster_centers = kmeans_model.cluster_centers_ # (n_clusters, D)\n",
    "    return cluster_ids, cluster_centers\n",
    "\n",
    "cluster_kmeans(relevant_pre_mlp_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare sample-level df merged with top-1 expert selections for a single test layer\n",
    "\"\"\"\n",
    "\n",
    "test_layer_ix = 1\n",
    "\n",
    "sample_df_test =\\\n",
    "    sample_df\\\n",
    "    .merge(\n",
    "        topk1_df.pipe(lambda df: df[df['layer_ix'] == test_layer_ix])[['expert', 'sample_ix']],\n",
    "        how = 'inner',\n",
    "        on = 'sample_ix'\n",
    "    )\\\n",
    "    .merge(\n",
    "        topk1_df.pipe(lambda df: df[df['layer_ix'] == test_layer_ix - 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']],\n",
    "        how = 'inner',\n",
    "        on = 'sample_ix'\n",
    "    )\n",
    "\n",
    "sample_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd21bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the sideways decomposition within activations routed to a single expert; this specifically REMOVES the part of h directly \n",
    "responsible for increasing/decreasing logit specifically for that expert; then cluster them\n",
    "\"\"\"\n",
    "cluster_ids = torch.full([all_pre_mlp_hs[test_layer_ix].shape[0]], -1, dtype = torch.int32)\n",
    "\n",
    "for this_expert in tqdm(sorted(sample_df_test['expert'].unique().tolist())):\n",
    "\n",
    "    # Extract sample indices for expert\n",
    "    this_sample_indices = sample_df_test[sample_df_test['expert'] == this_expert]['sample_ix'].tolist()\n",
    "    \n",
    "    # D-dimensional routing gate for expert route\n",
    "    this_gate = model.model.layers[test_layer_ix].mlp.gate.weight[this_expert, :].to(torch.float32).detach().cpu()\n",
    "    \n",
    "    # Remove only this expert’s axis to expose sub‑clusters\n",
    "    _, h_side = decompose_sideways(all_pre_mlp_hs[test_layer_ix][this_sample_indices], this_gate)\n",
    "\n",
    "    # Cluster within expert\n",
    "    this_cluster_ids, _ = cluster_kmeans(h_side, n_clusters = 10)\n",
    "    cluster_ids[this_sample_indices] = torch.tensor(this_cluster_ids, dtype = cluster_ids.dtype)\n",
    "\n",
    "sample_df_test_cl = sample_df_test.assign(cluster_id = cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da61ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Go back to the original decomposition to get the regular h_orth\n",
    "\"\"\"\n",
    "_, h_orth = decompose_orthogonal(\n",
    "    all_pre_mlp_hs[test_layer_ix].to(torch.float32),\n",
    "    model.model.layers[test_layer_ix].mlp.gate.weight.to(torch.float32).detach().cpu(),\n",
    "    method = 'svd'\n",
    ")\n",
    "h_orth = h_orth.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909823d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply the sub-cluster labels obtained from clustering h_sideways to the corresponding h_orth vectors\n",
    "This allows us to compare things in h_orth space; then compare cosine similarity. \n",
    "\"\"\"\n",
    "centroids = [] # List of np centroids\n",
    "tags = [] # (expert, cluster)\n",
    "\n",
    "for this_expert in sorted(sample_df_test_cl['expert'].unique().tolist()):\n",
    "    for this_cluster in [x for x in sorted(sample_df_test_cl['cluster_id'].unique().tolist()) if x != -1]: # Get clusters\n",
    "        this_e_c_sample_indices =\\\n",
    "            sample_df_test_cl\\\n",
    "            .pipe(lambda df: df[(df['expert'] == this_expert) & (df['cluster_id'] == this_cluster)])\\\n",
    "            ['sample_ix'].tolist()\n",
    "        \n",
    "        if len(this_e_c_sample_indices) <= 50: # Throw out tiny clusters\n",
    "            continue\n",
    "\n",
    "        v = h_orth[this_e_c_sample_indices].mean(0)\n",
    "        v = v / v.norm() # normalise for cosine\n",
    "        centroids.append(v)\n",
    "        tags.append((int(this_expert), int(this_cluster)))\n",
    "\n",
    "centroids = torch.stack(centroids) # this_expert * K x D\n",
    "cosine_sims = sklearn.metrics.pairwise.cosine_similarity(centroids.numpy()) # this_expert * K x this_expert * K; pairwise sim between each expert-cluster pair\n",
    "\n",
    "within, across = [], []\n",
    "for i, (e_i, _) in enumerate(tags):\n",
    "    for j, (e_j, _) in enumerate(tags):\n",
    "        if i >= j: continue # upper‑tri only\n",
    "        (within if e_i==e_j else across).append(cosine_sims[i, j])\n",
    "\n",
    "mean_cos_within  = float(np.mean(within))\n",
    "mean_cos_across  = float(np.mean(across))\n",
    "print(f\"mean cosine  (within expert):  {mean_cos_within:.3f}\")\n",
    "print(f\"mean cosine  (across experts): {mean_cos_across:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
