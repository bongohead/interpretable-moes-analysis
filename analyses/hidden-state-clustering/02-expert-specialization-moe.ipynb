{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to test orthogonality of expert specialization.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18084712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import cupy\n",
    "import cuml\n",
    "import sklearn\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "from utils.svd import decompose_orthogonal, decompose_sideways\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10fc64",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d50e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_id = 'allenai/OLMoE-1B-7B-0125-Instruct'\n",
    "model_prefix = 'olmoe'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ebf8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix):\n",
    "    all_pre_mlp_hs = torch.load(f'data/{model_prefix}/all-pre-mlp-hidden-states.pt')\n",
    "    all_expert_outputs = torch.load(f'data/{model_prefix}/all-expert-outputs.pt')\n",
    "    with open(f'data/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    return all_pre_mlp_hs, all_expert_outputs, metadata['sample_df'], metadata['topk_df'], metadata['all_pre_mlp_hidden_states_layers'], metadata['all_expert_outputs_layers']\n",
    "\n",
    "all_pre_mlp_hs_import, all_expert_outputs_import, sample_df_import, topk_df_import, act_map, expert_map = load_data(model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6db4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df_raw =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: df.groupby(['batch_ix', 'sequence_ix', 'token_ix']).ngroup())\\\n",
    "    .assign(seq_id = lambda df: df.groupby(['batch_ix', 'sequence_ix']).ngroup())\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df_raw[['sample_ix', 'batch_ix', 'sequence_ix', 'token_ix']], how = 'inner', on = ['sequence_ix', 'token_ix', 'batch_ix'])\\\n",
    "    .drop(columns = ['sequence_ix', 'token_ix', 'batch_ix'])\n",
    "\n",
    "topk1_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\n",
    "\n",
    "sample_df =\\\n",
    "    sample_df_raw\\\n",
    "    .drop(columns = ['batch_ix', 'sequence_ix'])\n",
    "\n",
    "del sample_df_import, topk_df_import\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16dccba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {layer_ix: all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(act_map)}\n",
    "\n",
    "all_expert_outputs = all_expert_outputs_import.to(torch.float16)\n",
    "compare_bf16_fp16_batched(all_expert_outputs_import, all_expert_outputs)\n",
    "del all_expert_outputs_import\n",
    "all_expert_outputs = {layer_ix: all_expert_outputs[:, save_ix, :, :] for save_ix, layer_ix in enumerate(expert_map)}\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04e099",
   "metadata": {},
   "source": [
    "## Visualize activation clusters of single (layer, expert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bcf4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize clusters\n",
    "\"\"\"\n",
    "layer_ix = 9\n",
    "expert_id = 1\n",
    "\n",
    "relevant_sample_ids =\\\n",
    "    topk1_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == layer_ix])\\\n",
    "    .pipe(lambda df: df[df['expert'] == expert_id])\\\n",
    "    .sort_values(by = 'sample_ix', ascending = True)\\\n",
    "    ['sample_ix']\\\n",
    "    .tolist()\n",
    "\n",
    "# Get expert IDs of previous layer\n",
    "prev_experts_df =\\\n",
    "    topk1_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == layer_ix - 1])\\\n",
    "    .pipe(lambda df: df[df['sample_ix'].isin(relevant_sample_ids)])\\\n",
    "    .rename(columns = {'expert': 'prev_expert'})\\\n",
    "    [['sample_ix', 'prev_expert']]\n",
    "\n",
    "# Get sample dfs of relevant sample IDs, include expert IDs of previous layer\n",
    "relevant_samples_df =\\\n",
    "    sample_df[sample_df['sample_ix'].isin(relevant_sample_ids)]\\\n",
    "    .merge(prev_experts_df, on = 'sample_ix', how = 'inner')\n",
    "\n",
    "display(relevant_samples_df)\n",
    "\n",
    "relevant_pre_mlp_hs = all_pre_mlp_hs[layer_ix][relevant_sample_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d67fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helpers\n",
    "\"\"\"\n",
    "def reduce_pca(input_tensor: torch.Tensor, n_components = 2):\n",
    "    hs_cupy = cupy.asarray(input_tensor.to(torch.float32))\n",
    "    model = cuml.PCA(\n",
    "        iterated_power = 100,\n",
    "        n_components = n_components,\n",
    "        verbose = True\n",
    "    )\n",
    "    model.fit(hs_cupy)\n",
    "    # print(f'Explained variance ratio: {model.explained_variance_ratio_}')\n",
    "    print(f'Cumulative variance ratio: {np.cumsum(model.explained_variance_ratio_)[-1]}')\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return pred\n",
    "\n",
    "def reduce_umap(input_tensor: torch.Tensor, n_components = 2, metric = 'cosine', n_epochs = 200):\n",
    "    hs_cupy = cupy.asarray(input_tensor.to(torch.float32))\n",
    "    model = cuml.UMAP(\n",
    "        n_components = n_components, \n",
    "        n_neighbors = 20, # 15 for default, smaller = more local data preserved [2 - 100]\n",
    "        metric = metric, # euclidean, cosine, manhattan, l2, hamming\n",
    "        min_dist = 0.5, # 0.1 by default, effective distance between embedded points\n",
    "        n_epochs = n_epochs, # 200 by default for large datasets\n",
    "        random_state = 123, # Allow parallelism\n",
    "        verbose = False\n",
    "    )\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return pred\n",
    "\n",
    "def plot_manifold(plot_df, color_col, hover_col):\n",
    "    px.scatter(\n",
    "        plot_df,\n",
    "        x = 'd1', y = 'd2', color = color_col, hover_data = [hover_col]\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aee7db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot PCA + UMAP, color by previous expert ID\n",
    "\"\"\"\n",
    "pca_res = reduce_pca(relevant_pre_mlp_hs, 2)\n",
    "pca_plot_df =\\\n",
    "    pd.concat([pd.DataFrame({'d1': pca_res[:, 0], 'd2': pca_res[:, 1]}), relevant_samples_df], axis = 1)\\\n",
    "    .sample(5000)\\\n",
    "    .assign(prev_expert = lambda df: df['prev_expert'].astype(str))\n",
    "plot_manifold(pca_plot_df, 'prev_expert', 'token')\n",
    "plot_manifold(pca_plot_df, 'source', 'token')\n",
    "\n",
    "ump_res = reduce_umap(relevant_pre_mlp_hs, 2, 'cosine')\n",
    "ump_plot_df =\\\n",
    "    pd.concat([pd.DataFrame({'d1': pca_res[:, 0], 'd2': pca_res[:, 1]}), relevant_samples_df], axis = 1)\\\n",
    "    .sample(5000)\\\n",
    "    .assign(prev_expert = lambda df: df['prev_expert'].astype(str))\n",
    "plot_manifold(ump_plot_df, 'prev_expert', 'token')\n",
    "plot_manifold(ump_plot_df, 'source', 'token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8fae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot PCA + UMAP, but this time include token samples from outside this expert\n",
    "\"\"\"\n",
    "nonrelevant_sample_ids =\\\n",
    "    topk1_df\\\n",
    "    .pipe(lambda df: df[df['layer_ix'] == layer_ix])\\\n",
    "    .pipe(lambda df: df[df['expert'] != expert_id])\\\n",
    "    .sort_values(by = 'sample_ix', ascending = True)\\\n",
    "    ['sample_ix']\\\n",
    "    .tolist()\n",
    "\n",
    "# Get sample dfs of relevant sample IDs, include expert IDs of previous layer\n",
    "nonrelevant_samples_df = sample_df[sample_df['sample_ix'].isin(nonrelevant_sample_ids)]\n",
    "nonrelevant_pre_mlp_hs = all_pre_mlp_hs[layer_ix][nonrelevant_sample_ids]\n",
    "\n",
    "# Prep all_samples_df samples df\n",
    "all_samples_df = pd.concat([\n",
    "    relevant_samples_df.assign(prev_expert = lambda df: df['prev_expert'].astype(str)), \n",
    "    nonrelevant_samples_df.assign(prev_expert = 'NA')\n",
    "]).reset_index(drop = True)\n",
    "\n",
    "pca_res = reduce_pca(torch.concat([relevant_pre_mlp_hs, nonrelevant_pre_mlp_hs], dim = 0), 2)\n",
    "pca_plot_df =\\\n",
    "    pd.concat([pd.DataFrame({'d1': pca_res[:, 0], 'd2': pca_res[:, 1]}), all_samples_df], axis = 1)\\\n",
    "    .pipe(lambda df: pd.concat([\n",
    "        df[df['prev_expert'] == 'NA'].sample(10_000),\n",
    "        df[df['prev_expert'] != 'NA'].sample(10_000)\n",
    "    ]))\n",
    "\n",
    "plot_manifold(pca_plot_df, 'prev_expert', 'token')\n",
    "plot_manifold(pca_plot_df, 'source', 'token')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347bb496",
   "metadata": {},
   "source": [
    "## Cross-layer source mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot cross-layer source mappings\n",
    "\"\"\"\n",
    "layers_to_test = [0, 2]\n",
    "\n",
    "for layer_ix in layers_to_test:\n",
    "    pca_res = reduce_pca(all_pre_mlp_hs[layer_ix], 2)\n",
    "    pca_plot_df =\\\n",
    "        pd.concat([pd.DataFrame({'d1': pca_res[:, 0], 'd2': pca_res[:, 1]}), relevant_samples_df], axis = 1)\\\n",
    "        .sample(5000)\n",
    "    plot_manifold(pca_plot_df, 'source', 'token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(pca_plot_df, 'source', 'token')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866413bd",
   "metadata": {},
   "source": [
    "## SVD clustering: sideways -> h_orth\n",
    "1. Decompose the pre-mlp hidden states into h_sideways WITHIN each group of activations that route to a single expert, with respect to just the D-dimensional routing gate for that single expert. Then, still within that single expert, cluster those h_sideways activations.\n",
    "2. Repeat across all experts.\n",
    "3. After obtaining these cluters, do h_orth (the regular decomposition using all activations with respect to the entire routing gate).\n",
    "4. Extract calculate the cluster centroids from h_orth, using the cluster ids/labels extracted from h_sideways earlier. This results in n_experts * n_clusters_per_expert cluster centers.\n",
    "5. Calculate (using cosine similarity?) the within-expert against across-expert averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bbb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib, utils.svd as svd\n",
    "# decompose_sideways = importlib.reload(svd).decompose_sideways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e7b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_kmeans(layer_hs: torch.Tensor, n_clusters = 512):\n",
    "    kmeans_model = cuml.cluster.KMeans(n_clusters = n_clusters, max_iter = 1000, random_state = 123)\n",
    "    kmeans_model.fit(cupy.asarray(layer_hs.to(torch.float32)))\n",
    "    clear_all_cuda_memory(False)\n",
    "    cluster_ids = kmeans_model.labels_.tolist() # n_samples\n",
    "    cluster_centers = kmeans_model.cluster_centers_ # (n_clusters, D)\n",
    "    return cluster_ids, cluster_centers\n",
    "\n",
    "cluster_kmeans(relevant_pre_mlp_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare sample-level df merged with top-1 expert selections for a single test layer\n",
    "\"\"\"\n",
    "\n",
    "test_layer_ix = 1\n",
    "\n",
    "sample_df_test =\\\n",
    "    sample_df\\\n",
    "    .merge(\n",
    "        topk1_df.pipe(lambda df: df[df['layer_ix'] == test_layer_ix])[['expert', 'sample_ix']],\n",
    "        how = 'inner',\n",
    "        on = 'sample_ix'\n",
    "    )\\\n",
    "    .merge(\n",
    "        topk1_df.pipe(lambda df: df[df['layer_ix'] == test_layer_ix - 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']],\n",
    "        how = 'inner',\n",
    "        on = 'sample_ix'\n",
    "    )\n",
    "\n",
    "sample_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd21bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the sideways decomposition within activations routed to a single expert; this specifically REMOVES the part of h directly \n",
    "responsible for increasing/decreasing logit specifically for that expert; then cluster them\n",
    "\"\"\"\n",
    "cluster_ids = torch.full([all_pre_mlp_hs[test_layer_ix].shape[0]], -1, dtype = torch.int32)\n",
    "\n",
    "for this_expert in tqdm(sorted(sample_df_test['expert'].unique().tolist())):\n",
    "\n",
    "    # Extract sample indices for expert\n",
    "    this_sample_indices = sample_df_test[sample_df_test['expert'] == this_expert]['sample_ix'].tolist()\n",
    "    \n",
    "    # D-dimensional routing gate for expert route\n",
    "    this_gate = model.model.layers[test_layer_ix].mlp.gate.weight[this_expert, :].to(torch.float32).detach().cpu()\n",
    "    \n",
    "    # Remove only this expert’s axis to expose sub‑clusters\n",
    "    _, h_side = decompose_sideways(all_pre_mlp_hs[test_layer_ix][this_sample_indices], this_gate)\n",
    "\n",
    "    # Cluster within expert\n",
    "    this_cluster_ids, _ = cluster_kmeans(h_side, n_clusters = 10)\n",
    "    cluster_ids[this_sample_indices] = torch.tensor(this_cluster_ids, dtype = cluster_ids.dtype)\n",
    "\n",
    "sample_df_test_cl = sample_df_test.assign(cluster_id = cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da61ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Go back to the original decomposition to get the regular h_orth\n",
    "\"\"\"\n",
    "_, h_orth = decompose_orthogonal(\n",
    "    all_pre_mlp_hs[test_layer_ix].to(torch.float32),\n",
    "    model.model.layers[test_layer_ix].mlp.gate.weight.to(torch.float32).detach().cpu(),\n",
    "    method = 'svd'\n",
    ")\n",
    "h_orth = h_orth.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909823d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply the sub-cluster labels obtained from clustering h_sideways to the corresponding h_orth vectors\n",
    "This allows us to compare things in h_orth space; then compare cosine similarity. \n",
    "\"\"\"\n",
    "centroids = [] # List of np centroids\n",
    "tags = [] # (expert, cluster)\n",
    "\n",
    "for this_expert in sorted(sample_df_test_cl['expert'].unique().tolist()):\n",
    "    for this_cluster in [x for x in sorted(sample_df_test_cl['cluster_id'].unique().tolist()) if x != -1]: # Get clusters\n",
    "        this_e_c_sample_indices =\\\n",
    "            sample_df_test_cl\\\n",
    "            .pipe(lambda df: df[(df['expert'] == this_expert) & (df['cluster_id'] == this_cluster)])\\\n",
    "            ['sample_ix'].tolist()\n",
    "        \n",
    "        if len(this_e_c_sample_indices) <= 50: # Throw out tiny clusters\n",
    "            continue\n",
    "\n",
    "        v = h_orth[this_e_c_sample_indices].mean(0)\n",
    "        v = v / v.norm() # normalise for cosine\n",
    "        centroids.append(v)\n",
    "        tags.append((int(this_expert), int(this_cluster)))\n",
    "\n",
    "centroids = torch.stack(centroids) # this_expert * K x D\n",
    "cosine_sims = sklearn.metrics.pairwise.cosine_similarity(centroids.numpy()) # this_expert * K x this_expert * K; pairwise sim between each expert-cluster pair\n",
    "\n",
    "within, across = [], []\n",
    "for i, (e_i, _) in enumerate(tags):\n",
    "    for j, (e_j, _) in enumerate(tags):\n",
    "        if i >= j: continue # upper‑tri only\n",
    "        (within if e_i==e_j else across).append(cosine_sims[i, j])\n",
    "\n",
    "mean_cos_within  = float(np.mean(within))\n",
    "mean_cos_across  = float(np.mean(across))\n",
    "print(f\"mean cosine  (within expert):  {mean_cos_within:.3f}\")\n",
    "print(f\"mean cosine  (across experts): {mean_cos_across:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae135d00",
   "metadata": {},
   "source": [
    "## SVD clustering: h_orth vs h_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare sample-level df merged with top-1 expert selections for a single test layer\n",
    "\"\"\"\n",
    "\n",
    "test_layer_ix = 10\n",
    "\n",
    "topk1_df =\\\n",
    "    topk_df\\\n",
    "    .pipe(lambda df: df[df['topk_ix'] == 1])\n",
    "\n",
    "sample_df_test =\\\n",
    "    sample_df\\\n",
    "    .merge(\n",
    "        topk1_df.pipe(lambda df: df[df['layer_ix'] == test_layer_ix])[['expert', 'sample_ix']],\n",
    "        how = 'inner',\n",
    "        on = 'sample_ix'\n",
    "    )\\\n",
    "    .merge(\n",
    "        topk1_df.pipe(lambda df: df[df['layer_ix'] == test_layer_ix - 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']],\n",
    "        how = 'inner',\n",
    "        on = 'sample_ix'\n",
    "    )\n",
    "\n",
    "sample_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Go back to the original decomposition to get the regular h_orth\n",
    "\"\"\"\n",
    "h_para, h_orth = decompose_orthogonal(\n",
    "    all_pre_mlp_hs[test_layer_ix].to(torch.float32),\n",
    "    model.model.layers[test_layer_ix].mlp.gate.weight.to(torch.float32).detach().cpu(),\n",
    "    method = 'svd'\n",
    ")\n",
    "h_para = h_para.to(torch.float32)\n",
    "h_orth = h_orth.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot PCA + UMAP, color by expert id\n",
    "\"\"\"\n",
    "def reduce_pca(input_tensor: torch.Tensor, n_components = 2):\n",
    "    hs_cupy = cupy.asarray(input_tensor.to(torch.float32))\n",
    "    model = cuml.PCA(\n",
    "        iterated_power = 100,\n",
    "        n_components = n_components,\n",
    "        verbose = True\n",
    "    )\n",
    "    model.fit(hs_cupy)\n",
    "    # print(f'Explained variance ratio: {model.explained_variance_ratio_}')\n",
    "    print(f'Cumulative variance ratio: {np.cumsum(model.explained_variance_ratio_)[-1]}')\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return pred\n",
    "\n",
    "def reduce_umap(input_tensor: torch.Tensor, n_components = 2, metric = 'cosine', n_epochs = 200):\n",
    "    hs_cupy = cupy.asarray(input_tensor.to(torch.float32))\n",
    "    model = cuml.UMAP(\n",
    "        n_components = n_components, \n",
    "        n_neighbors = 20, # 15 for default, smaller = more local data preserved [2 - 100]\n",
    "        metric = metric, # euclidean, cosine, manhattan, l2, hamming\n",
    "        min_dist = 0.5, # 0.1 by default, effective distance between embedded points\n",
    "        n_epochs = n_epochs, # 200 by default for large datasets\n",
    "        random_state = 123, # Allow parallelism\n",
    "        verbose = False\n",
    "    )\n",
    "    pred = cupy.asnumpy(model.fit_transform(hs_cupy))\n",
    "    clear_all_cuda_memory(False)\n",
    "    return pred\n",
    "\n",
    "def plot_manifold(reduced_np, sample_df):\n",
    "    px.scatter(\n",
    "        pd.concat([pd.DataFrame({'d1': reduced_np[:, 0], 'd2': reduced_np[:, 1]}), sample_df], axis = 1)\\\n",
    "            .sample(5000)\\\n",
    "            .assign(prev_expert = lambda df: df['expert'].astype(str)),\n",
    "        x = 'd1', y = 'd2', color = 'expert', hover_data = ['token']\n",
    "    ).show()\n",
    "\n",
    "#pca_res = reduce_pca(h_para, 2)\n",
    "plot_manifold(pca_res, sample_df_test)\n",
    "\n",
    "# plot_reduction(relevant_pre_mlp_hs, relevant_samples_df, reduce_umap, 2, 'cosine', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67598c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(pca_res, sample_df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
