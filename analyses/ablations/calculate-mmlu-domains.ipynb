{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab5d74",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "### Calculate most common MMLU domains per pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f72dd9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ---- Load libs ----\n",
    "library(tidyverse)\n",
    "library(slider)\n",
    "\n",
    "model_prefix = 'qwen3moe'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dfaefc",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17572862",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ---- Load data ----\n",
    "local({\n",
    "\n",
    "    raw_sample_df =\n",
    "        data.table::fread(str_glue('data/{model_prefix}/samples.csv'), strip.white = F) %>%\n",
    "        arrange(q_ix, token_ix) %>%\n",
    "        mutate(., shifted_token_ix = 1:n(), .by = c('q_ix'))\n",
    "    \n",
    "    raw_topk_df = data.table::fread(str_glue('data/{model_prefix}/topks.csv'))\n",
    "\n",
    "    last_shared_tok =\n",
    "        raw_sample_df %>%\n",
    "        group_by(shifted_token_ix, token) %>%\n",
    "        summarize(., n_questions_with_token_ix_tok = n(), .groups = 'drop') %>% \n",
    "        filter(., n_questions_with_token_ix_tok == length(unique(raw_sample_df$q_ix))) %>%\n",
    "        .$shifted_token_ix %>%\n",
    "        max\n",
    "\n",
    "    sample_df =\n",
    "        raw_sample_df %>%\n",
    "        filter(., shifted_token_ix > last_shared_tok)\n",
    "\n",
    "    topk_df = raw_topk_df %>% inner_join(select(sample_df, q_ix, token_ix), by = c('q_ix', 'token_ix'))\n",
    "\n",
    "    topk1_df = topk_df %>% filter(., topk_ix == 1) %>% select(-topk_ix)\n",
    "\n",
    "    sample_df <<- sample_df\n",
    "    topk_df <<- topk_df\n",
    "    topk1_df <<- topk1_df\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f8c2b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "domain_lang_map =\n",
    "    sample_df %>%\n",
    "    group_by(domain, lang) %>% \n",
    "    summarize(., n_tokens = n(), .groups = 'drop')\n",
    "\n",
    "domain_lang_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad334d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Check experts, layers\n",
    "print(sort(unique(topk1_df$expert)))\n",
    "print(sort(unique(topk1_df$layer_ix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8bd1b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "base_accs =\n",
    "    sample_df %>%\n",
    "    group_by(domain, lang, q_ix, question_output_token, answer_char) %>% \n",
    "    summarize(tokens_per_question = n(), .groups = 'drop') %>%\n",
    "    mutate(., is_correct = ifelse(str_squish(question_output_token) == answer_char, 1, 0)) %>%\n",
    "    group_by(domain, lang) %>%\n",
    "    summarize(base_acc = sum(is_correct)/n(), questions = n(), total_tokens = sum(tokens_per_question), .groups = 'drop')\n",
    "\n",
    "print(base_accs)\n",
    "write_csv(base_accs, str_glue('data/{model_prefix}/base_accs.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d65728",
   "metadata": {},
   "source": [
    "## Ablation method 1: most common transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165515a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ---- Get transition counts by q_ix ----\n",
    "toks_with_paths =\n",
    "    topk1_df %>%\n",
    "    select(., layer_ix, q_ix, token_ix, expert) %>%\n",
    "    group_by(., q_ix, token_ix) %>%\n",
    "    arrange(., layer_ix, .by_group = T) %>%\n",
    "    mutate(\n",
    "        path = slide(expert, .f = \\(x) x, .before = 1, .after = 0),\n",
    "        layers = slide(layer_ix, .f = \\(x) x, .before = 1, .after = 0)\n",
    "    ) %>%\n",
    "    ungroup() %>%\n",
    "    filter(., layer_ix > 0) %>%\n",
    "    left_join(., select(sample_df, q_ix, token_ix, domain, lang), by = c('q_ix', 'token_ix'))\n",
    "\n",
    "head(toks_with_paths, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927f51f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cat('# Samples: ', nrow(sample_df))\n",
    "cat('\\n# Topk1 Experts (n_layers x samples): ', nrow(topk1_df))\n",
    "cat('\\n# Paths ((n_layers - 1) x samples): ', nrow(toks_with_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5840fdf",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dom_x_path =\n",
    "    toks_with_paths %>%\n",
    "    group_by(., domain, lang, path, layers) %>%\n",
    "    summarize(., n_samples = n(), .groups = 'drop')\n",
    "\n",
    "head(dom_x_path, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674ae0d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dom_tok_counts =\n",
    "    dom_x_path %>%\n",
    "    group_by(., domain, lang) %>%\n",
    "    summarize(., n_tok_samples = sum(n_samples), .groups = 'drop') %>%\n",
    "    mutate(., n_tok_prop = n_tok_samples/sum(n_tok_samples))\n",
    "\n",
    "dom_tok_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec8759",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Path counts, pivot domains out\n",
    "test_domain = 'biology'\n",
    "test_lang = 'en'\n",
    "\n",
    "dom_x_path %>%\n",
    "    pivot_wider(., id_cols = c(layers, path), names_from = c(domain, lang), values_from = n_samples, values_fill = 0) %>% \n",
    "    print()\n",
    "\n",
    "spec_paths =\n",
    "    dom_x_path %>%\n",
    "    group_by(layers, path) %>%\n",
    "    mutate(., prop_of_samples = n_samples/sum(n_samples)) %>%\n",
    "    ungroup() %>%\n",
    "    filter(., domain == test_domain & lang == test_lang) %>%\n",
    "    left_join(dom_tok_counts, by = c('domain', 'lang')) %>%\n",
    "    filter(., prop_of_samples >= n_tok_prop * 1)\n",
    "\n",
    "cat('Unique path counts: ', nrow(spec_paths), ' of ', nrow(spec_paths))\n",
    "cat('\\nPaths taken counts: ', sum(spec_paths$n_samples), ' of ', sum(filter(dom_x_path, domain == test_domain & lang == test_lang)$n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d4626",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze proportions to be ablated\n",
    "toks_with_paths %>%\n",
    "    left_join(transmute(spec_paths, layers, path, is_spec = 1), by = c('path', 'layers')) %>%\n",
    "    mutate(., is_spec = ifelse(!is.na(is_spec), 1, 0)) %>%\n",
    "    group_by(q_ix, token_ix, domain, lang) %>%\n",
    "    summarize(., n_spec_paths = sum(is_spec), n_possible_paths = n(), .groups = 'drop') %>%\n",
    "    group_by(domain, lang) %>%\n",
    "    summarize(\n",
    "        n_questions = n_distinct(q_ix),\n",
    "        n_toks = n(),\n",
    "        n_toks_with_any_spec_path = sum(ifelse(n_spec_paths > 0, 1, 0)),\n",
    "        n_toks_with_half_spec_path = sum(ifelse(n_spec_paths >= n_possible_paths * .5, 1, 0)),\n",
    "        n_spec_paths = sum(n_spec_paths),\n",
    "        n_possible_paths = sum(n_possible_paths),\n",
    "        .groups = 'drop'\n",
    "    ) %>%\n",
    "    mutate(\n",
    "        .,\n",
    "        prop_spec_paths = n_spec_paths/n_possible_paths,\n",
    "        prop_toks_with_any_spec_path = n_toks_with_any_spec_path/n_toks,\n",
    "        prop_toks_with_half_spec_path = n_toks_with_half_spec_path/n_toks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8440763",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# We save it in format\n",
    "# {\n",
    "#     layer: [\n",
    "#         [prefix, target_e]\n",
    "#         ...\n",
    "#     ]\n",
    "# }\n",
    "exportable_format = \n",
    "    spec_paths %>%\n",
    "    mutate(\n",
    "        target_layer = map_int(layers, \\(x) tail(x, 1)),\n",
    "        target_expert = map_int(path, \\(x) tail(x, 1)),\n",
    "        expert_prefix = map(path, \\(x) head(x, -1)),\n",
    "        rule_pair = map2(expert_prefix, target_expert, \\(x, y) list(x, y))\n",
    "    ) %>%\n",
    "    select(target_layer, rule_pair) %>%\n",
    "    group_by(., target_layer) %>%\n",
    "    summarize(\n",
    "        rules = list(rule_pair),\n",
    "        .groups = 'drop'\n",
    "    ) %>%\n",
    "    {setNames(.$rules, .$target_layer)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d691e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "length(exportable_format$`1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5f9b6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "json_output = jsonlite::toJSON(exportable_format, simplifyVector = F, auto_unbox = T, pretty = F)\n",
    "writeLines(json_output, str_glue('data/{model_prefix}/path_ablation_targets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a09b2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sample_df %>%\n",
    "    distinct(q_ix, domain, lang) %>%\n",
    "    mutate(., row_ix = 1:n()) %>% \n",
    "    group_by(domain, lang) %>%\n",
    "    summarize(., start = min(row_ix), end = max(row_ix), .groups = 'drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf94dc5",
   "metadata": {},
   "source": [
    "## Ablation method 2: within-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d6e77",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dom_expert_layer_counts =\n",
    "    topk1_df %>%\n",
    "    select(., layer_ix, q_ix, token_ix, expert) %>%\n",
    "    group_by(., q_ix, token_ix) %>%\n",
    "    filter(., layer_ix > 0) %>%\n",
    "    left_join(., select(sample_df, q_ix, token_ix, domain, lang), by = c('q_ix', 'token_ix')) %>%\n",
    "    group_by(., domain, lang, layer_ix, expert) %>%\n",
    "    summarize(., n_samples = n(), .groups = 'drop')\n",
    "\n",
    "head(dom_expert_layer_counts, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbd926",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dom_full_tok_counts =\n",
    "    dom_expert_layer_counts %>%\n",
    "    group_by(., domain, lang) %>%\n",
    "    summarize(., n_tok_samples = sum(n_samples), .groups = 'drop') %>%\n",
    "    mutate(., n_tok_prop = n_tok_samples/sum(n_tok_samples))\n",
    "\n",
    "dom_full_tok_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277380d4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dom_expert_layer_counts %>%\n",
    "    mutate(domain_lang = paste0(domain, '_', lang)) %>%\n",
    "    pivot_wider(., id_cols = c(layer_ix, expert), names_from = domain_lang, values_from = n_samples, values_fill = 0) %>% \n",
    "    print()\n",
    "\n",
    "spec_experts =\n",
    "    dom_expert_layer_counts %>%\n",
    "    group_by(layer_ix, expert) %>%\n",
    "    mutate(\n",
    "        .,\n",
    "        prop_of_samples = n_samples/sum(n_samples)\n",
    "        ) %>%\n",
    "    ungroup() %>%\n",
    "    filter(., domain == test_domain & lang == test_lang) %>%\n",
    "    left_join(dom_full_tok_counts, by = c('domain', 'lang')) %>%\n",
    "    filter(., prop_of_samples >= n_tok_prop * 5)\n",
    "\n",
    "cat('Specialized experts: ', sum(spec_experts$n_samples), ' of ', sum(filter(dom_expert_layer_counts, domain == test_domain & lang == test_lang)$n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63815292",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze proportions to be ablated\n",
    "topk1_df %>%\n",
    "    left_join(transmute(spec_experts, layer_ix, expert, is_spec = 1), by = c('layer_ix', 'expert')) %>%\n",
    "    left_join(select(sample_df, domain, lang, q_ix, token_ix), by = c('q_ix', 'token_ix')) %>%\n",
    "    mutate(., is_spec = ifelse(!is.na(is_spec), 1, 0)) %>%\n",
    "    group_by(q_ix, token_ix, domain, lang) %>%\n",
    "    summarize(., n_spec_exps = sum(is_spec), n_possible_exps = n(), .groups = 'drop') %>%\n",
    "    group_by(domain, lang) %>%\n",
    "    summarize(\n",
    "        n_questions = n_distinct(q_ix),\n",
    "        n_toks = n(),\n",
    "        n_toks_with_any_spec_exp = sum(ifelse(n_spec_exps > 0, 1, 0)),\n",
    "        n_toks_with_half_spec_exp = sum(ifelse(n_spec_exps >= n_possible_exps * .5, 1, 0)),\n",
    "        n_spec_exps = sum(n_spec_exps),\n",
    "        n_possible_exps = sum(n_possible_exps),\n",
    "        .groups = 'drop'\n",
    "    ) %>%\n",
    "    mutate(\n",
    "        .,\n",
    "        prop_spec_exps = n_spec_exps/n_possible_exps,\n",
    "        prop_toks_with_any_spec_exp = n_toks_with_any_spec_exp/n_toks,\n",
    "        prop_toks_with_half_spec_exp = n_toks_with_half_spec_exp/n_toks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8be7f",
   "metadata": {},
   "source": [
    "## Ablation method 3: within-layer, multiple experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca990488",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get multi-topk [order matters!]\n",
    "toks_with_multi_topk =\n",
    "    topk_df %>%\n",
    "    filter(topk_ix %in% 1:2) %>%\n",
    "    arrange(q_ix, token_ix, layer_ix, topk_ix, expert) %>%\n",
    "    group_by(q_ix, token_ix, layer_ix) %>%\n",
    "    arrange(topk_ix, .by_group = T) %>% # Order matters, switch topk_ix for expert otherwise\n",
    "    summarize(., experts = list(expert), .groups = 'drop') %>%\n",
    "    left_join(., select(sample_df, q_ix, token_ix, domain, lang), by = c('q_ix', 'token_ix'))\n",
    "    \n",
    "dom_x_experts = \n",
    "    toks_with_multi_topk %>%\n",
    "    group_by(., domain, lang, layer_ix, experts) %>%\n",
    "    summarize(., n_samples = n(), .groups = 'drop')\n",
    "\n",
    "print(head(topks_by_multi_topk, 5))\n",
    "\n",
    "dom_counts =\n",
    "    dom_x_experts %>%\n",
    "    group_by(., domain, lang) %>%\n",
    "    summarize(., n_tok_samples = sum(n_samples), .groups = 'drop') %>%\n",
    "    mutate(., n_tok_prop = n_tok_samples/sum(n_tok_samples))\n",
    "\n",
    "print(head(dom_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462ffab",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dom_x_experts %>%\n",
    "    pivot_wider(., id_cols = c(layer_ix, experts), names_from = c(domain, lang), values_from = n_samples, values_fill = 0) %>% \n",
    "    print()\n",
    "\n",
    "spec_multi_topk =\n",
    "    dom_x_experts %>%\n",
    "    group_by(layer_ix, experts) %>%\n",
    "    mutate(., prop_of_samples = n_samples/sum(n_samples)) %>%\n",
    "    ungroup() %>%\n",
    "    filter(., domain == test_domain & lang == test_lang) %>%\n",
    "    left_join(dom_counts, by = c('domain', 'lang')) %>%\n",
    "    filter(., prop_of_samples >= n_tok_prop * 5)\n",
    "\n",
    "cat('Specialized [exp1, exp2]: ', sum(spec_multi_topk$n_samples), ' of ', sum(filter(dom_x_experts, domain == test_domain & lang == test_lang)$n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb4fe2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze proportions to be ablated\n",
    "toks_with_multi_topk %>%\n",
    "    left_join(transmute(spec_multi_topk, layer_ix, experts, is_spec = 1), by = c('layer_ix', 'experts')) %>%\n",
    "    mutate(., is_spec = ifelse(!is.na(is_spec), 1, 0)) %>%\n",
    "    group_by(q_ix, token_ix, domain, lang) %>%\n",
    "    summarize(., n_spec_exp_pairs = sum(is_spec), n_possible_exp_pairs = n(), .groups = 'drop') %>%\n",
    "    group_by(domain, lang) %>%\n",
    "    summarize(\n",
    "        n_questions = n_distinct(q_ix),\n",
    "        n_toks = n(),\n",
    "        n_toks_with_any_spec_exp_pairs = sum(ifelse(n_spec_exp_pairs > 0, 1, 0)),\n",
    "        n_toks_with_half_spec_exp_pairs = sum(ifelse(n_spec_exp_pairs >= n_possible_exp_pairs * .5, 1, 0)),\n",
    "        n_spec_exp_pairs = sum(n_spec_exp_pairs),\n",
    "        n_possible_exp_pairs = sum(n_possible_exp_pairs),\n",
    "        .groups = 'drop'\n",
    "    ) %>%\n",
    "    mutate(\n",
    "        .,\n",
    "        prop_spec_exp_pairs = n_spec_exp_pairs/n_possible_exp_pairs,\n",
    "        prop_toks_with_any_spec_exp_pairs = n_toks_with_any_spec_exp_pairs/n_toks,\n",
    "        prop_toks_with_half_spec_exp_pairs = n_toks_with_half_spec_exp_pairs/n_toks\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
