{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Runs forward passes with ablation! Run after `calculate-mmlu-domains.ipynb`.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d710dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.store_outputs import convert_outputs_to_df\n",
    "from utils import pretrained_models\n",
    "\n",
    "import pickle\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043893c1",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74be5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\n",
    "Architectures supported currently for path/expert ablation:\n",
    "- Qwen3MoE architecture, includes Qwen3-30B-A3B, Qwen3-235B-A22B\n",
    "\"\"\"\n",
    "selected_model_index = 0\n",
    "\n",
    "def get_model(index):\n",
    "    model = [\n",
    "        ('Qwen/Qwen3-30B-A3B', 'qwen3moe', 'qwen3moe')\n",
    "    ][index]\n",
    "\n",
    "    return model[0], model[1], model[2]\n",
    "\n",
    "model_id, model_prefix, model_architecture = get_model(selected_model_index)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e974b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load path & expert ablation functions - these run forward passes - this verifies these are equivalent to a standard model() call output when no ablation\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_with_path_ablation = getattr(model_module, f\"run_{model_architecture}_with_path_ablation\")\n",
    "run_model_with_expert_ablation = getattr(model_module, f\"run_{model_architecture}_with_expert_ablation\")\n",
    "\n",
    "test_path_ablate_dict = {\n",
    "    1: [\n",
    "        ((0, ), 0)\n",
    "    ]\n",
    "}\n",
    "\n",
    "test_expert_ablate_dict = {\n",
    "    1: [64]\n",
    "}\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_custom_forward_pass(model, pad_token_id, run_fn, ablate_dict):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "    original_results = model(**inputs)\n",
    "    custom_results = run_fn(model, inputs['input_ids'], inputs['attention_mask'], ablate_dict, ablate_if_in_topk = False)\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    assert len(custom_results['all_topk_experts']) == len(custom_results['all_topk_weights']), 'Length of topk IDs and weights not equal'\n",
    "    print(f\"Length of topk: {len(custom_results['all_topk_experts'])}\")\n",
    "    print(f\"Topk size: {custom_results['all_topk_experts'][0].shape}\")\n",
    "    print(f\"First token topk IDs: {custom_results['all_topk_experts'][0][1,]}\")\n",
    "    print(f\"First token topk weights: {custom_results['all_topk_weights'][0][1,]}\")\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), model.config.vocab_size).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id, run_model_with_path_ablation, test_path_ablate_dict)\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id, run_model_with_expert_ablation, test_expert_ablate_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d8f4e",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ceb260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Which domain and language to ablate, as well as which k? These should be derived from `calculate-mmlu-domains.ipynb`, and should correspond to\n",
    " the output JSONs from that file.\n",
    "\"\"\"\n",
    "test_domain = 'biology'\n",
    "test_lang = 'en'\n",
    "test_k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3878e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset, dataloader\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_metadata():\n",
    "    with open(f'./data/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    return metadata['mmlu_df'], metadata['test_ds']\n",
    "\n",
    "mmlu_df, test_ds = load_metadata()\n",
    "test_dl = DataLoader(test_ds, batch_size = 8, shuffle = False, collate_fn = stack_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a819ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load path & expert ablation targets\n",
    "\"\"\"\n",
    "import json \n",
    "\n",
    "def load_path_ablation_targets_from_json(filepath):\n",
    "    \"\"\"\n",
    "    Parse JSON, converting it into a dictionary mapping layer integers to a list of tuples:\n",
    "    {\n",
    "        5: [\n",
    "            ((0,), 2), # Ablate expert #2 in layer #5 if previous expert was expert #0\n",
    "            ((1, 2), 10) # Ablate expert #10 in layer #5 if previous expert was expert #1 -> expert #2 (in previous 2 layers)\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        raw_targets = json.load(f)\n",
    "\n",
    "    ablation_targets_parsed = {}\n",
    "\n",
    "    for layer_str, rules_list in raw_targets.items():\n",
    "        layer_int = int(layer_str)\n",
    "        layer_rules = []\n",
    "        for rule_pair in rules_list:\n",
    "            if len(rule_pair) == 2 and (isinstance(rule_pair[0], list) or isinstance(rule_pair[1], int)) and isinstance(rule_pair[1], int):\n",
    "                prefix_list = rule_pair[0] if isinstance(rule_pair[0], list) else [rule_pair[0]] # If an int, cast to a singleton list\n",
    "                target_expert = rule_pair[1]\n",
    "                prefix_tuple = tuple(prefix_list)\n",
    "                layer_rules.append((prefix_tuple, target_expert))\n",
    "            else:\n",
    "                print(f\"Warning: Invalid rule format {rule_pair} in layer {layer_int}.\")\n",
    "\n",
    "        if layer_rules:\n",
    "            ablation_targets_parsed[layer_int] = layer_rules\n",
    "\n",
    "    return ablation_targets_parsed\n",
    "\n",
    "def load_expert_ablation_targets_from_json(filepath):\n",
    "    \"\"\"\n",
    "    Parse JSON, converting it into a dictionary mapping layer integers to a list of ints:\n",
    "    {\n",
    "        5: [0, 2] # Ablate experts 0 and 2 in layer 5\n",
    "    }\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        raw_targets = json.load(f)\n",
    "\n",
    "    ablation_targets_parsed = {}\n",
    "\n",
    "    for layer_str, experts_list in raw_targets.items():\n",
    "        layer_int = int(layer_str)\n",
    "        layer_rules = []\n",
    "\n",
    "        if isinstance(experts_list, int):\n",
    "            layer_rules.append(experts_list)\n",
    "        elif isinstance(experts_list, list):\n",
    "            for expert_target in experts_list:\n",
    "                layer_rules.append(int(expert_target))\n",
    "        else:\n",
    "            print(f\"Warning: Invalid format.\")\n",
    "\n",
    "        if layer_rules:\n",
    "            ablation_targets_parsed[layer_int] = layer_rules\n",
    "\n",
    "    return ablation_targets_parsed\n",
    "\n",
    "\n",
    "path_ablation_targets = load_path_ablation_targets_from_json(f'data/{model_prefix}/path_ablation_targets_{test_domain}_{test_lang}_{test_k}.json')\n",
    "print(f'Paths to ablate: {sum([len(x) for _, x in path_ablation_targets.items()])}')\n",
    "\n",
    "expert_ablation_targets = load_expert_ablation_targets_from_json(f'data/{model_prefix}/expert_ablation_targets_{test_domain}_{test_lang}_{test_k}.json')\n",
    "print(f'Experts to ablate: {sum([len(x) for _, x in expert_ablation_targets.items()])}')\n",
    "\n",
    "# multipath_ablation_targets = load_path_ablation_targets_from_json(f'data/{model_prefix}/multipath_ablation_targets_{test_domain}_{test_lang}_{test_k}.json')\n",
    "# print(f'Multipaths to ablate: {sum([len(x) for _, x in multipath_ablation_targets.items()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper to check accuracy stats\n",
    "\"\"\"\n",
    "def check_accuracy(sample_level_df):\n",
    "    \"\"\"\n",
    "    Check accuracy against base_accs.csv file\n",
    "    \n",
    "    Params:\n",
    "        @sample_level_df: A token-level or question-level dataframe containing `q_ix` and `question_output_token`.\n",
    "    \"\"\"\n",
    "    accuracy_df =\\\n",
    "        sample_level_df\\\n",
    "        .merge(mmlu_df, how = 'inner', on = 'q_ix')\\\n",
    "        .groupby(['q_ix', 'lang_ix', 'domain_ix', 'source_id', 'domain', 'lang', 'question_output_token', 'answer_char'], as_index = False)\\\n",
    "        .agg(n_tokens = ('q_ix', 'count'))\\\n",
    "        .assign(is_correct = lambda df: np.where(df['question_output_token'].str.strip() == df['answer_char'], 1, 0))\\\n",
    "        .groupby(['domain', 'lang'], as_index = False)\\\n",
    "        .agg(n_accurate = ('is_correct', 'sum'), n_total = ('q_ix', 'count'))\\\n",
    "        .assign(accuracy = lambda df: df['n_accurate']/df['n_total'])\\\n",
    "        .merge(pd.read_csv(f'data/{model_prefix}/test_accuracy.csv')[['domain', 'lang', 'base_accuracy']], on = ['domain', 'lang'], how = 'inner')\\\n",
    "        .assign(chg = lambda df: (df['accuracy'] - df['base_accuracy'])/df['base_accuracy'])\n",
    "    \n",
    "    return accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f180d3f",
   "metadata": {},
   "source": [
    "## Run path ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes with path ablation\n",
    "\"\"\"\n",
    "path_sample_dfs = []\n",
    "\n",
    "for batch_ix, batch in tqdm(enumerate(test_dl), total = len(test_dl)):\n",
    "\n",
    "    input_ids = batch['input_ids'].to(main_device)\n",
    "    attention_mask = batch['attention_mask'].to(main_device)\n",
    "    original_tokens = batch['original_tokens']\n",
    "    q_indices = batch['q_indices']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = run_model_with_path_ablation(model, input_ids, attention_mask, ablation_targets = path_ablation_targets, ablate_if_in_topk = True)\n",
    "    \n",
    "    # Check no bugs by validating output/perplexity\n",
    "    if batch_ix == 0:\n",
    "        loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), model.config.vocab_size).detach().cpu().item()\n",
    "        for i in range(min(5, input_ids.size(0))):\n",
    "            decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = True)\n",
    "            next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "            print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "        print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "    \n",
    "    # Create mapping of seq_ix to q_ix, so we can drop batch_ix and seq_ix and return q_ix\n",
    "    seq_to_q_map = pd.DataFrame({'sequence_ix': list(range(0, input_ids.shape[0])), 'q_ix': batch['q_indices']})\n",
    "\n",
    "    # Convert original tokens back to (seq_ix, token_ix) level for later storage\n",
    "    original_tokens_df = pd.DataFrame(\n",
    "        [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "        columns = ['sequence_ix', 'token_ix', 'token']\n",
    "    )\n",
    "\n",
    "    # Final token outputs - get the decoded outputs for the final tokens only, since convert_outputs_to_df only returns output tokens as IDs\n",
    "    question_token_outputs = tokenizer.batch_decode(torch.argmax(output['logits'][:, -1, :], dim = 1).tolist())\n",
    "    question_token_outputs_df = pd.DataFrame({'sequence_ix': list(range(0, input_ids.shape[0])), 'question_output_token': question_token_outputs})\n",
    "\n",
    "    # Create sample (token) level dataframe\n",
    "    sample_df =\\\n",
    "        convert_outputs_to_df(input_ids, attention_mask, output['logits'])\\\n",
    "        .merge(question_token_outputs_df, how = 'left', on = ['sequence_ix'])\\\n",
    "        .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "        .merge(seq_to_q_map, how = 'inner', on = ['sequence_ix'])\\\n",
    "        .drop(columns = ['sequence_ix'])\n",
    "    \n",
    "    path_sample_dfs.append(sample_df)\n",
    "\n",
    "    if batch_ix > 0 and batch_ix % 20 == 0:\n",
    "        print(output['num_ablations_applied'])\n",
    "        display(check_accuracy(pd.concat(path_sample_dfs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42853370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export results\n",
    "\"\"\"\n",
    "path_sample_df = pd.concat(path_sample_dfs)\n",
    "path_accuracy_df = check_accuracy(path_sample_df)\n",
    "display(path_accuracy_df)\n",
    "\n",
    "path_sample_df.to_csv(f\"data/{model_prefix}/path_ablation_samples_{test_domain}_{test_lang}_{test_k}.csv\", index = False)\n",
    "path_accuracy_df.to_csv(f\"data/{model_prefix}/path_ablation_accuracy_{test_domain}_{test_lang}_{test_k}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13102a9",
   "metadata": {},
   "source": [
    "## Run expert ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b19cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes with expert ablation\n",
    "\"\"\"\n",
    "expert_sample_dfs = []\n",
    "\n",
    "for batch_ix, batch in tqdm(enumerate(test_dl), total = len(test_dl)):\n",
    "\n",
    "    input_ids = batch['input_ids'].to(main_device)\n",
    "    attention_mask = batch['attention_mask'].to(main_device)\n",
    "    original_tokens = batch['original_tokens']\n",
    "    q_indices = batch['q_indices']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = run_model_with_expert_ablation(model, input_ids, attention_mask, ablation_targets = expert_ablation_targets, ablate_if_in_topk = False)\n",
    "    \n",
    "    # Check no bugs by validating output/perplexity\n",
    "    if batch_ix == 0:\n",
    "        loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), model.config.vocab_size).detach().cpu().item()\n",
    "        for i in range(min(5, input_ids.size(0))):\n",
    "            decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = True)\n",
    "            next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "            print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "        print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "    \n",
    "    # Create mapping of seq_ix to q_ix, so we can drop batch_ix and seq_ix and return q_ix\n",
    "    seq_to_q_map = pd.DataFrame({'sequence_ix': list(range(0, input_ids.shape[0])), 'q_ix': batch['q_indices']})\n",
    "\n",
    "    # Convert original tokens back to (seq_ix, token_ix) level for later storage\n",
    "    original_tokens_df = pd.DataFrame(\n",
    "        [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "        columns = ['sequence_ix', 'token_ix', 'token']\n",
    "    )\n",
    "\n",
    "    # Final token outputs - get the decoded outputs for the final tokens only, since convert_outputs_to_df only returns output tokens as IDs\n",
    "    question_token_outputs = tokenizer.batch_decode(torch.argmax(output['logits'][:, -1, :], dim = 1).tolist())\n",
    "    question_token_outputs_df = pd.DataFrame({'sequence_ix': list(range(0, input_ids.shape[0])), 'question_output_token': question_token_outputs})\n",
    "\n",
    "    # Create sample (token) level dataframe\n",
    "    sample_df =\\\n",
    "        convert_outputs_to_df(input_ids, attention_mask, output['logits'])\\\n",
    "        .merge(question_token_outputs_df, how = 'left', on = ['sequence_ix'])\\\n",
    "        .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "        .merge(seq_to_q_map, how = 'inner', on = ['sequence_ix'])\\\n",
    "        .drop(columns = ['sequence_ix'])\n",
    "    \n",
    "    expert_sample_dfs.append(sample_df)\n",
    "\n",
    "    if batch_ix > 0 and batch_ix % 20 == 0:\n",
    "        print(output['num_ablations_applied'])\n",
    "        display(check_accuracy(pd.concat(expert_sample_dfs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export results\n",
    "\"\"\"\n",
    "expert_sample_df = pd.concat(expert_sample_dfs)\n",
    "expert_accuracy_df = check_accuracy(expert_sample_df)\n",
    "display(expert_accuracy_df)\n",
    "\n",
    "expert_sample_df.to_csv(f'data/{model_prefix}/expert_ablation_samples_{test_domain}_{test_lang}_{test_k}.csv', index = False)\n",
    "expert_accuracy_df.to_csv(f'data/{model_prefix}/expert_ablation_accuracy_{test_domain}_{test_lang}_{test_k}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e317f8",
   "metadata": {},
   "source": [
    "## Multipath testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Run forward passes with path ablation\n",
    "# \"\"\"\n",
    "# multipath_sample_dfs = []\n",
    "\n",
    "# for batch_ix, batch in tqdm(enumerate(test_dl), total = len(test_dl)):\n",
    "\n",
    "#     input_ids = batch['input_ids'].to(main_device)\n",
    "#     attention_mask = batch['attention_mask'].to(main_device)\n",
    "#     original_tokens = batch['original_tokens']\n",
    "#     q_indices = batch['q_indices']\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         output = run_model_with_path_ablation(model, input_ids, attention_mask, ablation_targets = multipath_ablation_targets, ablate_if_in_topk = False)\n",
    "    \n",
    "#     # Check no bugs by validating output/perplexity\n",
    "#     if batch_ix == 0:\n",
    "#         loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), model.config.vocab_size).detach().cpu().item()\n",
    "#         for i in range(min(5, input_ids.size(0))):\n",
    "#             decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = True)\n",
    "#             next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "#             print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "#         print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "    \n",
    "#     # Create mapping of seq_ix to q_ix, so we can drop batch_ix and seq_ix and return q_ix\n",
    "#     seq_to_q_map = pd.DataFrame({'sequence_ix': list(range(0, input_ids.shape[0])), 'q_ix': batch['q_indices']})\n",
    "\n",
    "#     # Convert original tokens back to (seq_ix, token_ix) level for later storage\n",
    "#     original_tokens_df = pd.DataFrame(\n",
    "#         [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "#         columns = ['sequence_ix', 'token_ix', 'token']\n",
    "#     )\n",
    "\n",
    "#     # Final token outputs - get the decoded outputs for the final tokens only, since convert_outputs_to_df only returns output tokens as IDs\n",
    "#     question_token_outputs = tokenizer.batch_decode(torch.argmax(output['logits'][:, -1, :], dim = 1).tolist())\n",
    "#     question_token_outputs_df = pd.DataFrame({'sequence_ix': list(range(0, input_ids.shape[0])), 'question_output_token': question_token_outputs})\n",
    "\n",
    "#     # Create sample (token) level dataframe\n",
    "#     sample_df =\\\n",
    "#         convert_outputs_to_df(input_ids, attention_mask, output['logits'])\\\n",
    "#         .merge(question_token_outputs_df, how = 'left', on = ['sequence_ix'])\\\n",
    "#         .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "#         .merge(seq_to_q_map, how = 'inner', on = ['sequence_ix'])\\\n",
    "#         .drop(columns = ['sequence_ix'])\n",
    "    \n",
    "#     multipath_sample_dfs.append(sample_df)\n",
    "\n",
    "#     if batch_ix > 0 and batch_ix % 20 == 0:\n",
    "#         print(output['num_ablations_applied'])\n",
    "#         display(check_accuracy(pd.concat(multipath_sample_dfs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
