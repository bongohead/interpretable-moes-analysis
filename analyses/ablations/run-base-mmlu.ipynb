{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f6489",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export base MMLU performance.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.store_outputs import convert_outputs_to_df\n",
    "from utils import pretrained_models\n",
    "\n",
    "import pickle\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d22c6",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\n",
    "Architectures supported currently:\n",
    "- OlMoE architecture, includes OLMoE-1B-7B-0125-Instruct (1B/7B)\n",
    "- Qwen2MoE architecture, inclues Qwen1.5-MoE-A2.7B-Chat (2.7B/14.3B), Qwen2-57B-A14B (14B/57B)\n",
    "- Deepseek v2 architecture, includes Deepseek-v2-Lite (2.4B/15.7B), Deepseek-v2 (21B/236B)\n",
    "- Deepseek v3 architecture, includes Deepseek-v3 (37B/671B), Deepseek-R1 (37B/671B), Moonlight-16B-A3B (3B/16B)\n",
    "- Qwen3MoE architecture, includes Qwen3-30B-A3B, Qwen3-235B-A22B\n",
    "\"\"\"\n",
    "selected_model_index = 4\n",
    "\n",
    "def get_model(index):\n",
    "    model = [\n",
    "        ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 'olmoe'),\n",
    "        ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 'qwen2moe'),\n",
    "        ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 'dsv2'),\n",
    "        ('moonshotai/Moonlight-16B-A3B', 'moonlight', 'dsv3'),\n",
    "        ('Qwen/Qwen3-30B-A3B', 'qwen3moe', 'qwen3moe')\n",
    "    ][index]\n",
    "\n",
    "    return model[0], model[1], model[2]\n",
    "\n",
    "model_id, model_prefix, model_architecture = get_model(selected_model_index)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f685d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load reverse-engineered forward pass functions that return topk expert IDs and weights\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_return_topk = getattr(model_module, f\"run_{model_architecture}_return_topk\")\n",
    "\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "    original_results = model(**inputs)\n",
    "    custom_results = run_model_return_topk(model, inputs['input_ids'], inputs['attention_mask'], return_hidden_states = False)\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    assert len(custom_results['all_topk_experts']) == len(custom_results['all_topk_weights']), 'Length of topk IDs and weights not equal'\n",
    "    print(f\"Length of topk: {len(custom_results['all_topk_experts'])}\")\n",
    "    print(f\"Topk size: {custom_results['all_topk_experts'][0].shape}\")\n",
    "    print(f\"First token topk IDs: {custom_results['all_topk_experts'][0][1,]}\")\n",
    "    print(f\"First token topk weights: {custom_results['all_topk_weights'][0][1,]}\")\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), model.config.vocab_size).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953cd66f",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4773ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Get MMLU data and domains to test\n",
    "\"\"\"\n",
    "mmlu_ds = {\n",
    "    lang: load_dataset(\"CohereLabs/Global-MMLU\", lang, split = 'test')\n",
    "    for lang in ['en']\n",
    "}\n",
    "\n",
    "display(pd.DataFrame(mmlu_ds['en']).groupby('subject', as_index = False).agg(count = ('sample_id', 'count')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict containing {final_domains: [source1, source2], ...}\n",
    "mmlu_domain_mappings = {\n",
    "    # 'accounting': ['professional_accounting'],\n",
    "    'math': ['elementary_mathematics'], \n",
    "    # 'statistics': ['high_school_statistics'],\n",
    "    'compsci': ['high_school_computer_science', 'college_computer_science'],\n",
    "    # 'chemistry': ['high_school_chemistry'],\n",
    "    'biology': ['high_school_biology'],\n",
    "    'physics': ['high_school_physics'],\n",
    "    'psychology': ['high_school_psychology'],\n",
    "    'philosophy': ['philosophy'],\n",
    "    'medicine': ['clinical_knowledge'],\n",
    "    'law': ['professional_law']\n",
    "}\n",
    "\n",
    "def get_mmlu_df(raw_ds, domain_map, max_questions_per_domain):\n",
    "    \"\"\"\n",
    "    Clean + prep MMLU dataset\n",
    "    \"\"\"\n",
    "    source_to_domain_map = {source: domain for domain, sources in domain_map.items() for source in sources} # Map each source => domain\n",
    "    final_ds = []\n",
    "    lang_ix = 0\n",
    "    for q in raw_ds:\n",
    "        if q['subject'] not in source_to_domain_map.keys():\n",
    "            continue\n",
    "        domain_count = len([x for x in final_ds if x['domain'] == source_to_domain_map[q['subject']]])\n",
    "        if domain_count >= max_questions_per_domain:\n",
    "            continue\n",
    "        final_ds.append({\n",
    "            'lang_ix': lang_ix,\n",
    "            'domain_ix': domain_count,\n",
    "            'source_id': q['sample_id'],\n",
    "            'stem': q['question'],\n",
    "            'choices': [q['option_a'], q['option_b'], q['option_c'], q['option_d']],\n",
    "            'domain': source_to_domain_map[q['subject']],\n",
    "            'answer_char': q['answer']\n",
    "        })\n",
    "        lang_ix += 1\n",
    "\n",
    "    return pd.DataFrame(final_ds)\n",
    "\n",
    "mmlu_raw_dfs_by_lang = {\n",
    "    lang: get_mmlu_df(ds, mmlu_domain_mappings, 250).assign(lang = lang)\n",
    "    for lang, ds in mmlu_ds.items()\n",
    "}\n",
    "\n",
    "# assert all(mmlu_raw_dfs_by_lang['en']['source_id'] == mmlu_raw_dfs_by_lang['es']['source_id'])\n",
    "# assert all(mmlu_raw_dfs_by_lang['en']['source_id'] == mmlu_raw_dfs_by_lang['zh']['source_id'])\n",
    "\n",
    "mmlu_raw_df = pd.concat([df for _, df in mmlu_raw_dfs_by_lang.items()]).pipe(lambda df: df.assign(q_ix = list(range(0, len(df)))))\n",
    "mmlu_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671b61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create function to map MMLU data into an instruct-formatted string\n",
    "\"\"\"\n",
    "append_think = '\\n<think>\\n\\n</think>\\n\\n' if model_prefix == 'qwen3moe' else ''\n",
    "\n",
    "def prep_question(question, choices):\n",
    "    prompt = f\"{question}\\n\"\n",
    "    for i, option in enumerate(choices):\n",
    "        letter = chr(65 + i)\n",
    "        prompt += f\"{letter}. {option}\\n\"\n",
    "    return prompt\n",
    "\n",
    "fs_ex = [\n",
    "    # [q for q in mmlu_ds['en'] if q['subject'] == 'anatomy'][0]\n",
    "    {'stem': 'What is 1 + 1?', 'option_a': '5', 'option_b': '2', 'option_c': '1', 'option_d': '4', 'answer': 'B'},\n",
    "    {'stem': 'A dog is a type of what?', 'option_a': 'Plant', 'option_b': 'Organ', 'option_c': 'Chemical', 'option_d': 'Animal', 'answer': 'D'}\n",
    "]\n",
    "\n",
    "base_prompt = [\n",
    "    {'role': 'system', 'content': 'You will be given a multiple-choice question. Respond only with: \"The correct answer is X\", replacing X with the character code of the correct answer.'},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[0]['stem'], [fs_ex[0]['option_a'], fs_ex[0]['option_b'], fs_ex[0]['option_c'], fs_ex[0]['option_d']])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + fs_ex[0]['answer']},\n",
    "    {'role': 'user', 'content': prep_question(fs_ex[1]['stem'], [fs_ex[1]['option_a'], fs_ex[1]['option_b'], fs_ex[1]['option_c'], fs_ex[1]['option_d']])},\n",
    "    {'role': 'assistant', 'content': 'The correct answer is ' + fs_ex[1]['answer']}\n",
    "]\n",
    "\n",
    "mmlu_df = \\\n",
    "    mmlu_raw_df\\\n",
    "    .assign(\n",
    "        input_prompt = lambda df: df.apply(\n",
    "            lambda q: tokenizer.apply_chat_template(\n",
    "                base_prompt + [{'role': 'user', 'content': prep_question(q['stem'], q['choices'])}, {'role': 'assistant', 'content': f'{append_think}The correct answer is'}],\n",
    "                tokenize = False,\n",
    "                add_generation_prompt = False,\n",
    "                continue_final_message = True # Otherwise appends eos token\n",
    "            ), \n",
    "            axis = 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "display(mmlu_df)\n",
    "print(mmlu_df['input_prompt'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader. The dataloader returns the original tokens - this is important for BPE tokenizers as otherwise it's difficult to reconstruct the correct string later!\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Must be under max length to confirm nothing was truncated\n",
    "print(\n",
    "    tokenizer(mmlu_df['input_prompt'].tolist(), add_special_tokens = False, max_length = 1024, padding = 'max_length', truncation = True, return_tensors = 'pt')\\\n",
    "        ['attention_mask']\\\n",
    "        .sum(dim = 1)\\\n",
    "        .max()\n",
    ")\n",
    "\n",
    "test_ds = ReconstructableTextDataset(mmlu_df['input_prompt'].tolist(), tokenizer, max_length = 1024, q_indices = mmlu_df['q_ix'].tolist())\n",
    "test_dl = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size = 4,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59112e7e",
   "metadata": {},
   "source": [
    "## Run first pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0da3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run forward passes - store token level dataframe and topk-level dataframe\n",
    "\"\"\"\n",
    "sample_dfs = []\n",
    "topk_dfs = []\n",
    "\n",
    "for batch_ix, batch in tqdm(enumerate(test_dl), total = len(test_dl)):\n",
    "\n",
    "    input_ids = batch['input_ids'].to(main_device)\n",
    "    attention_mask = batch['attention_mask'].to(main_device)\n",
    "    original_tokens = batch['original_tokens']\n",
    "    q_indices = batch['q_indices']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = run_model_return_topk(model, input_ids, attention_mask, return_hidden_states = False)\n",
    "\n",
    "    # Check no bugs by validating output/perplexity\n",
    "    if batch_ix == 0:\n",
    "        loss = ForCausalLMLoss(output['logits'], torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids), model.config.vocab_size).detach().cpu().item()\n",
    "        for i in range(min(5, input_ids.size(0))):\n",
    "            decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = True)\n",
    "            next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "            print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "        print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "    \n",
    "    # Create mapping of seq_ix to q_ix, so we can drop batch_ix and seq_ix and return q_ix\n",
    "    seq_to_q_map = pd.DataFrame({'sequence_ix': list(range(0, input_ids.shape[0])), 'q_ix': batch['q_indices']})\n",
    "\n",
    "    # Convert original tokens back to (seq_ix, token_ix) level for later storage\n",
    "    original_tokens_df = pd.DataFrame(\n",
    "        [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "        columns = ['sequence_ix', 'token_ix', 'token']\n",
    "    )\n",
    "\n",
    "    # Final token outputs - get the decoded outputs for the final tokens only, since convert_outputs_to_df only returns output tokens as IDs\n",
    "    question_token_outputs = tokenizer.batch_decode(torch.argmax(output['logits'][:, -1, :], dim = 1).tolist())\n",
    "    question_token_outputs_df = pd.DataFrame({'sequence_ix': list(range(0, input_ids.shape[0])), 'question_output_token': question_token_outputs})\n",
    "\n",
    "    # Create sample (token) level dataframe\n",
    "    sample_df =\\\n",
    "        convert_outputs_to_df(input_ids, attention_mask, output['logits'])\\\n",
    "        .merge(question_token_outputs_df, how = 'left', on = ['sequence_ix'])\\\n",
    "        .merge(original_tokens_df, how = 'left', on = ['token_ix', 'sequence_ix'])\\\n",
    "        .merge(seq_to_q_map, how = 'inner', on = ['sequence_ix'])\\\n",
    "        .drop(columns = ['sequence_ix'])\n",
    "    \n",
    "    # Create topk x layer_ix x sample level dataframe\n",
    "    topk_df =\\\n",
    "        convert_topk_to_df(input_ids, attention_mask, output['all_topk_experts'], output['all_topk_weights'])\\\n",
    "        .merge(seq_to_q_map, how = 'inner')\\\n",
    "        .drop(columns = ['sequence_ix', 'token_id'])\n",
    "\n",
    "    sample_dfs.append(sample_df)\n",
    "    topk_dfs.append(topk_df)\n",
    "\n",
    "    if batch_ix > 0 and batch_ix % 50 == 0:\n",
    "        display(\n",
    "            pd.concat(sample_dfs).merge(mmlu_df, how = 'inner', on = 'q_ix')\\\n",
    "                .groupby(['q_ix', 'lang_ix', 'domain_ix', 'source_id', 'domain', 'lang', 'question_output_token', 'answer_char'], as_index = False)\\\n",
    "                .agg(n_tokens = ('q_ix', 'count'))\\\n",
    "                .assign(is_correct = lambda df: np.where(df['question_output_token'].str.strip() == df['answer_char'], 1, 0))\\\n",
    "                .groupby(['domain', 'lang'], as_index = False)\\\n",
    "                .agg(n_accurate = ('is_correct', 'sum'), n_total = ('q_ix', 'count'))\\\n",
    "                .assign(accuracy = lambda df: df['n_accurate']/df['n_total'])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99285a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save data\n",
    "\"\"\"\n",
    "output_dir = f'data/{model_prefix}'\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "with open(f'{output_dir}/metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(\n",
    "        {\n",
    "            'mmlu_df': mmlu_df, \n",
    "            'test_ds': test_ds\n",
    "        },\n",
    "        f\n",
    "    )\n",
    "\n",
    "sample_df = pd.concat(sample_dfs).merge(mmlu_df, how = 'inner', on = 'q_ix').drop(columns = 'input_prompt')\n",
    "topk_df = pd.concat(topk_dfs)\n",
    "\n",
    "sample_df.to_csv(f'{output_dir}/samples.csv', mode = 'w', index = False)\n",
    "topk_df.to_csv(f'{output_dir}/topks.csv', mode = 'w', index = False)\n",
    "topk_df.pipe(lambda df: df[df['topk_ix'] == 1]).to_csv(f'{output_dir}/topk1s.csv', mode = 'w', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quick check for accuracy\n",
    "\"\"\"\n",
    "pd.concat(sample_dfs).merge(mmlu_df, how = 'inner', on = 'q_ix')\\\n",
    "    .groupby(['q_ix', 'lang_ix', 'domain_ix', 'source_id', 'domain', 'question_output_token', 'answer_char'], as_index = False)\\\n",
    "    .agg(n_tokens = ('q_ix', 'count'))\\\n",
    "    .assign(is_correct = lambda df: np.where(df['question_output_token'].str.strip() == df['answer_char'], 1, 0))\\\n",
    "    .groupby('domain', as_index = False)\\\n",
    "    .agg(n_accurate = ('is_correct', 'sum'), n_total = ('q_ix', 'count'))\\\n",
    "    .assign(accuracy = lambda df: df['n_accurate']/df['n_total'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
