{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Runs forward passes with different top-k values ablated (set to zero).\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea7af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.store_outputs import convert_outputs_to_df\n",
    "from utils import pretrained_models\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4ce3f",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\n",
    "Architectures supported currently:\n",
    "- OlMoE architecture, includes OLMoE-1B-7B-0125-Instruct (1B/7B)\n",
    "- Qwen2MoE architecture, inclues Qwen1.5-MoE-A2.7B-Chat (2.7B/14.3B), Qwen2-57B-A14B (14B/57B)\n",
    "- Deepseek v2 architecture, includes Deepseek-v2-Lite (2.4B/15.7B), Deepseek-v2 (21B/236B)\n",
    "- Deepseek v3 architecture, includes Deepseek-v3 (37B/671B), Deepseek-R1 (37B/671B), Moonlight-16B-A3B (3B/16B)\n",
    "- Qwen3MoE architecture, includes Qwen3-30B-A3B (3B/30B), Qwen3-235B-A22B (22B/235B)\n",
    "\"\"\"\n",
    "selected_model_index = 1\n",
    "\n",
    "def get_model(index):\n",
    "    model = [\n",
    "        ('allenai/OLMoE-1B-7B-0125-Instruct', 'olmoe', 'olmoe'),\n",
    "        ('Qwen/Qwen1.5-MoE-A2.7B-Chat', 'qwen1.5moe', 'qwen2moe'),\n",
    "        ('deepseek-ai/DeepSeek-V2-Lite', 'dsv2', 'dsv2'),\n",
    "        ('moonshotai/Moonlight-16B-A3B', 'moonlight', 'dsv3'),\n",
    "        ('Qwen/Qwen3-30B-A3B', 'qwen3moe', 'qwen3moe')\n",
    "    ][index]\n",
    "\n",
    "    return model[0], model[1], model[2]\n",
    "\n",
    "model_id, model_prefix, model_architecture = get_model(selected_model_index)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token = False, add_bos_token = False, padding_side = 'left', trust_remote_code = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, trust_remote_code = True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba52c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test ablation function and ensure that without ablated experts (topk_to_ablate = []), it returns the same response as the base model call\n",
    "\"\"\"\n",
    "model_module = importlib.import_module(f\"utils.pretrained_models.{model_architecture}\")\n",
    "run_model_with_ablation = getattr(model_module, f\"run_{model_architecture}_with_topk_ablation\")\n",
    "\n",
    "def test_custom_forward_pass(model, pad_token_id):\n",
    "    inputs = tokenizer(['Hi! I am a dog and I like to bark', 'Vegetables are good for'], return_tensors = 'pt', padding = 'max_length', truncation = True, max_length = 512).to(model.device)\n",
    "    original_results = model(**inputs)\n",
    "    custom_results = run_model_with_ablation(model, inputs['input_ids'], inputs['attention_mask'], layers_to_ablate = list(range(0, 100)), topk_to_ablate = [])\n",
    "    assert torch.equal(original_results.logits, custom_results['logits']), 'Error in custom forward'\n",
    "    loss = ForCausalLMLoss(custom_results['logits'], torch.where(inputs['input_ids'] == pad_token_id, torch.tensor(-100), inputs['input_ids']), model.config.vocab_size).detach().cpu().item()\n",
    "    print(f\"LM loss: {loss}\")\n",
    "\n",
    "test_custom_forward_pass(model, tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4026577d",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset - FW edu\n",
    "\"\"\"\n",
    "def load_raw_ds():\n",
    "    rng = np.random.default_rng(seed = seed)\n",
    "    ds_en = load_dataset('HuggingFaceFW/fineweb-edu', 'CC-MAIN-2024-51', split = 'train', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "    \n",
    "    def get_data(ds, n_samples):\n",
    "        raw_data = []\n",
    "        ds_iter = iter(ds)\n",
    "        for _ in range(n_samples):\n",
    "            sample = next(ds_iter, None)\n",
    "            if sample is None:\n",
    "                break\n",
    "            raw_data.append({'text': sample['text']})\n",
    "        \n",
    "        return raw_data\n",
    "    \n",
    "    combined_ds = get_data(ds_en, 100)\n",
    "\n",
    "    perm = rng.permutation(len(combined_ds))\n",
    "    combined_ds = [combined_ds[i] for i in perm]\n",
    "\n",
    "    return combined_ds\n",
    "\n",
    "raw_data = load_raw_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a520f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader.\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    ReconstructableTextDataset([x['text'] for x in raw_data], tokenizer, max_length = 512),\n",
    "    batch_size = 16,\n",
    "    shuffle = False,\n",
    "    collate_fn = stack_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729229e",
   "metadata": {},
   "source": [
    "## Ablation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30488bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_all_with_ablation(model, layers_to_ablate: list, topk_to_ablate: list, renorm: bool, verbose = False):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model with experts ablated (set to 0) with experts identified by top-k position\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run forward passes on.\n",
    "        @layers_to_ablate: The layers to ablate, 0-indexed.\n",
    "        @topk: The topk to ablate, 0-indexed.\n",
    "        @renorm: Whether to renormalize the expert weights after ablation.\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for batch_ix, batch in tqdm(enumerate(test_dl), total = len(test_dl), disable = not verbose):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(main_device)\n",
    "        attention_mask = batch['attention_mask'].to(main_device)\n",
    "\n",
    "        output = run_model_with_ablation(model, input_ids, attention_mask, layers_to_ablate = layers_to_ablate, topk_to_ablate = topk_to_ablate, renorm = renorm)\n",
    "\n",
    "        labels =  torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids)\n",
    "        batch_loss = ForCausalLMLoss(output['logits'], labels, model.config.vocab_size).detach().cpu().item()\n",
    "        token_count = (labels != -100).sum().item()\n",
    "\n",
    "        # Check no bugs by validating output/perplexity\n",
    "        if batch_ix == 0 and verbose:\n",
    "            for i in range(min(5, input_ids.size(0))):\n",
    "                decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = True)\n",
    "                next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "            print(f\"Tokens: {token_count:d} | PPL: {torch.exp(torch.tensor(batch_loss)).item():.2f}\")\n",
    "\n",
    "        total_loss += batch_loss * token_count\n",
    "        total_tokens += token_count\n",
    "    \n",
    "    avg_loss = total_loss/total_tokens\n",
    "    avg_ppl = torch.exp(torch.tensor(avg_loss)).item()\n",
    "\n",
    "    return {\n",
    "        'total_loss': total_loss,\n",
    "        'total_tokens': total_tokens,\n",
    "        'avg_loss': avg_loss,\n",
    "        'avg_ppl': avg_ppl\n",
    "    }\n",
    "\n",
    "\n",
    "if model_prefix == 'olmoe':\n",
    "    all_layers = list(range(16))\n",
    "    all_topk = list(range(8))\n",
    "elif model_prefix == 'qwen1.5moe':\n",
    "    all_layers = list(range(25))\n",
    "    all_topk = list(range(4))\n",
    "elif model_prefix == 'dsv2':\n",
    "    all_layers = list(range(26))\n",
    "    all_topk = list(range(6))\n",
    "elif model_prefix == 'moonlight':\n",
    "    all_layers = list(range(26))\n",
    "    all_topk = list(range(6))\n",
    "elif model_prefix == 'qwen3moe':\n",
    "    all_layers = list(range(48))\n",
    "    all_topk = list(range(8))\n",
    "\n",
    "base_result = run_all_with_ablation(model, [], [], False)\n",
    "base_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f3f7af",
   "metadata": {},
   "source": [
    "## Basic topk ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ablating single topk's\n",
    "\"\"\"\n",
    "print('----- No Renorm -----')\n",
    "for ablation_k in all_topk:\n",
    "    ablate_res = run_all_with_ablation(model, all_layers, [ablation_k], False)\n",
    "    print(f\"Ablated topk={ablation_k} => PPL: {ablate_res['avg_ppl']:.2f}\")\n",
    "\n",
    "print('----- Renorm -----')\n",
    "for ablation_k in all_topk:\n",
    "    ablate_res = run_all_with_ablation(model, all_layers, [ablation_k], True)\n",
    "    print(f\"Ablated topk={ablation_k} => PPL: {ablate_res['avg_ppl']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ablating all smaller topks\n",
    "\"\"\"\n",
    "print('----- No Renorm -----')\n",
    "for ablation_k in all_topk:\n",
    "    ablate_res = run_all_with_ablation(model, all_layers, list(range(ablation_k, max(all_topk) + 1)), False)\n",
    "    print(f\"Ablated topk>={ablation_k} => PPL: {ablate_res['avg_ppl']:.2f}\")\n",
    "\n",
    "print('----- Renorm -----')\n",
    "for ablation_k in all_topk:\n",
    "    ablate_res = run_all_with_ablation(model, all_layers, list(range(ablation_k, max(all_topk) + 1)), True)\n",
    "    print(f\"Ablated topk>={ablation_k} => PPL: {ablate_res['avg_ppl']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb916af",
   "metadata": {},
   "source": [
    "## Layer-targeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bcec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ablation_k in all_topk:\n",
    "    # int(np.floor(np.median(all_layers)))\n",
    "    ablate_res = run_all_with_ablation(\n",
    "        model,\n",
    "        [4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "        list(range(ablation_k, max(all_topk) + 1)),\n",
    "        False\n",
    "    )\n",
    "    \n",
    "    print(f\"Ablated topk>={ablation_k} => PPL: {ablate_res['avg_ppl']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ablation_k in all_topk:\n",
    "    # int(np.floor(np.median(all_layers)))\n",
    "    ablate_res = run_all_with_ablation(\n",
    "        model,\n",
    "        [0, 1, 2],\n",
    "        list(range(ablation_k, max(all_topk) + 1)),\n",
    "        False\n",
    "    )\n",
    "    \n",
    "    print(f\"Ablated topk>={ablation_k} => PPL: {ablate_res['avg_ppl']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ablation_k in all_topk:\n",
    "    # int(np.floor(np.median(all_layers)))\n",
    "    ablate_res = run_all_with_ablation(\n",
    "        model,\n",
    "        [13, 14, 15],\n",
    "        list(range(ablation_k, max(all_topk) + 1)),\n",
    "        False\n",
    "    )\n",
    "    \n",
    "    print(f\"Ablated topk>={ablation_k} => PPL: {ablate_res['avg_ppl']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25249b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ablation_k in all_topk:\n",
    "    # int(np.floor(np.median(all_layers)))\n",
    "    ablate_res = run_all_with_ablation(\n",
    "        model,\n",
    "        [10, 11, 12],\n",
    "        list(range(ablation_k, max(all_topk) + 1)),\n",
    "        False\n",
    "    )\n",
    "    \n",
    "    print(f\"Ablated topk>={ablation_k} => PPL: {ablate_res['avg_ppl']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ablation_k in all_topk:\n",
    "    # int(np.floor(np.median(all_layers)))\n",
    "    ablate_res = run_all_with_ablation(\n",
    "        model,\n",
    "        [4, 5, 6],\n",
    "        list(range(ablation_k, max(all_topk) + 1)),\n",
    "        False\n",
    "    )\n",
    "    \n",
    "    print(f\"Ablated topk>={ablation_k} => PPL: {ablate_res['avg_ppl']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
