{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to use SVD to decompose hidden states based on whether they're used by routing or not.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy\n",
    "import cuml\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "import regex\n",
    "import scipy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.loader import load_model_and_tokenizer\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "from utils.svd import decompose_orthogonal, decompose_sideways, get_svd_proj\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()\n",
    "\n",
    "ws = '/workspace/interpretable-moes-analysis'\n",
    "svd_dir = f'{ws}/experiments/geometry/svd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_prefix = 'gpt-oss-20b'\n",
    "tokenizer, model, model_architecture, model_n_moe_layers, model_n_dense_layers = load_model_and_tokenizer(model_prefix, device = main_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get 0-indexed MoE layer indices\n",
    "\"\"\"\n",
    "with open(f'{ws}/experiments/geometry/activations/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "    layer_indices = pickle.load(f).get('layer_mappings')\n",
    "\n",
    "layer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get projection matrices\n",
    "\"\"\"\n",
    "v_mats = {}\n",
    "v_mats_demeaned = {}\n",
    "router_objs = {}\n",
    "\n",
    "for layer_ix in tqdm(range(0, model_n_dense_layers + model_n_moe_layers)):\n",
    "    if layer_ix not in layer_indices:\n",
    "        continue\n",
    "    if model_prefix == 'kimivl': \n",
    "        gate_obj = model.language_model.model.layers[layer_ix].mlp.gate.weight\n",
    "    elif model_prefix == 'granite':\n",
    "        gate_obj = model.model.layers[layer_ix].block_sparse_moe.router.layer.weight\n",
    "    else:\n",
    "        gate_obj = model.model.layers[layer_ix].mlp.router.weight\n",
    "\n",
    "    router_obj = gate_obj.detach().cpu().to(torch.float32)\n",
    "    U, S, V = get_svd_proj(router_obj)\n",
    "    U_demeaned, S_demeaned, V_demeaned = get_svd_proj(router_obj - router_obj.mean(dim = 0, keepdim = True))\n",
    "    \n",
    "    v_mats[layer_ix] = V\n",
    "    v_mats_demeaned[layer_ix] = V_demeaned\n",
    "    router_objs[layer_ix] = router_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix, max_data_files):\n",
    "    \"\"\"\n",
    "    Load data saved by `export-activations.ipynb`\n",
    "    \"\"\"\n",
    "    folders = [f'{ws}/experiments/geometry/activations/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    sample_parts = []\n",
    "    topk_parts = []\n",
    "    hs_parts = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_parts.append(pd.read_feather(f'{f}/samples.feather'))\n",
    "        topk_parts.append(pd.read_feather(f'{f}/topks.feather'))\n",
    "        tensors = load_file(f'{f}/activations.safetensors', device = 'cpu')\n",
    "        hs_parts.append(tensors['all_pre_mlp_hs'])\n",
    "\n",
    "    sample_df = pd.concat(sample_parts, ignore_index = True)\n",
    "    topk_df = pd.concat(topk_parts, ignore_index = True)\n",
    "    pre_mlp_hs = torch.concat(hs_parts, dim = 0)\n",
    "\n",
    "    prompts_df = pd.read_feather(f'{ws}/experiments/geometry/activations/{model_prefix}/prompts.feather')\n",
    "    \n",
    "    gc.collect()\n",
    "    return prompts_df, sample_df, topk_df, pre_mlp_hs, layer_indices\n",
    "\n",
    "# 5 except glm4moe=3\n",
    "prompts_df, sample_df_import, topk_df_import, all_pre_mlp_hs_import = load_data(model_prefix, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df[['sample_ix', 'prompt_ix', 'token_ix']], how = 'inner', on = ['prompt_ix', 'token_ix'])\\\n",
    "    .drop(columns = ['token_ix'])\\\n",
    "    .assign(layer_ix = lambda df: df['layer_ix'] + model_n_dense_layers)\n",
    "\n",
    "def get_sample_df_for_layer(sample_df, topk_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Helper to take the sample df and merge layer-level expert selection information\n",
    "    \"\"\"\n",
    "    topk_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix])\n",
    "    topk_l1_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 1])\n",
    "    topk_l2_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 2])\n",
    "\n",
    "    layer_df =\\\n",
    "        sample_df\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 1])[['sample_ix', 'expert']], how = 'inner', on = 'sample_ix')\\\n",
    "        .merge(topk_l1_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_l2_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev2_expert'})[['sample_ix', 'prev2_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 2]).rename(columns = {'expert': 'expert2'})[['sample_ix', 'expert2']], how = 'left', on = 'sample_ix')\\\n",
    "        .assign(leading_path = lambda df: df['prev2_expert'] + '-' + df['prev_expert'])\n",
    "    \n",
    "    return layer_df\n",
    "\n",
    "del sample_df_import, topk_df_import\n",
    "\n",
    "gc.collect()\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {(layer_ix + model_n_dense_layers): all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(layer_indices)}\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How much the routing matrix itself rotates\n",
    "\"\"\"\n",
    "def get_proj_overlap(layer_ix):\n",
    "    M = (v_mats[layer_ix].T  @ v_mats[layer_ix + 1])\n",
    "    overlap = (M * M).sum() / min(v_mats[layer_ix].shape[1], v_mats[layer_ix + 1].shape[1])\n",
    "    rd_ratio = v_mats[layer_ix].shape[1] / model.config.hidden_size\n",
    "    return {'overlap': round(overlap.item(), 2), 'rd_ratio': round(rd_ratio, 2)}\n",
    "\n",
    "for layer_ix in list(all_pre_mlp_hs.keys())[:-1]:\n",
    "    proj_overlap = get_proj_overlap(layer_ix)\n",
    "    print(f'Layer {layer_ix}: {proj_overlap}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_mats[10].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vis(hl, ):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_blind calculations\n",
    "(all_pre_mlp_hs[layer_ix].float() @ proj_matrices[layer_ix]['V']) @ proj_matrices[layer_ix]['V'].T\n",
    "\n",
    "(all_pre_mlp_hs[layer_ix] @ v_mats[layer_ix].to(torch.float16)) @ v_mats[layer_ix].to(torch.float16).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ix = 10\n",
    "\n",
    "h_l = all_pre_mlp_hs[layer_ix].float()\n",
    "h_l1 = all_pre_mlp_hs[layer_ix + 1].float()\n",
    "\n",
    "v_mat_l1 = v_mats[layer_ix + 1]\n",
    "\n",
    "h_vis_l_l1 = (h_l @ v_mat_l1) @ v_mat_l1.T\n",
    "h_blind_l_l1 = h_l - h_vis_l_l1\n",
    "\n",
    "h_vis_l1_l1 = (h_l1 @ v_mat_l1) @ v_mat_l1.T\n",
    "h_blind_l1_l1 = h_l1 - h_vis_l1_l1\n",
    "\n",
    "para_cos_sim = torch.nn.functional.cosine_similarity(h_vis_l_l1, h_vis_l1_l1, dim = -1)\n",
    "para_cos_sim = para_cos_sim.mean(dim = 0)\n",
    "\n",
    "orth_cos_sim = torch.nn.functional.cosine_similarity(h_blind_l_l1, h_blind_l1_l1, dim = -1)\n",
    "orth_cos_sim = orth_cos_sim.mean(dim = 0)\n",
    "\n",
    "para_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orth_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ix = 8\n",
    "\n",
    "h_l  = all_pre_mlp_hs[layer_ix].float()\n",
    "h_l1 = all_pre_mlp_hs[layer_ix + 1].float()\n",
    "\n",
    "V_l = v_mats_demeaned[layer_ix].float()\n",
    "V_l1 = v_mats_demeaned[layer_ix + 1].float()\n",
    "\n",
    "ctrl_l  = (h_l  @ V_l) @ V_l.T\n",
    "ctrl_l1 = (h_l1 @ V_l1) @ V_l1.T\n",
    "\n",
    "rest_l  = h_l  - ctrl_l\n",
    "rest_l1 = h_l1 - ctrl_l1\n",
    "\n",
    "# cosine similarities per token\n",
    "cos_ctrl = torch.nn.functional.cosine_similarity(ctrl_l, ctrl_l1, dim = -1)\n",
    "cos_rest = torch.nn.functional.cosine_similarity(rest_l, rest_l1, dim = -1)\n",
    "cos_full = torch.nn.functional.cosine_similarity(h_l, h_l1, dim = -1)\n",
    "\n",
    "# reduce to scalar (mean over all tokens)\n",
    "cos_ctrl_mean = cos_ctrl.mean(dim = 0)\n",
    "cos_rest_mean = cos_rest.mean(dim = 0)\n",
    "cos_full_mean = cos_full.mean(dim = 0)\n",
    "\n",
    "cos_ctrl_mean, cos_rest_mean, cos_full_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ix = 8\n",
    "k_ahead = 1\n",
    "\n",
    "h_l  = all_pre_mlp_hs[layer_ix].float()\n",
    "h_l1 = all_pre_mlp_hs[layer_ix + k_ahead].float()\n",
    "\n",
    "V_l = v_mats_demeaned[layer_ix].float() # (D, r)\n",
    "V_l1 = v_mats_demeaned[layer_ix + k_ahead].float() # (D, r)\n",
    "\n",
    "# Control part relative to layer l+1 router directions: P_{l+1} h\n",
    "ctrl_l  = (h_l  @ V_l1) @ V_l1.T\n",
    "ctrl_l1 = (h_l1 @ V_l1) @ V_l1.T\n",
    "\n",
    "# Rest relative to same directions: (I - P_{l+1}) h\n",
    "rest_l  = h_l  - ctrl_l\n",
    "rest_l1 = h_l1 - ctrl_l1\n",
    "\n",
    "# cosine similarities per token\n",
    "cos_ctrl = torch.nn.functional.cosine_similarity(ctrl_l, ctrl_l1, dim = -1)\n",
    "cos_rest = torch.nn.functional.cosine_similarity(rest_l, rest_l1, dim = -1)\n",
    "cos_full = torch.nn.functional.cosine_similarity(h_l, h_l1, dim = -1)\n",
    "\n",
    "# reduce to scalar (mean over all tokens)\n",
    "cos_ctrl_mean = cos_ctrl.mean(dim = 0)\n",
    "cos_rest_mean = cos_rest.mean(dim = 0)\n",
    "cos_full_mean = cos_full.mean(dim = 0)\n",
    "\n",
    "cos_ctrl_mean, cos_rest_mean, cos_full_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = h_l.float()   # N x D\n",
    "y = h_l1.float()  # N x D\n",
    "N, D = x.shape\n",
    "r = V_l1.shape[1]\n",
    "\n",
    "def rand_Q(D, r, device):\n",
    "    G = torch.randn(D, r, device=device)\n",
    "    Q, _ = torch.linalg.qr(G, mode='reduced')   # D x r, orthonormal cols\n",
    "    return Q\n",
    "\n",
    "# your observed\n",
    "a = x @ V_l1.float()\n",
    "b = y @ V_l1.float()\n",
    "cos_router = F.cosine_similarity(a, b, dim=-1).mean().item()\n",
    "\n",
    "# baseline distribution\n",
    "vals = []\n",
    "for _ in range(50):   # 50 is often enough\n",
    "    Q = rand_Q(D, r, x.device)\n",
    "    ax = x @ Q\n",
    "    by = y @ Q\n",
    "    vals.append(F.cosine_similarity(ax, by, dim=-1).mean().item())\n",
    "\n",
    "cos_router, (sum(vals)/len(vals), min(vals), max(vals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_from_h_l  = x @ R_l1.T\n",
    "scores_from_h_l1 = y @ R_l1.T\n",
    "top1_match = (scores_from_h_l.argmax(-1) == scores_from_h_l1.argmax(-1)).float().mean().item()\n",
    "top1_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = h_l1 - h_l\n",
    "delta_ctrl = (delta @ V_l1)          # r-dim coefficients\n",
    "ctrl_prev  = (h_l @ V_l1)\n",
    "ratio = delta_ctrl.norm(dim=-1) / (ctrl_prev.norm(dim=-1) + 1e-8)\n",
    "\n",
    "ratio.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_l_in_l   = h_l  @ V_l          # [N,r]\n",
    "z_l_in_l1  = h_l  @ V_l1         # [N,r]\n",
    "M = V_l.T @ V_l1                 # [r,r]\n",
    "\n",
    "z_shared = z_l_in_l @ M          # [N,r]\n",
    "frac_shared = (z_shared.norm(dim=-1)**2 / (z_l_in_l1.norm(dim=-1)**2 + 1e-8)).mean()\n",
    "\n",
    "frac_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_ctrl_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_ctrl_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ix = 10\n",
    "h_l = all_pre_mlp_hs[layer_ix].float()\n",
    "h_l1 = all_pre_mlp_hs[layer_ix + 1].float()\n",
    "V = v_mats[layer_ix + 1].float()  # D x r\n",
    "\n",
    "# control coefficients\n",
    "a = h_l @ V  \n",
    "b = h_l1 @ V\n",
    "\n",
    "ctrl_cos = torch.nn.functional.cosine_similarity(a, b, dim=-1).mean()\n",
    "\n",
    "# reconstruct for rest\n",
    "ctrl_l = a @ V.T  \n",
    "ctrl_l1 = b @ V.T\n",
    "rest_l = h_l - ctrl_l\n",
    "rest_l1 = h_l1 - ctrl_l1\n",
    "\n",
    "rest_cos = torch.nn.functional.cosine_similarity(rest_l, rest_l1, dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sims = torch.nn.functional.cosine_similarity(\n",
    "    (all_pre_mlp_hs[layer_ix] @ v_mats[layer_ix].to(torch.float16)) @ v_mats[layer_ix].to(torch.float16).T,\n",
    "    (all_pre_mlp_hs[layer_ix] @ v_mats[layer_ix + 1].to(torch.float16)) @ v_mats[layer_ix + 1].to(torch.float16).T,\n",
    "    dim = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pre_mlp_hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get row-space rotation stability + CIs\n",
    "\"\"\"\n",
    "def get_bootstrap_cos_sims(hs_by_layer: dict[int, torch.Tensor], *, doc_ids: list[int], bs_samples: int = 40, samples_per_bs: int = 1_000):\n",
    "    \"\"\"\n",
    "    Get cross-layer nonparametric bootstrap samples\n",
    "    \"\"\"\n",
    "    g = torch.Generator().manual_seed(123)\n",
    "\n",
    "    layer_indices = sorted(hs_by_layer.keys())\n",
    "    adj_layers = [(a, b) for a, b in zip(layer_indices, layer_indices[1:]) if b == a + 1]\n",
    "    results = torch.empty((bs_samples, len(adj_layers)), dtype = torch.float)\n",
    "\n",
    "    # Doc ids for block bootstrap\n",
    "    docs = torch.as_tensor(doc_ids)\n",
    "    uniq = docs.unique()\n",
    "    idxs_by_doc = [(docs == d).nonzero(as_tuple = True)[0] for d in uniq]\n",
    "\n",
    "    for bootstrap_ix in tqdm(range(bs_samples)):\n",
    "        # sample documents with replacement until we have >= samples_per_bs tokens\n",
    "        take_idxs = []\n",
    "        total = 0\n",
    "        while total < samples_per_bs:\n",
    "            j = int(torch.randint(low = 0, high = len(idxs_by_doc), size = (1,), generator = g))\n",
    "            idx = idxs_by_doc[j]\n",
    "            take_idxs.append(idx)\n",
    "            total += idx.numel()\n",
    "        sample_indices = torch.cat(take_idxs)[:samples_per_bs]\n",
    "\n",
    "        # sample_indices = torch.randint(low = 0, high = n_samples, size = (samples_per_bs,), generator = g)\n",
    "        # Stack needed pairs (cur, next) each of shape (samples_per_bs, adj_layers, D)\n",
    "        lhs = torch.stack([hs_by_layer[a][sample_indices] for a, _ in adj_layers], dim = 1)\n",
    "        rhs = torch.stack([hs_by_layer[b][sample_indices] for _, b in adj_layers], dim = 1)\n",
    "\n",
    "        # Return (samples_per_bs, adj_layers) -> mean over samples to get (adj_layers)\n",
    "        cos_sims = torch.nn.functional.cosine_similarity(lhs, rhs, dim = -1)\n",
    "        results[bootstrap_ix] = cos_sims.mean(dim = 0)\n",
    "\n",
    "    means = results.mean(dim = 0)\n",
    "    cis_lo, cis_hi = torch.quantile(results, torch.tensor([0.005, 0.995], dtype = results[0].dtype), dim = 0)\n",
    "\n",
    "    return means.numpy(), cis_lo.numpy(), cis_hi.numpy()\n",
    "\n",
    "para_means, para_cis_lo, para_cis_hi = get_bootstrap_cos_sims(h_para_by_layer, doc_ids = sample_df['seq_id'].tolist(), bs_samples = 200, samples_per_bs = 30)\n",
    "\n",
    "print(f\"Mean across layer transitions + samples: {para_means.mean():.2f} +/- {(para_cis_hi - para_means).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's take the pre-MLP hidden states and split them using SVD into parallel and orthogonal components.\n",
    "\"\"\"\n",
    "h_para_by_layer = {}\n",
    "h_orth_by_layer = {}\n",
    "\n",
    "for layer_ix in tqdm(list(all_pre_mlp_hs.keys())):\n",
    "    if model_prefix == 'kimivl': \n",
    "        gate_obj = model.language_model.model.layers[layer_ix].mlp.gate.weight\n",
    "    elif model_prefix == 'granite':\n",
    "        gate_obj = model.model.layers[layer_ix].block_sparse_moe.router.layer.weight\n",
    "    else:\n",
    "        gate_obj = model.model.layers[layer_ix].mlp.router.weight\n",
    "\n",
    "    h_para_by_layer[layer_ix], h_orth_by_layer[layer_ix] = decompose_orthogonal(\n",
    "        all_pre_mlp_hs[layer_ix].to(torch.float32),\n",
    "        gate_obj.detach().cpu().to(torch.float32),\n",
    "        'svd'\n",
    "    )\n",
    "\n",
    "# The model can be deleted now to clear memory if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orth vs Para Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get row-space rotation stability + CIs\n",
    "\"\"\"\n",
    "def get_bootstrap_cos_sims(hs_by_layer: dict[int, torch.Tensor], *, doc_ids: list[int], bs_samples: int = 40, samples_per_bs: int = 1_000):\n",
    "    \"\"\"\n",
    "    Get cross-layer nonparametric bootstrap samples\n",
    "    \"\"\"\n",
    "    g = torch.Generator().manual_seed(123)\n",
    "\n",
    "    layer_indices = sorted(hs_by_layer.keys())\n",
    "    adj_layers = [(a, b) for a, b in zip(layer_indices, layer_indices[1:]) if b == a + 1]\n",
    "    results = torch.empty((bs_samples, len(adj_layers)), dtype = torch.float)\n",
    "\n",
    "    # Doc ids for block bootstrap\n",
    "    docs = torch.as_tensor(doc_ids)\n",
    "    uniq = docs.unique()\n",
    "    idxs_by_doc = [(docs == d).nonzero(as_tuple = True)[0] for d in uniq]\n",
    "\n",
    "    for bootstrap_ix in tqdm(range(bs_samples)):\n",
    "        # sample documents with replacement until we have >= samples_per_bs tokens\n",
    "        take_idxs = []\n",
    "        total = 0\n",
    "        while total < samples_per_bs:\n",
    "            j = int(torch.randint(low = 0, high = len(idxs_by_doc), size = (1,), generator = g))\n",
    "            idx = idxs_by_doc[j]\n",
    "            take_idxs.append(idx)\n",
    "            total += idx.numel()\n",
    "        sample_indices = torch.cat(take_idxs)[:samples_per_bs]\n",
    "\n",
    "        # sample_indices = torch.randint(low = 0, high = n_samples, size = (samples_per_bs,), generator = g)\n",
    "        # Stack needed pairs (cur, next) each of shape (samples_per_bs, adj_layers, D)\n",
    "        lhs = torch.stack([hs_by_layer[a][sample_indices] for a, _ in adj_layers], dim = 1)\n",
    "        rhs = torch.stack([hs_by_layer[b][sample_indices] for _, b in adj_layers], dim = 1)\n",
    "\n",
    "        # Return (samples_per_bs, adj_layers) -> mean over samples to get (adj_layers)\n",
    "        cos_sims = torch.nn.functional.cosine_similarity(lhs, rhs, dim = -1)\n",
    "        results[bootstrap_ix] = cos_sims.mean(dim = 0)\n",
    "\n",
    "    means = results.mean(dim = 0)\n",
    "    cis_lo, cis_hi = torch.quantile(results, torch.tensor([0.005, 0.995], dtype = results[0].dtype), dim = 0)\n",
    "\n",
    "    return means.numpy(), cis_lo.numpy(), cis_hi.numpy()\n",
    "\n",
    "para_means, para_cis_lo, para_cis_hi = get_bootstrap_cos_sims(h_para_by_layer, doc_ids = sample_df['seq_id'].tolist(), bs_samples = 200, samples_per_bs = 30)\n",
    "\n",
    "print(f\"Mean across layer transitions + samples: {para_means.mean():.2f} +/- {(para_cis_hi - para_means).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get null-space rotation stability + CIs\n",
    "\"\"\"\n",
    "orth_means, orth_cis_lo, orth_cis_hi = get_bootstrap_cos_sims(h_orth_by_layer, doc_ids = sample_df['seq_id'].tolist(), bs_samples = 200, samples_per_bs = 30)\n",
    "\n",
    "print(f\"Mean across layer transitions + samples: {orth_means.mean():.2f} +/- {(orth_cis_hi - orth_means).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "export_df = pd.DataFrame({\n",
    "    'layer_ix_1': list(range(model_n_dense_layers + 1, len(all_pre_mlp_hs) + model_n_dense_layers)), # +1 to 1 index\n",
    "    'para_mean_across_layers': para_means,\n",
    "    'orth_mean_across_layers': orth_means,\n",
    "    'para_cis_hi': para_cis_hi,\n",
    "    'para_cis_lo': para_cis_lo,\n",
    "    'orth_cis_hi': orth_cis_hi,\n",
    "    'orth_cis_lo': orth_cis_lo\n",
    "})\n",
    "\n",
    "display(export_df)\n",
    "\n",
    "export_df.to_csv(f'{svd_dir}/svd-transition-stability-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction/probing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LR helpers\n",
    "\"\"\"\n",
    "def run_probe(x_cp, y_cp):\n",
    "    \"\"\"\n",
    "    Fit an LR probe + get labels / predictions / accuracy\n",
    "    \"\"\"\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.2, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 500, fit_intercept = True)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    y_hat = lr_model.predict(x_test)\n",
    "\n",
    "    y_test_np = cupy.asnumpy(y_test)\n",
    "    y_hat_np = cupy.asnumpy(y_hat)\n",
    "\n",
    "    acc, acc_lo, acc_hi = get_acc_with_ci(y_test_np, y_hat_np)\n",
    "    nmi, nmi_lo, nmi_hi = get_nmi_with_ci(y_test_np, y_hat_np)\n",
    "\n",
    "    return acc, acc_lo, acc_hi, nmi, nmi_lo, nmi_hi\n",
    "\n",
    "# def run_probe_with_mi(x_cp, y_cp):\n",
    "#     \"\"\"\n",
    "#     Fit an LR probe; return normalized MI\n",
    "#     \"\"\"\n",
    "#     x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.2, random_state = 123)\n",
    "#     lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 100, fit_intercept = True)\n",
    "#     lr_model.fit(x_train, y_train)\n",
    "#     accuracy = lr_model.score(x_test, y_test)\n",
    "#     train_acc = lr_model.score(x_train, y_train)\n",
    "#     y_actual_np = cupy.asnumpy(y_test)\n",
    "#     y_pred_np = cupy.asnumpy(lr_model.predict(x_test))\n",
    "#     mi = sklearn.metrics.mutual_info_score(y_actual_np, y_pred_np) # nats\n",
    "#     max_entropy = sklearn.metrics.mutual_info_score(y_actual_np, y_actual_np) # H(y)\n",
    "#     return accuracy, mi, max_entropy, train_acc\n",
    "\n",
    "def get_acc_with_ci(y_true, y_pred, alpha = 0.01):\n",
    "    \"\"\"\n",
    "    Get accuracy with CI - standard Wilson CIs\n",
    "    \"\"\"\n",
    "    n = y_true.size\n",
    "    k = (y_true == y_pred).sum()\n",
    "    z = scipy.stats.norm.ppf(1 - alpha/2)\n",
    "    phat = k / n\n",
    "    denom = 1 + z*z/n\n",
    "    center = (phat + z*z/(2*n)) / denom\n",
    "    half = z * np.sqrt((phat*(1-phat) + z*z/(4*n))/n) / denom\n",
    "    return phat, center - half, center + half\n",
    "\n",
    "def get_nmi_with_ci(y_true, y_pred, alpha = 0.01, base = 2.0):\n",
    "    \"\"\"\n",
    "    Get normalized MI with CI - standard asymptotic CIs applying delta method to the entropy estimators\n",
    "    \"\"\"\n",
    "    y = np.asarray(y_true); yhat = np.asarray(y_pred)\n",
    "    n = y.size\n",
    "    uy, yi = np.unique(y, return_inverse = True)\n",
    "    uh, hi = np.unique(yhat, return_inverse = True)\n",
    "    C, H = uy.size, uh.size\n",
    "\n",
    "    N = np.zeros((C, H), float)\n",
    "    for i in range(n): N[yi[i], hi[i]] += 1.0\n",
    "    P = N / n\n",
    "    py = P.sum(1, keepdims = True) # Cx1\n",
    "    ph = P.sum(0, keepdims = True) # 1xH\n",
    "\n",
    "    # MI and H(Y) in nats\n",
    "    mask = (P>0) & (py>0) & (ph>0)\n",
    "    term = np.zeros_like(P)\n",
    "    term[mask] = np.log(P[mask]) - np.log(py.repeat(H,1)[mask]) - np.log(ph.repeat(C,0)[mask])\n",
    "\n",
    "    # Gradients wrt P (nats)\n",
    "    gI = np.zeros_like(P); gI[mask] = term[mask]\n",
    "    gH = -(np.log(py) + 1.0) # Cx1\n",
    "    gH = np.tile(gH, (1, H)) # CxH\n",
    "\n",
    "    # Multinomial delta (nats^2)\n",
    "    sI1 = (gI*gI*P).sum(); sI2 = (gI*P).sum()**2\n",
    "    sH1 = (gH*gH*P).sum(); sH2 = (gH*P).sum()**2\n",
    "    sC1 = (gI*gH*P).sum(); sC2 = (gI*P).sum()*(gH*P).sum()\n",
    "\n",
    "    # Convert to base (bits by default)\n",
    "    logb = np.log(base)\n",
    "    I_bits = (P * term).sum() / logb\n",
    "    Hy_bits =  -(py * np.log(py)).sum() / logb\n",
    "    varI = (sI1 - sI2)/(n * logb**2)\n",
    "    varH = (sH1 - sH2)/(n * logb**2)\n",
    "    covIH = (sC1 - sC2)/(n * logb**2)\n",
    "\n",
    "    R = I_bits/Hy_bits\n",
    "    dI = 1.0/Hy_bits\n",
    "    dH = -I_bits/(Hy_bits**2)\n",
    "    seR = np.sqrt(dI*dI*varI + dH*dH*varH + 2*dI*dH*covIH)\n",
    "    z = scipy.stats.norm.ppf(1 - alpha/2)\n",
    "    return float(R), float(R - z*seR), float(R + z*seR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic regression - predict expert ID\n",
    "\"\"\"\n",
    "current_layer_accuracy = []\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())):\n",
    "    \n",
    "    expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "\n",
    "    expert_ids_cp = cupy.asarray(expert_ids)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, expert_ids_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, expert_ids_cp)\n",
    "\n",
    "    current_layer_accuracy.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(current_layer_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Use h_para and h_orth to predict NEXT layer expert ids (note - this does not remove expert info, remove below)\n",
    "# \"\"\"\n",
    "# next_layer_accuracy = []\n",
    "# for test_layer in tqdm(list(h_para_by_layer.keys())[:-1]):\n",
    "    \n",
    "#     expert_ids =\\\n",
    "#         topk_df\\\n",
    "#         .pipe(lambda df: df[df['layer_ix'] == test_layer + 1])\\\n",
    "#         .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "#         ['expert'].tolist()\n",
    "\n",
    "#     expert_ids_cp = cupy.asarray(expert_ids)\n",
    "#     x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "#     x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "#     para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, expert_ids_cp)\n",
    "#     orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, expert_ids_cp)\n",
    "\n",
    "#     next_layer_accuracy.append({\n",
    "#         'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "#         'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "#         'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "#         'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "#         'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "#     })\n",
    "\n",
    "# pd.DataFrame(next_layer_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict NEXT layer expert ids.\n",
    "Remove expert centroids of CURRENT layer first to prevent vis to piggy-back on spurious correlations between layers.\n",
    "\"\"\"\n",
    "centroids_para = {}\n",
    "centroids_orth = {}\n",
    "\n",
    "# Get current-layer expert IDs for layer\n",
    "for layer_ix in list(h_para_by_layer.keys()):\n",
    "\n",
    "    cur_layer_expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == layer_ix])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "\n",
    "    cur_layer_expert_ids_cp = cupy.asarray(cur_layer_expert_ids)\n",
    "\n",
    "    # H_para/h_orth for layer\n",
    "    h_para_cp = cupy.asarray(h_para_by_layer[layer_ix].to(torch.float16).detach().cpu())\n",
    "    h_orth_cp = cupy.asarray(h_orth_by_layer[layer_ix].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Compute centroids per expert id\n",
    "    centroids_para[layer_ix] = {}\n",
    "    centroids_orth[layer_ix] = {}\n",
    "\n",
    "    for e in set(cur_layer_expert_ids):\n",
    "        idx_cp = cupy.where(cur_layer_expert_ids_cp == e)[0]\n",
    "        centroids_para[layer_ix][e] = h_para_cp[idx_cp].mean(axis = 0)\n",
    "        centroids_orth[layer_ix][e] = h_orth_cp[idx_cp].mean(axis = 0)\n",
    "\n",
    "next_layer_accuracy_cond = []\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[:-1]):\n",
    "\n",
    "    # Target = next-layer slot-1 expert IDs (same as before)\n",
    "    y_cp =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer + 1])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "    y_cp = cupy.asarray(y_cp)\n",
    "\n",
    "    # Current-layer top-1 expert IDs - needed for residual lookup\n",
    "    cur_exp_cp =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "    cur_exp_cp = cupy.asarray(cur_exp_cp)\n",
    "\n",
    "    # Pull h_para / h_orth tensors and convert to cupy\n",
    "    h_para_cp = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    h_orth_cp = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Subtract extract centroids\n",
    "    mu_para_mat = cupy.stack([centroids_para[test_layer][int(e)] for e in cur_exp_cp])\n",
    "    mu_orth_mat = cupy.stack([centroids_orth[test_layer][int(e)] for e in cur_exp_cp])\n",
    "    h_para_res = h_para_cp - mu_para_mat\n",
    "    h_orth_res = h_orth_cp - mu_orth_mat\n",
    "\n",
    "    # Run the unchanged probe\n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(h_para_res, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(h_orth_res, y_cp)\n",
    "\n",
    "    next_layer_accuracy_cond.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(next_layer_accuracy_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict PREV layer expert ids.\n",
    "\"\"\"\n",
    "prev_layer_accuracy_cond = []\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[1:]):\n",
    "\n",
    "    # Target = next-layer slot-1 expert IDs (same as before)\n",
    "    y_cp =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer - 1])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "    y_cp = cupy.asarray(y_cp)\n",
    "\n",
    "    # Current-layer top-1 expert IDs - needed for residual lookup\n",
    "    cur_exp_cp =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "    cur_exp_cp = cupy.asarray(cur_exp_cp)\n",
    "\n",
    "    # Pull h_para / h_orth tensors and convert to cupy\n",
    "    h_para_cp = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    h_orth_cp = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Subtract extract centroids\n",
    "    mu_para_mat = cupy.stack([centroids_para[test_layer][int(e)] for e in cur_exp_cp])\n",
    "    mu_orth_mat = cupy.stack([centroids_orth[test_layer][int(e)] for e in cur_exp_cp])\n",
    "    h_para_res = h_para_cp - mu_para_mat\n",
    "    h_orth_res = h_orth_cp - mu_orth_mat\n",
    "\n",
    "    # Run the unchanged probe\n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(h_para_res, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(h_orth_res, y_cp)\n",
    "\n",
    "    prev_layer_accuracy_cond.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(prev_layer_accuracy_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict path motif layer expert ids.\n",
    "\"\"\"\n",
    "path_motif_accuracy_cond = []\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[1:]):\n",
    "\n",
    "\n",
    "    prev_layer_expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer - 1])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "    \n",
    "    cur_layer_expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "\n",
    "    y_df =\\\n",
    "        pd.DataFrame({'cur_layer_expert_ids': cur_layer_expert_ids, 'prev_layer_expert_ids': prev_layer_expert_ids})\\\n",
    "        .assign(path = lambda df: df['prev_layer_expert_ids'].astype(str) + '->' + df['cur_layer_expert_ids'].astype(str))\n",
    "        \n",
    "    y_map = {path: i for i, path in enumerate(y_df['path'].unique())}\n",
    "    y_cp = cupy.asarray(y_df['path'].map(y_map))\n",
    "    \n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Probe\n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, y_cp)\n",
    "\n",
    "    path_motif_accuracy_cond.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "    print(pd.DataFrame(path_motif_accuracy_cond))\n",
    "\n",
    "display(pd.DataFrame(path_motif_accuracy_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict path motif layer expert ids.\n",
    "\"\"\"\n",
    "path_motif_accuracy_cond = []\n",
    "\n",
    "this_cur_layer_expert_id = 1\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[1:]):\n",
    "\n",
    "    valid_samples =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        .pipe(lambda df: df[df['expert'] == this_cur_layer_expert_id])\\\n",
    "        ['sample_ix'].tolist()\n",
    "\n",
    "    prev_layer_expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['sample_ix'].isin(valid_samples)])\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer - 1])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "        \n",
    "    y_cp = cupy.asarray(prev_layer_expert_ids)\n",
    "    \n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer][valid_samples, :].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer][valid_samples, :].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Probe\n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, y_cp)\n",
    "\n",
    "    path_motif_accuracy_cond.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(path_motif_accuracy_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_layer_expert_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export results\n",
    "\"\"\"\n",
    "layer_transitions_export_df = pd.concat([\n",
    "    pd.DataFrame(current_layer_accuracy).assign(target = 'current_layer'),\n",
    "    pd.DataFrame(next_layer_accuracy_cond).assign(target = 'next_layer')\n",
    "]).assign(model = model_prefix)\n",
    "\n",
    "display(layer_transitions_export_df)\n",
    "\n",
    "layer_transitions_export_df.to_csv(f'{svd_dir}/svd-probe-expert-id-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict language - presplit, seperate TIDs\n",
    "\"\"\"\n",
    "# def run_lr_with_mi_presplit(x_train, x_test, y_train, y_test):\n",
    "#     lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 1000, fit_intercept = True)\n",
    "#     lr_model.fit(x_train, y_train)\n",
    "#     accuracy = lr_model.score(x_test, y_test)\n",
    "#     train_acc = lr_model.score(x_train, y_train)\n",
    "#     y_actual_np = cupy.asnumpy(y_test)\n",
    "#     y_pred_np = cupy.asnumpy(lr_model.predict(x_test))\n",
    "#     mi = sklearn.metrics.mutual_info_score(y_actual_np, y_pred_np) # nats\n",
    "#     max_entropy = sklearn.metrics.mutual_info_score(y_actual_np, y_actual_np) # H(y)\n",
    "#     return accuracy, mi.item(), max_entropy.item(), train_acc\n",
    "\n",
    "# lang_probe_accs = []\n",
    "# # Split train/test, different TIDs in each\n",
    "# gss = sklearn.model_selection.GroupShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 123)\n",
    "# train_ix, test_ix = next(gss.split(sample_df, groups = sample_df['token_id']))\n",
    "\n",
    "# train_sample_df = sample_df.take(train_ix)\n",
    "# test_sample_df = sample_df.take(test_ix)\n",
    "\n",
    "# # Prep y values\n",
    "# source_mapping = {source: i for i, source in enumerate(sample_df['source'].unique())}\n",
    "\n",
    "# y_train = cupy.asarray(train_sample_df.assign(source = lambda df: df['source'].map(source_mapping))['source'].tolist())\n",
    "# y_test = cupy.asarray(test_sample_df.assign(source = lambda df: df['source'].map(source_mapping))['source'].tolist())\n",
    "\n",
    "# for test_layer in tqdm(list(h_para_by_layer.keys())[::2]):\n",
    "\n",
    "#     x_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "#     x_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    \n",
    "#     x_train_para = x_para[train_sample_df['sample_ix'].tolist(), :]\n",
    "#     x_test_para = x_para[test_sample_df['sample_ix'].tolist(), :]\n",
    "#     x_train_orth = x_orth[train_sample_df['sample_ix'].tolist(), :]\n",
    "#     x_test_orth = x_orth[test_sample_df['sample_ix'].tolist(), :]\n",
    "\n",
    "#     para_res = run_lr_with_mi_presplit(x_train_para, x_test_para, y_train, y_test)\n",
    "#     orth_res = run_lr_with_mi_presplit(x_train_orth, x_test_orth, y_train, y_test)\n",
    "\n",
    "#     lang_probe_accs.append({\n",
    "#         'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "#         'para_acc': para_res[0],\n",
    "#         'para_train_acc': para_res[3],\n",
    "#         'para_mi_bits': para_res[1]/np.log(2.0),\n",
    "#         'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "#         'para_mi_pct': para_res[1]/para_res[2],\n",
    "#         'orth_acc': orth_res[0],\n",
    "#         'orth_train_acc': orth_res[3],\n",
    "#         'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "#         'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "#         'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "#     })\n",
    "\n",
    "#     display(pd.DataFrame(lang_probe_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict Language\n",
    "\"\"\"\n",
    "lang_probe_accs = []\n",
    "\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())):\n",
    "\n",
    "    source_mapping = {source: i for i, source in enumerate(sample_df['source'].unique())}\n",
    "\n",
    "    # Probe en/es token predictiveness\n",
    "    y_df =\\\n",
    "        sample_df\\\n",
    "        .assign(source = lambda df: df['source'].map(source_mapping))\n",
    "        # .pipe(lambda df: df[df['source'].isin(['en', 'es'])])\\ # Move up about assign(source=...)\n",
    "        # .pipe(lambda df: df[df['token'].apply(lambda x: bool(regex.search(r'\\p{L}', x)))])\n",
    "\n",
    "    selected_indices = y_df['sample_ix'].tolist()\n",
    "\n",
    "    y_df = y_df['source'].tolist()\n",
    "\n",
    "    y_cp = cupy.asarray(y_df)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer][selected_indices, :].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer][selected_indices, :].to(torch.float16).detach().cpu())\n",
    "    \n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, y_cp)\n",
    "\n",
    "    lang_probe_accs.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(lang_probe_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Predict Language + DEMEAN\n",
    "# \"\"\"\n",
    "# lang_probe_accs = []\n",
    "\n",
    "# for test_layer in tqdm(list(h_para_by_layer.keys())[::2]):\n",
    "\n",
    "#     source_mapping = {source: i for i, source in enumerate(sample_df['source'].unique())}\n",
    "\n",
    "#     # Probe en/es token predictiveness\n",
    "#     y_df =\\\n",
    "#         sample_df\\\n",
    "#         .pipe(lambda df: df[df['source'].isin(['en', 'es'])])\\\n",
    "#         .assign(source = lambda df: df['source'].map(source_mapping))\n",
    "#         #\\ .pipe(lambda df: df[df['token'].apply(lambda x: bool(regex.search(r'\\p{L}', x)))])\n",
    "\n",
    "#     selected_indices = y_df['sample_ix'].tolist()\n",
    "#     y_df = y_df['source'].tolist()\n",
    "#     y_cp = cupy.asarray(y_df)\n",
    "\n",
    "#     ### Demean by current-layer top-1 expert\n",
    "#     cur_exp_cp = topk_df.pipe(lambda df: df[df['layer_ix'] == test_layer]).pipe(lambda df: df[df['topk_ix'] == 1])['expert'].to_numpy()\n",
    "#     cur_exp_cp = cupy.asarray(cur_exp_cp)[selected_indices]  # aligned!\n",
    "\n",
    "#     # Subtract centroids\n",
    "#     x_cp_para = cupy.asarray(h_para_by_layer[test_layer][selected_indices, :].to(torch.float16).detach().cpu()) # Pull base data\n",
    "#     x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer][selected_indices, :].to(torch.float16).detach().cpu())\n",
    "#     mu_para_sel = cupy.stack([centroids_para[test_layer][int(e)] for e in cur_exp_cp.get()]) # Get centroids by cur expert\n",
    "#     mu_orth_sel = cupy.stack([centroids_orth[test_layer][int(e)] for e in cur_exp_cp.get()])\n",
    "#     x_cp_para = x_cp_para - mu_para_sel # Get demenead residuals\n",
    "#     x_cp_orth = x_cp_orth - mu_orth_sel\n",
    "\n",
    "#     para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, y_cp)\n",
    "#     orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, y_cp) \n",
    "\n",
    "#     lang_probe_accs.append({\n",
    "#         'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "#         'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "#         'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "#         'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "#         'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "#     })\n",
    "\n",
    "# display(pd.DataFrame(lang_probe_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "display(sample_df.groupby('source', as_index = False).agg(z = ('sample_ix', 'count')))\n",
    "lang_export_df = pd.DataFrame(lang_probe_accs)\n",
    "display(lang_export_df)\n",
    "\n",
    "lang_export_df.to_csv(f'{svd_dir}/svd-probe-lang-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict TID\n",
    "\"\"\"\n",
    "tid_probe_accs = []\n",
    "\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())):\n",
    "\n",
    "    clear_all_cuda_memory(False)\n",
    "\n",
    "    top_tids =\\\n",
    "        sample_df\\\n",
    "        .pipe(lambda df: df[df['source'] == 'en'])\\\n",
    "        .groupby(['token_id', 'token'], as_index = False)\\\n",
    "        .agg(n = ('token', 'count')).sort_values(by = 'n', ascending = False)\\\n",
    "        .head(500)\n",
    "\n",
    "    valid_samples =\\\n",
    "        sample_df\\\n",
    "        .assign(token_id = lambda df: np.where(df['token_id'].isin(top_tids['token_id']), df['token_id'], 999999))\n",
    "        # .pipe(lambda df: df[df['token_id'].isin(top_tids['token_id'].tolist())])\n",
    "\n",
    "    y_df =\\\n",
    "        valid_samples\\\n",
    "        ['token_id']\\\n",
    "        .tolist()\n",
    "\n",
    "    y_cp = cupy.asarray(y_df)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer][valid_samples['sample_ix'].tolist(), :].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer][valid_samples['sample_ix'].tolist(), :].to(torch.float16).detach().cpu())\n",
    "    \n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, y_cp)\n",
    "\n",
    "    tid_probe_accs.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(tid_probe_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "tid_export_df = pd.DataFrame(tid_probe_accs)\n",
    "display(tid_export_df)\n",
    "\n",
    "tid_export_df.to_csv(f'{svd_dir}/svd-probe-tid-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict Position\n",
    "\"\"\"\n",
    "position_probe_accs = []\n",
    "\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())):\n",
    "\n",
    "    clear_all_cuda_memory(False)\n",
    "\n",
    "    valid_samples =\\\n",
    "        sample_df\\\n",
    "        .pipe(lambda df: df[df['token_ix'].isin(range(500))])\n",
    "\n",
    "    y_df = (valid_samples['token_ix'] // 100).tolist()\n",
    "\n",
    "    y_cp = cupy.asarray(y_df)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer][valid_samples['sample_ix'].tolist(), :].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer][valid_samples['sample_ix'].tolist(), :].to(torch.float16).detach().cpu())\n",
    "    \n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, y_cp)\n",
    "    \n",
    "    position_probe_accs.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(position_probe_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "position_export_df = pd.DataFrame(position_probe_accs)\n",
    "display(position_export_df)\n",
    "\n",
    "position_export_df.to_csv(f'{svd_dir}/svd-probe-pos-{model_prefix}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
