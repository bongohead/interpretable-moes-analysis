{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This contains code to use SVD to decompose hidden states based on whether they're used by routing or not.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy\n",
    "import cuml\n",
    "\n",
    "import importlib\n",
    "import gc\n",
    "import pickle\n",
    "import os\n",
    "import regex\n",
    "import scipy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.loader import load_model_and_tokenizer\n",
    "from utils.quantize import compare_bf16_fp16_batched\n",
    "from utils.svd import decompose_orthogonal, decompose_sideways, get_svd_proj\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 1234\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()\n",
    "\n",
    "ws = '/workspace/interpretable-moes-analysis'\n",
    "svd_dir = f'{ws}/experiments/geometry/svd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_prefix = 'gpt-oss-20b'\n",
    "tokenizer, model, model_architecture, model_n_moe_layers, model_n_dense_layers = load_model_and_tokenizer(model_prefix, device = main_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get 0-indexed MoE layer indices\n",
    "\"\"\"\n",
    "with open(f'{ws}/experiments/geometry/activations/{model_prefix}/metadata.pkl', 'rb') as f:\n",
    "    layer_indices = pickle.load(f).get('layer_mappings')\n",
    "\n",
    "layer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get projection matrices\n",
    "\"\"\"\n",
    "def get_router_objects(model, model_prefix, model_n_dense_layers, model_n_moe_layers):\n",
    "    v_mats = {}\n",
    "    v_mats_demeaned = {}\n",
    "    router_mats = {}\n",
    "\n",
    "    for layer_ix in tqdm(range(0, model_n_dense_layers + model_n_moe_layers)):\n",
    "        if layer_ix not in layer_indices:\n",
    "            continue\n",
    "        if model_prefix == 'kimivl': \n",
    "            router_mat = model.language_model.model.layers[layer_ix].mlp.gate.weight\n",
    "        elif model_prefix == 'granite':\n",
    "            router_mat = model.model.layers[layer_ix].block_sparse_moe.router.layer.weight\n",
    "        else:\n",
    "            router_mat = model.model.layers[layer_ix].mlp.router.weight\n",
    "\n",
    "        router_mat = router_mat.detach().cpu().to(torch.float32)\n",
    "        router_mat = router_mat - router_mat.mean(dim = 0, keepdim = True) # Center logits\n",
    "        _, _, V = get_svd_proj(router_mat)\n",
    "        _, _, V_demeaned = get_svd_proj(router_mat - router_mat.mean(dim = 0, keepdim = True))\n",
    "        \n",
    "        v_mats[layer_ix] = V\n",
    "        v_mats_demeaned[layer_ix] = V_demeaned\n",
    "        router_mats[layer_ix] = router_mat\n",
    "\n",
    "    return v_mats, v_mats_demeaned, router_mats\n",
    "\n",
    "v_mats, v_mats_demeaned, router_mats = get_router_objects(model, model_prefix, model_n_dense_layers, model_n_moe_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "def load_data(model_prefix, max_data_files):\n",
    "    \"\"\"\n",
    "    Load data saved by `export-activations.ipynb`\n",
    "    \"\"\"\n",
    "    folders = [f'{ws}/experiments/geometry/activations/{model_prefix}/{i:02d}' for i in range(max_data_files)]\n",
    "    folders = [f for f in folders if os.path.isdir(f)]\n",
    "\n",
    "    sample_parts = []\n",
    "    topk_parts = []\n",
    "    hs_parts = []\n",
    "\n",
    "    for f in tqdm(folders):\n",
    "        sample_parts.append(pd.read_feather(f'{f}/samples.feather'))\n",
    "        topk_parts.append(pd.read_feather(f'{f}/topks.feather'))\n",
    "        tensors = load_file(f'{f}/activations.safetensors', device = 'cpu')\n",
    "        hs_parts.append(tensors['all_pre_mlp_hs'])\n",
    "\n",
    "    sample_df = pd.concat(sample_parts, ignore_index = True)\n",
    "    topk_df = pd.concat(topk_parts, ignore_index = True)\n",
    "    pre_mlp_hs = torch.concat(hs_parts, dim = 0)\n",
    "\n",
    "    prompts_df = pd.read_feather(f'{ws}/experiments/geometry/activations/{model_prefix}/prompts.feather')\n",
    "    \n",
    "    gc.collect()\n",
    "    return prompts_df, sample_df, topk_df, pre_mlp_hs\n",
    "\n",
    "# 5 except glm4moe=3\n",
    "prompts_df, sample_df_import, topk_df_import, all_pre_mlp_hs_import = load_data(model_prefix, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Let's clean up the mappings here. We'll get everything to a sample_ix level first.\n",
    "\"\"\"\n",
    "sample_df =\\\n",
    "    sample_df_import\\\n",
    "    .assign(sample_ix = lambda df: range(0, len(df)))\\\n",
    "    .reset_index()\n",
    "\n",
    "topk_df =\\\n",
    "    topk_df_import\\\n",
    "    .merge(sample_df[['sample_ix', 'prompt_ix', 'token_ix']], how = 'inner', on = ['prompt_ix', 'token_ix'])\\\n",
    "    .drop(columns = ['token_ix'])\\\n",
    "    .assign(layer_ix = lambda df: df['layer_ix'] + model_n_dense_layers)\n",
    "\n",
    "def get_sample_df_for_layer(sample_df, topk_df, layer_ix):\n",
    "    \"\"\"\n",
    "    Helper to take the sample df and merge layer-level expert selection information\n",
    "    \"\"\"\n",
    "    topk_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix])\n",
    "    topk_l1_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 1])\n",
    "    topk_l2_layer_df = topk_df.pipe(lambda df: df[df['layer_ix'] == layer_ix - 2])\n",
    "\n",
    "    layer_df =\\\n",
    "        sample_df\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 1])[['sample_ix', 'expert']], how = 'inner', on = 'sample_ix')\\\n",
    "        .merge(topk_l1_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev_expert'})[['sample_ix', 'prev_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_l2_layer_df.pipe(lambda df: df[df['topk_ix'] == 1]).rename(columns = {'expert': 'prev2_expert'})[['sample_ix', 'prev2_expert']], how = 'left', on = 'sample_ix')\\\n",
    "        .merge(topk_layer_df.pipe(lambda df: df[df['topk_ix'] == 2]).rename(columns = {'expert': 'expert2'})[['sample_ix', 'expert2']], how = 'left', on = 'sample_ix')\\\n",
    "        .assign(leading_path = lambda df: df['prev2_expert'] + '-' + df['prev_expert'])\n",
    "    \n",
    "    return layer_df\n",
    "\n",
    "del sample_df_import, topk_df_import\n",
    "\n",
    "gc.collect()\n",
    "display(topk_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert activations to fp16 (for compatibility with cupy later) + dict\n",
    "\"\"\"\n",
    "all_pre_mlp_hs = all_pre_mlp_hs_import.to(torch.float16)\n",
    "# compare_bf16_fp16_batched(all_pre_mlp_hs_import, all_pre_mlp_hs)\n",
    "del all_pre_mlp_hs_import\n",
    "all_pre_mlp_hs = {(layer_ix + model_n_dense_layers): all_pre_mlp_hs[:, save_ix, :] for save_ix, layer_ix in enumerate(layer_indices)}\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How much the routing matrix itself rotates\n",
    "\"\"\"\n",
    "def get_proj_overlap(layer_ix):\n",
    "    M = (v_mats[layer_ix].T  @ v_mats[layer_ix + 1])\n",
    "    _, S, _ = get_svd_proj(M, svd_tol = 0)\n",
    "    S_quantiles = np.quantile(S, [0, .25, .5, .75, 1])\n",
    "    S = [round(x, 2) for x in S_quantiles.tolist()]\n",
    "    overlap = (M * M).sum() / min(v_mats[layer_ix].shape[1], v_mats[layer_ix + 1].shape[1])\n",
    "    rd_ratio = v_mats[layer_ix].shape[1] / model.config.hidden_size\n",
    "    return {'overlap': round(overlap.item(), 2), 'rank': v_mats[layer_ix].shape[1], 'rd_ratio': round(rd_ratio, 2), 'sv_quantiles': S}\n",
    "\n",
    "for layer_ix in list(all_pre_mlp_hs.keys())[:-1]:\n",
    "    proj_overlap = get_proj_overlap(layer_ix)\n",
    "    if layer_ix % 2 == 0:\n",
    "        print(f'Layer {layer_ix}: {proj_overlap}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Blind/vis helper\n",
    "\"\"\"\n",
    "def get_vis_blind(hl, vmat, type = 'f32'):\n",
    "    if type == 'f32':\n",
    "        hlvis = (hl.float() @ vmat) @ vmat.T\n",
    "    elif type == 'f16':\n",
    "        hlvis = (hl @ vmat.to(torch.float16)) @ vmat.to(torch.float16).T\n",
    "    else:\n",
    "        raise Exception(\"Type must be 'bf16' or 'f32'\")\n",
    "\n",
    "    hlblind = hl - hlvis\n",
    "    return hlvis, hlblind\n",
    "\n",
    "get_vis_blind(all_pre_mlp_hs[10], v_mats[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "E1: Get standard cosine sims cos(h^{vis}_{l}, h^{vis}_{l+1}), cos(h^{blind}_{l}, h^{blind}_{l+1})\n",
    "\"\"\"\n",
    "def get_cos_sim(all_pre_mlp_hs, v_mats, layer_ix, k_ahead = 1, samples = -1):\n",
    "    g = torch.Generator().manual_seed(123)    \n",
    "    if samples == -1:\n",
    "        sample_ixs = list(range(0, all_pre_mlp_hs[layer_ix].shape[0]))\n",
    "    else:\n",
    "        sample_ixs = torch.randint(low = 0, high = all_pre_mlp_hs[layer_ix].shape[0], size = (samples,), generator = g)\n",
    "\n",
    "    h_vis_l, h_blind_l = get_vis_blind(all_pre_mlp_hs[layer_ix][sample_ixs, :], v_mats[layer_ix])\n",
    "    h_vis_l1, h_blind_l1 = get_vis_blind(all_pre_mlp_hs[layer_ix + k_ahead][sample_ixs, :], v_mats[layer_ix + k_ahead])\n",
    "\n",
    "    full_cos_sim = F.cosine_similarity(all_pre_mlp_hs[layer_ix][sample_ixs, :], all_pre_mlp_hs[layer_ix + k_ahead][sample_ixs, :], dim = -1).mean().item()\n",
    "    para_cos_sim = F.cosine_similarity(h_vis_l, h_vis_l1, dim = -1).mean().item()\n",
    "    orth_cos_sim = F.cosine_similarity(h_blind_l, h_blind_l1, dim = -1).mean().item()\n",
    "    \n",
    "    perm = torch.randperm(h_vis_l.shape[0], generator = g)\n",
    "    full_shuf_cos_sim = F.cosine_similarity(all_pre_mlp_hs[layer_ix][sample_ixs, :], all_pre_mlp_hs[layer_ix + k_ahead][sample_ixs, :][perm], dim=-1).mean().item()\n",
    "    para_shuf_cos_sim = F.cosine_similarity(h_vis_l, h_vis_l1[perm], dim=-1).mean().item()\n",
    "    orth_shuf_cos_sim = F.cosine_similarity(h_blind_l, h_blind_l1[perm], dim=-1).mean().item()\n",
    "\n",
    "    dm = lambda x: x - x.mean(0)\n",
    "    para_dm = F.cosine_similarity(dm(h_vis_l), dm(h_vis_l1), dim = -1).mean().item()\n",
    "    orth_dm = F.cosine_similarity(dm(h_blind_l), dm(h_blind_l1), dim = -1).mean().item()\n",
    "    para_dm_shuf = F.cosine_similarity(dm(h_vis_l), dm(h_vis_l1)[perm], dim = -1).mean().item()\n",
    "    orth_dm_shuf = F.cosine_similarity(dm(h_blind_l), dm(h_blind_l1)[perm], dim = -1).mean().item()\n",
    "\n",
    "    return {\n",
    "        'h': round(full_cos_sim, 2), \n",
    "        'hvis': round(para_cos_sim, 2),\n",
    "        'hblind': round(orth_cos_sim, 2),\n",
    "        'hvis_dm': round(para_dm, 2),\n",
    "        'hblind_dm': round(orth_dm, 2),\n",
    "        'h_shuf': round(full_shuf_cos_sim, 2),\n",
    "        'hvis_shuf': round(para_shuf_cos_sim, 2),\n",
    "        'hblind_shuf': round(orth_shuf_cos_sim, 2),\n",
    "        'hvis_dm_shuf': round(para_dm_shuf, 2),\n",
    "        'hblind_dm_shuf': round(orth_dm_shuf, 2),\n",
    "    }\n",
    "\n",
    "k_ahead = 1\n",
    "for layer_ix in list(all_pre_mlp_hs.keys())[:-k_ahead]:\n",
    "    if layer_ix % 2 == 0:\n",
    "        cos_sims = get_cos_sim(all_pre_mlp_hs, v_mats, layer_ix, k_ahead = k_ahead, samples = 10_000)\n",
    "        print(f'Layer {layer_ix}: {cos_sims}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "E2: Get projection-controlled cosine sims cos(h_l P_l1, h_l1 P_l1), cos(h_l (I-P_l1), h_l1 (I-P_l1))\n",
    "\"\"\"\n",
    "def get_cos_sim(all_pre_mlp_hs, v_mats, layer_ix, k_ahead = 1, samples = -1):\n",
    "\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    \n",
    "    if samples == -1:\n",
    "        sample_ixs = list(range(0, all_pre_mlp_hs[layer_ix].shape[0]))\n",
    "    else:\n",
    "        sample_ixs = torch.randint(low = 0, high = all_pre_mlp_hs[layer_ix].shape[0], size = (samples,), generator = g)\n",
    "\n",
    "    ctrl_l, rest_l = get_vis_blind(all_pre_mlp_hs[layer_ix][sample_ixs, :], v_mats[layer_ix + k_ahead])\n",
    "    ctrl_l1, rest_l1 = get_vis_blind(all_pre_mlp_hs[layer_ix + k_ahead][sample_ixs, :], v_mats[layer_ix + k_ahead])\n",
    "\n",
    "    ctrl_cos_sim = F.cosine_similarity(ctrl_l, ctrl_l1, dim = -1).mean().item()\n",
    "    rest_cos_sim = F.cosine_similarity(rest_l, rest_l1, dim = -1).mean().item()\n",
    "\n",
    "    perm = torch.randperm(ctrl_l.shape[0], generator = g)\n",
    "    ctrl_shuf = F.cosine_similarity(ctrl_l, ctrl_l1[perm], dim = -1).mean().item()\n",
    "    rest_shuf = F.cosine_similarity(rest_l, rest_l1[perm], dim = -1).mean().item()\n",
    "\n",
    "    dm = lambda x: x - x.mean(0)\n",
    "    ctrl_dm = F.cosine_similarity(dm(ctrl_l), dm(ctrl_l1), dim = -1).mean().item()\n",
    "    rest_dm = F.cosine_similarity(dm(rest_l), dm(rest_l1), dim = -1).mean().item()\n",
    "    ctrl_dm_shuf = F.cosine_similarity(dm(ctrl_l), dm(ctrl_l1)[perm], dim = -1).mean().item()\n",
    "    rest_dm_shuf = F.cosine_similarity(dm(rest_l), dm(rest_l1)[perm], dim = -1).mean().item()\n",
    "\n",
    "    # No base and base_shuf since same as E1\n",
    "    return {\n",
    "        'ctrl': round(ctrl_cos_sim, 2),\n",
    "        'rest': round(rest_cos_sim, 2),\n",
    "        'ctrl_dm': round(ctrl_dm, 2),\n",
    "        'rest_dm': round(rest_dm, 2),\n",
    "        'ctrl_shuf': round(ctrl_shuf, 2),\n",
    "        'rest_shuf': round(rest_shuf, 2),\n",
    "        'ctrl_dm_shuf': round(ctrl_dm_shuf, 2),\n",
    "        'rest_dm_shuf': round(rest_dm_shuf, 2),\n",
    "    }\n",
    "\n",
    "k_ahead = 1\n",
    "for layer_ix in list(all_pre_mlp_hs.keys())[:-k_ahead]:\n",
    "    if layer_ix % 2 == 0:\n",
    "        cos_sims = get_cos_sim(all_pre_mlp_hs, v_mats, layer_ix, k_ahead = k_ahead, samples = 10_000)\n",
    "        print(f'Layer {layer_ix}: {cos_sims}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "E3: Bootstrap quantiles + matched-token cosine similarity (E1 but bootstrapped)\n",
    "\"\"\"\n",
    "def precompute_token_cos_sims(all_pre_mlp_hs, v_mats, k_ahead = 1):\n",
    "    \"\"\"\n",
    "    Precompute per-token cosine similarities for all three channels across all (l, l+1) pairs.\n",
    "    Returns a dict keyed by layer index, each holding (N, ) tensors for 'full', 'vis', 'blind'.\n",
    "    \"\"\"\n",
    "    layer_indices = sorted(all_pre_mlp_hs.keys())\n",
    "    results = {}\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    for layer_ix in tqdm(layer_indices):\n",
    "        next_layer = layer_ix + k_ahead\n",
    "        if next_layer not in all_pre_mlp_hs:\n",
    "            continue\n",
    "        hl = all_pre_mlp_hs[layer_ix]\n",
    "        hl1 = all_pre_mlp_hs[next_layer]\n",
    "        h_vis_l, h_blind_l = get_vis_blind(hl, v_mats[layer_ix], type = 'f16')\n",
    "        h_vis_l1, h_blind_l1 = get_vis_blind(hl1, v_mats[next_layer], type = 'f16')\n",
    "        # Shuffled indices for random pairing\n",
    "        n = hl.shape[0]\n",
    "        shuf = torch.randperm(n, generator = g)\n",
    "        results[layer_ix] = {\n",
    "            # s'full': F.cosine_similarity(hl, hl1, dim = -1),\n",
    "            'vis': F.cosine_similarity(h_vis_l, h_vis_l1, dim = -1),\n",
    "            'blind': F.cosine_similarity(h_blind_l, h_blind_l1, dim = -1),\n",
    "            'vis_rand':  F.cosine_similarity(h_vis_l, h_vis_l1[shuf], dim = -1),\n",
    "            'blind_rand': F.cosine_similarity(h_blind_l, h_blind_l1[shuf], dim = -1),\n",
    "        }\n",
    "    return results\n",
    "\n",
    "def get_bootstrap_cos_sims(cos_sims_by_layer, doc_ids, bs_samples = 200, samples_per_bs = 1_000):\n",
    "    \"\"\"\n",
    "    Block bootstrap over precomputed per-token cosine similarities.\n",
    "    Params:\n",
    "        @cos_sims_by_layer: output of precompute_token_cos_sims\n",
    "        @doc_ids: list[int] of prompt/document ID per token (same order as sample_df)\n",
    "        @bs_samples: # of bootstrap iterations\n",
    "        @samples_per_bs: target tokens per bootstrap draw (sample prompts until we reach this threshold)\n",
    "    Returns:\n",
    "        layer_indices: sorted list of layer indices (one per pair)\n",
    "        output: dict with keys 'full', 'vis', 'blind', each a tuple (means, cis_lo, cis_hi) as np arrays of shape (n_pairs,)\n",
    "    \"\"\"\n",
    "    g = torch.Generator().manual_seed(123)\n",
    "    layer_indices = sorted(cos_sims_by_layer.keys())\n",
    "    n_pairs = len(layer_indices)\n",
    "    channels = ['vis', 'blind', 'vis_rand', 'blind_rand']\n",
    "    docs = torch.as_tensor(doc_ids)\n",
    "    uniq = docs.unique()\n",
    "    idxs_by_doc = [(docs == d).nonzero(as_tuple = True)[0] for d in uniq]\n",
    "    results = {ch: torch.empty((bs_samples, n_pairs), dtype = torch.float) for ch in channels}\n",
    "    for b in tqdm(range(bs_samples)):\n",
    "        take_idxs = []\n",
    "        total = 0\n",
    "        while total < samples_per_bs:\n",
    "            j = int(torch.randint(low = 0, high = len(idxs_by_doc), size=(1,), generator = g))\n",
    "            take_idxs.append(idxs_by_doc[j])\n",
    "            total += idxs_by_doc[j].numel()\n",
    "        sample_indices = torch.cat(take_idxs)[:samples_per_bs]\n",
    "        for ch in channels:\n",
    "            vals = torch.stack([cos_sims_by_layer[l][ch][sample_indices] for l in layer_indices], dim = 1)\n",
    "            results[ch][b] = vals.mean(dim=0)\n",
    "    output = []\n",
    "    for i, layer_ix in enumerate(layer_indices):\n",
    "        row = {'layer': layer_ix}\n",
    "        for ch in channels:\n",
    "            col = results[ch][:, i]\n",
    "            row[f'{ch}_mean'] = round(col.mean().item(), 3)\n",
    "            lo, hi = torch.quantile(col, torch.tensor([0.025, 0.975], dtype=col.dtype))\n",
    "            row[f'{ch}_lo'] = round(lo.item(), 3)\n",
    "            row[f'{ch}_hi'] = round(hi.item(), 3)\n",
    "        output.append(row)\n",
    "    return output\n",
    "\n",
    "cos_sims_by_layer = precompute_token_cos_sims(all_pre_mlp_hs, v_mats, k_ahead = 1)\n",
    "\n",
    "bs_results = get_bootstrap_cos_sims(\n",
    "    cos_sims_by_layer,\n",
    "    doc_ids = sample_df['prompt_ix'].tolist(),\n",
    "    bs_samples = 200,\n",
    "    samples_per_bs = 100,\n",
    ")\n",
    "\n",
    "for row in bs_results:\n",
    "    print(\n",
    "        f\"Layer {row['layer']:>3d}: \"\n",
    "        f\"vis={row['vis_mean']:.2f} [{row['vis_lo']:.2f}, {row['vis_hi']:.2f}] (rand={row['vis_rand_mean']:.2f}) | \"\n",
    "        f\"blind={row['blind_mean']:.2f} [{row['blind_lo']:.2f}, {row['blind_hi']:.2f}] (rand={row['blind_rand_mean']:.2f})\"\n",
    "        # f\"full={row['full_mean']:.3f} [{row['full_lo']:.3f}, {row['full_hi']:.3f}] (rand={row['full_rand_mean']:.3f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "export_df = pd.DataFrame({\n",
    "    'layer_ix_1': list(range(model_n_dense_layers + 1, len(all_pre_mlp_hs) + model_n_dense_layers)), # +1 to 1 index\n",
    "    'para_mean_across_layers': para_means,\n",
    "    'orth_mean_across_layers': orth_means,\n",
    "    'para_cis_hi': para_cis_hi,\n",
    "    'para_cis_lo': para_cis_lo,\n",
    "    'orth_cis_hi': orth_cis_hi,\n",
    "    'orth_cis_lo': orth_cis_lo\n",
    "})\n",
    "\n",
    "display(export_df)\n",
    "\n",
    "export_df.to_csv(f'{svd_dir}/svd-transition-stability-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction/probing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LR helpers\n",
    "\"\"\"\n",
    "def run_probe(x_cp, y_cp):\n",
    "    \"\"\"\n",
    "    Fit an LR probe + get labels / predictions / accuracy\n",
    "    \"\"\"\n",
    "    x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.2, random_state = 123)\n",
    "    lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 500, fit_intercept = True)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    y_hat = lr_model.predict(x_test)\n",
    "\n",
    "    y_test_np = cupy.asnumpy(y_test)\n",
    "    y_hat_np = cupy.asnumpy(y_hat)\n",
    "\n",
    "    acc, acc_lo, acc_hi = get_acc_with_ci(y_test_np, y_hat_np)\n",
    "    nmi, nmi_lo, nmi_hi = get_nmi_with_ci(y_test_np, y_hat_np)\n",
    "\n",
    "    return acc, acc_lo, acc_hi, nmi, nmi_lo, nmi_hi\n",
    "\n",
    "# def run_probe_with_mi(x_cp, y_cp):\n",
    "#     \"\"\"\n",
    "#     Fit an LR probe; return normalized MI\n",
    "#     \"\"\"\n",
    "#     x_train, x_test, y_train, y_test = cuml.train_test_split(x_cp, y_cp, test_size = 0.2, random_state = 123)\n",
    "#     lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 100, fit_intercept = True)\n",
    "#     lr_model.fit(x_train, y_train)\n",
    "#     accuracy = lr_model.score(x_test, y_test)\n",
    "#     train_acc = lr_model.score(x_train, y_train)\n",
    "#     y_actual_np = cupy.asnumpy(y_test)\n",
    "#     y_pred_np = cupy.asnumpy(lr_model.predict(x_test))\n",
    "#     mi = sklearn.metrics.mutual_info_score(y_actual_np, y_pred_np) # nats\n",
    "#     max_entropy = sklearn.metrics.mutual_info_score(y_actual_np, y_actual_np) # H(y)\n",
    "#     return accuracy, mi, max_entropy, train_acc\n",
    "\n",
    "def get_acc_with_ci(y_true, y_pred, alpha = 0.01):\n",
    "    \"\"\"\n",
    "    Get accuracy with CI - standard Wilson CIs\n",
    "    \"\"\"\n",
    "    n = y_true.size\n",
    "    k = (y_true == y_pred).sum()\n",
    "    z = scipy.stats.norm.ppf(1 - alpha/2)\n",
    "    phat = k / n\n",
    "    denom = 1 + z*z/n\n",
    "    center = (phat + z*z/(2*n)) / denom\n",
    "    half = z * np.sqrt((phat*(1-phat) + z*z/(4*n))/n) / denom\n",
    "    return phat, center - half, center + half\n",
    "\n",
    "def get_nmi_with_ci(y_true, y_pred, alpha = 0.01, base = 2.0):\n",
    "    \"\"\"\n",
    "    Get normalized MI with CI - standard asymptotic CIs applying delta method to the entropy estimators\n",
    "    \"\"\"\n",
    "    y = np.asarray(y_true); yhat = np.asarray(y_pred)\n",
    "    n = y.size\n",
    "    uy, yi = np.unique(y, return_inverse = True)\n",
    "    uh, hi = np.unique(yhat, return_inverse = True)\n",
    "    C, H = uy.size, uh.size\n",
    "\n",
    "    N = np.zeros((C, H), float)\n",
    "    for i in range(n): N[yi[i], hi[i]] += 1.0\n",
    "    P = N / n\n",
    "    py = P.sum(1, keepdims = True) # Cx1\n",
    "    ph = P.sum(0, keepdims = True) # 1xH\n",
    "\n",
    "    # MI and H(Y) in nats\n",
    "    mask = (P>0) & (py>0) & (ph>0)\n",
    "    term = np.zeros_like(P)\n",
    "    term[mask] = np.log(P[mask]) - np.log(py.repeat(H,1)[mask]) - np.log(ph.repeat(C,0)[mask])\n",
    "\n",
    "    # Gradients wrt P (nats)\n",
    "    gI = np.zeros_like(P); gI[mask] = term[mask]\n",
    "    gH = -(np.log(py) + 1.0) # Cx1\n",
    "    gH = np.tile(gH, (1, H)) # CxH\n",
    "\n",
    "    # Multinomial delta (nats^2)\n",
    "    sI1 = (gI*gI*P).sum(); sI2 = (gI*P).sum()**2\n",
    "    sH1 = (gH*gH*P).sum(); sH2 = (gH*P).sum()**2\n",
    "    sC1 = (gI*gH*P).sum(); sC2 = (gI*P).sum()*(gH*P).sum()\n",
    "\n",
    "    # Convert to base (bits by default)\n",
    "    logb = np.log(base)\n",
    "    I_bits = (P * term).sum() / logb\n",
    "    Hy_bits =  -(py * np.log(py)).sum() / logb\n",
    "    varI = (sI1 - sI2)/(n * logb**2)\n",
    "    varH = (sH1 - sH2)/(n * logb**2)\n",
    "    covIH = (sC1 - sC2)/(n * logb**2)\n",
    "\n",
    "    R = I_bits/Hy_bits\n",
    "    dI = 1.0/Hy_bits\n",
    "    dH = -I_bits/(Hy_bits**2)\n",
    "    seR = np.sqrt(dI*dI*varI + dH*dH*varH + 2*dI*dH*covIH)\n",
    "    z = scipy.stats.norm.ppf(1 - alpha/2)\n",
    "    return float(R), float(R - z*seR), float(R + z*seR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic regression - predict expert ID\n",
    "\"\"\"\n",
    "current_layer_accuracy = []\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())):\n",
    "    \n",
    "    expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "\n",
    "    expert_ids_cp = cupy.asarray(expert_ids)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, expert_ids_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, expert_ids_cp)\n",
    "\n",
    "    current_layer_accuracy.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(current_layer_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Use h_para and h_orth to predict NEXT layer expert ids (note - this does not remove expert info, remove below)\n",
    "# \"\"\"\n",
    "# next_layer_accuracy = []\n",
    "# for test_layer in tqdm(list(h_para_by_layer.keys())[:-1]):\n",
    "    \n",
    "#     expert_ids =\\\n",
    "#         topk_df\\\n",
    "#         .pipe(lambda df: df[df['layer_ix'] == test_layer + 1])\\\n",
    "#         .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "#         ['expert'].tolist()\n",
    "\n",
    "#     expert_ids_cp = cupy.asarray(expert_ids)\n",
    "#     x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "#     x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "#     para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, expert_ids_cp)\n",
    "#     orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, expert_ids_cp)\n",
    "\n",
    "#     next_layer_accuracy.append({\n",
    "#         'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "#         'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "#         'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "#         'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "#         'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "#     })\n",
    "\n",
    "# pd.DataFrame(next_layer_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict NEXT layer expert ids.\n",
    "Remove expert centroids of CURRENT layer first to prevent vis to piggy-back on spurious correlations between layers.\n",
    "\"\"\"\n",
    "centroids_para = {}\n",
    "centroids_orth = {}\n",
    "\n",
    "# Get current-layer expert IDs for layer\n",
    "for layer_ix in list(h_para_by_layer.keys()):\n",
    "\n",
    "    cur_layer_expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == layer_ix])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "\n",
    "    cur_layer_expert_ids_cp = cupy.asarray(cur_layer_expert_ids)\n",
    "\n",
    "    # H_para/h_orth for layer\n",
    "    h_para_cp = cupy.asarray(h_para_by_layer[layer_ix].to(torch.float16).detach().cpu())\n",
    "    h_orth_cp = cupy.asarray(h_orth_by_layer[layer_ix].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Compute centroids per expert id\n",
    "    centroids_para[layer_ix] = {}\n",
    "    centroids_orth[layer_ix] = {}\n",
    "\n",
    "    for e in set(cur_layer_expert_ids):\n",
    "        idx_cp = cupy.where(cur_layer_expert_ids_cp == e)[0]\n",
    "        centroids_para[layer_ix][e] = h_para_cp[idx_cp].mean(axis = 0)\n",
    "        centroids_orth[layer_ix][e] = h_orth_cp[idx_cp].mean(axis = 0)\n",
    "\n",
    "next_layer_accuracy_cond = []\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[:-1]):\n",
    "\n",
    "    # Target = next-layer slot-1 expert IDs (same as before)\n",
    "    y_cp =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer + 1])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "    y_cp = cupy.asarray(y_cp)\n",
    "\n",
    "    # Current-layer top-1 expert IDs - needed for residual lookup\n",
    "    cur_exp_cp =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "    cur_exp_cp = cupy.asarray(cur_exp_cp)\n",
    "\n",
    "    # Pull h_para / h_orth tensors and convert to cupy\n",
    "    h_para_cp = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    h_orth_cp = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Subtract extract centroids\n",
    "    mu_para_mat = cupy.stack([centroids_para[test_layer][int(e)] for e in cur_exp_cp])\n",
    "    mu_orth_mat = cupy.stack([centroids_orth[test_layer][int(e)] for e in cur_exp_cp])\n",
    "    h_para_res = h_para_cp - mu_para_mat\n",
    "    h_orth_res = h_orth_cp - mu_orth_mat\n",
    "\n",
    "    # Run the unchanged probe\n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(h_para_res, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(h_orth_res, y_cp)\n",
    "\n",
    "    next_layer_accuracy_cond.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(next_layer_accuracy_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict PREV layer expert ids.\n",
    "\"\"\"\n",
    "prev_layer_accuracy_cond = []\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[1:]):\n",
    "\n",
    "    # Target = next-layer slot-1 expert IDs (same as before)\n",
    "    y_cp =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer - 1])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "    y_cp = cupy.asarray(y_cp)\n",
    "\n",
    "    # Current-layer top-1 expert IDs - needed for residual lookup\n",
    "    cur_exp_cp =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "    cur_exp_cp = cupy.asarray(cur_exp_cp)\n",
    "\n",
    "    # Pull h_para / h_orth tensors and convert to cupy\n",
    "    h_para_cp = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    h_orth_cp = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Subtract extract centroids\n",
    "    mu_para_mat = cupy.stack([centroids_para[test_layer][int(e)] for e in cur_exp_cp])\n",
    "    mu_orth_mat = cupy.stack([centroids_orth[test_layer][int(e)] for e in cur_exp_cp])\n",
    "    h_para_res = h_para_cp - mu_para_mat\n",
    "    h_orth_res = h_orth_cp - mu_orth_mat\n",
    "\n",
    "    # Run the unchanged probe\n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(h_para_res, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(h_orth_res, y_cp)\n",
    "\n",
    "    prev_layer_accuracy_cond.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(prev_layer_accuracy_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict path motif layer expert ids.\n",
    "\"\"\"\n",
    "path_motif_accuracy_cond = []\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[1:]):\n",
    "\n",
    "\n",
    "    prev_layer_expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer - 1])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "    \n",
    "    cur_layer_expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].tolist()\n",
    "\n",
    "    y_df =\\\n",
    "        pd.DataFrame({'cur_layer_expert_ids': cur_layer_expert_ids, 'prev_layer_expert_ids': prev_layer_expert_ids})\\\n",
    "        .assign(path = lambda df: df['prev_layer_expert_ids'].astype(str) + '->' + df['cur_layer_expert_ids'].astype(str))\n",
    "        \n",
    "    y_map = {path: i for i, path in enumerate(y_df['path'].unique())}\n",
    "    y_cp = cupy.asarray(y_df['path'].map(y_map))\n",
    "    \n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Probe\n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, y_cp)\n",
    "\n",
    "    path_motif_accuracy_cond.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "    print(pd.DataFrame(path_motif_accuracy_cond))\n",
    "\n",
    "display(pd.DataFrame(path_motif_accuracy_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use h_para and h_orth to predict path motif layer expert ids.\n",
    "\"\"\"\n",
    "path_motif_accuracy_cond = []\n",
    "\n",
    "this_cur_layer_expert_id = 1\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())[1:]):\n",
    "\n",
    "    valid_samples =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        .pipe(lambda df: df[df['expert'] == this_cur_layer_expert_id])\\\n",
    "        ['sample_ix'].tolist()\n",
    "\n",
    "    prev_layer_expert_ids =\\\n",
    "        topk_df\\\n",
    "        .pipe(lambda df: df[df['sample_ix'].isin(valid_samples)])\\\n",
    "        .pipe(lambda df: df[df['layer_ix'] == test_layer - 1])\\\n",
    "        .pipe(lambda df: df[df['topk_ix'] == 1])\\\n",
    "        ['expert'].to_numpy()\n",
    "        \n",
    "    y_cp = cupy.asarray(prev_layer_expert_ids)\n",
    "    \n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer][valid_samples, :].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer][valid_samples, :].to(torch.float16).detach().cpu())\n",
    "\n",
    "    # Probe\n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, y_cp)\n",
    "\n",
    "    path_motif_accuracy_cond.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(path_motif_accuracy_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_layer_expert_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export results\n",
    "\"\"\"\n",
    "layer_transitions_export_df = pd.concat([\n",
    "    pd.DataFrame(current_layer_accuracy).assign(target = 'current_layer'),\n",
    "    pd.DataFrame(next_layer_accuracy_cond).assign(target = 'next_layer')\n",
    "]).assign(model = model_prefix)\n",
    "\n",
    "display(layer_transitions_export_df)\n",
    "\n",
    "layer_transitions_export_df.to_csv(f'{svd_dir}/svd-probe-expert-id-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict language - presplit, seperate TIDs\n",
    "\"\"\"\n",
    "# def run_lr_with_mi_presplit(x_train, x_test, y_train, y_test):\n",
    "#     lr_model = cuml.linear_model.LogisticRegression(penalty = 'l2', max_iter = 1000, fit_intercept = True)\n",
    "#     lr_model.fit(x_train, y_train)\n",
    "#     accuracy = lr_model.score(x_test, y_test)\n",
    "#     train_acc = lr_model.score(x_train, y_train)\n",
    "#     y_actual_np = cupy.asnumpy(y_test)\n",
    "#     y_pred_np = cupy.asnumpy(lr_model.predict(x_test))\n",
    "#     mi = sklearn.metrics.mutual_info_score(y_actual_np, y_pred_np) # nats\n",
    "#     max_entropy = sklearn.metrics.mutual_info_score(y_actual_np, y_actual_np) # H(y)\n",
    "#     return accuracy, mi.item(), max_entropy.item(), train_acc\n",
    "\n",
    "# lang_probe_accs = []\n",
    "# # Split train/test, different TIDs in each\n",
    "# gss = sklearn.model_selection.GroupShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 123)\n",
    "# train_ix, test_ix = next(gss.split(sample_df, groups = sample_df['token_id']))\n",
    "\n",
    "# train_sample_df = sample_df.take(train_ix)\n",
    "# test_sample_df = sample_df.take(test_ix)\n",
    "\n",
    "# # Prep y values\n",
    "# source_mapping = {source: i for i, source in enumerate(sample_df['source'].unique())}\n",
    "\n",
    "# y_train = cupy.asarray(train_sample_df.assign(source = lambda df: df['source'].map(source_mapping))['source'].tolist())\n",
    "# y_test = cupy.asarray(test_sample_df.assign(source = lambda df: df['source'].map(source_mapping))['source'].tolist())\n",
    "\n",
    "# for test_layer in tqdm(list(h_para_by_layer.keys())[::2]):\n",
    "\n",
    "#     x_para = cupy.asarray(h_para_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "#     x_orth = cupy.asarray(h_orth_by_layer[test_layer].to(torch.float16).detach().cpu())\n",
    "    \n",
    "#     x_train_para = x_para[train_sample_df['sample_ix'].tolist(), :]\n",
    "#     x_test_para = x_para[test_sample_df['sample_ix'].tolist(), :]\n",
    "#     x_train_orth = x_orth[train_sample_df['sample_ix'].tolist(), :]\n",
    "#     x_test_orth = x_orth[test_sample_df['sample_ix'].tolist(), :]\n",
    "\n",
    "#     para_res = run_lr_with_mi_presplit(x_train_para, x_test_para, y_train, y_test)\n",
    "#     orth_res = run_lr_with_mi_presplit(x_train_orth, x_test_orth, y_train, y_test)\n",
    "\n",
    "#     lang_probe_accs.append({\n",
    "#         'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "#         'para_acc': para_res[0],\n",
    "#         'para_train_acc': para_res[3],\n",
    "#         'para_mi_bits': para_res[1]/np.log(2.0),\n",
    "#         'para_entropy_bits': para_res[2]/np.log(2.0),\n",
    "#         'para_mi_pct': para_res[1]/para_res[2],\n",
    "#         'orth_acc': orth_res[0],\n",
    "#         'orth_train_acc': orth_res[3],\n",
    "#         'orth_mi_bits': orth_res[1]/np.log(2.0),\n",
    "#         'orth_entropy_bits': orth_res[2]/np.log(2.0),\n",
    "#         'orth_mi_pct': orth_res[1]/orth_res[2]\n",
    "#     })\n",
    "\n",
    "#     display(pd.DataFrame(lang_probe_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict Language\n",
    "\"\"\"\n",
    "lang_probe_accs = []\n",
    "\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())):\n",
    "\n",
    "    source_mapping = {source: i for i, source in enumerate(sample_df['source'].unique())}\n",
    "\n",
    "    # Probe en/es token predictiveness\n",
    "    y_df =\\\n",
    "        sample_df\\\n",
    "        .assign(source = lambda df: df['source'].map(source_mapping))\n",
    "        # .pipe(lambda df: df[df['source'].isin(['en', 'es'])])\\ # Move up about assign(source=...)\n",
    "        # .pipe(lambda df: df[df['token'].apply(lambda x: bool(regex.search(r'\\p{L}', x)))])\n",
    "\n",
    "    selected_indices = y_df['sample_ix'].tolist()\n",
    "\n",
    "    y_df = y_df['source'].tolist()\n",
    "\n",
    "    y_cp = cupy.asarray(y_df)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer][selected_indices, :].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer][selected_indices, :].to(torch.float16).detach().cpu())\n",
    "    \n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, y_cp)\n",
    "\n",
    "    lang_probe_accs.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(lang_probe_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Predict Language + DEMEAN\n",
    "# \"\"\"\n",
    "# lang_probe_accs = []\n",
    "\n",
    "# for test_layer in tqdm(list(h_para_by_layer.keys())[::2]):\n",
    "\n",
    "#     source_mapping = {source: i for i, source in enumerate(sample_df['source'].unique())}\n",
    "\n",
    "#     # Probe en/es token predictiveness\n",
    "#     y_df =\\\n",
    "#         sample_df\\\n",
    "#         .pipe(lambda df: df[df['source'].isin(['en', 'es'])])\\\n",
    "#         .assign(source = lambda df: df['source'].map(source_mapping))\n",
    "#         #\\ .pipe(lambda df: df[df['token'].apply(lambda x: bool(regex.search(r'\\p{L}', x)))])\n",
    "\n",
    "#     selected_indices = y_df['sample_ix'].tolist()\n",
    "#     y_df = y_df['source'].tolist()\n",
    "#     y_cp = cupy.asarray(y_df)\n",
    "\n",
    "#     ### Demean by current-layer top-1 expert\n",
    "#     cur_exp_cp = topk_df.pipe(lambda df: df[df['layer_ix'] == test_layer]).pipe(lambda df: df[df['topk_ix'] == 1])['expert'].to_numpy()\n",
    "#     cur_exp_cp = cupy.asarray(cur_exp_cp)[selected_indices]  # aligned!\n",
    "\n",
    "#     # Subtract centroids\n",
    "#     x_cp_para = cupy.asarray(h_para_by_layer[test_layer][selected_indices, :].to(torch.float16).detach().cpu()) # Pull base data\n",
    "#     x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer][selected_indices, :].to(torch.float16).detach().cpu())\n",
    "#     mu_para_sel = cupy.stack([centroids_para[test_layer][int(e)] for e in cur_exp_cp.get()]) # Get centroids by cur expert\n",
    "#     mu_orth_sel = cupy.stack([centroids_orth[test_layer][int(e)] for e in cur_exp_cp.get()])\n",
    "#     x_cp_para = x_cp_para - mu_para_sel # Get demenead residuals\n",
    "#     x_cp_orth = x_cp_orth - mu_orth_sel\n",
    "\n",
    "#     para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, y_cp)\n",
    "#     orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, y_cp) \n",
    "\n",
    "#     lang_probe_accs.append({\n",
    "#         'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "#         'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "#         'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "#         'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "#         'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "#     })\n",
    "\n",
    "# display(pd.DataFrame(lang_probe_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "display(sample_df.groupby('source', as_index = False).agg(z = ('sample_ix', 'count')))\n",
    "lang_export_df = pd.DataFrame(lang_probe_accs)\n",
    "display(lang_export_df)\n",
    "\n",
    "lang_export_df.to_csv(f'{svd_dir}/svd-probe-lang-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict TID\n",
    "\"\"\"\n",
    "tid_probe_accs = []\n",
    "\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())):\n",
    "\n",
    "    clear_all_cuda_memory(False)\n",
    "\n",
    "    top_tids =\\\n",
    "        sample_df\\\n",
    "        .pipe(lambda df: df[df['source'] == 'en'])\\\n",
    "        .groupby(['token_id', 'token'], as_index = False)\\\n",
    "        .agg(n = ('token', 'count')).sort_values(by = 'n', ascending = False)\\\n",
    "        .head(500)\n",
    "\n",
    "    valid_samples =\\\n",
    "        sample_df\\\n",
    "        .assign(token_id = lambda df: np.where(df['token_id'].isin(top_tids['token_id']), df['token_id'], 999999))\n",
    "        # .pipe(lambda df: df[df['token_id'].isin(top_tids['token_id'].tolist())])\n",
    "\n",
    "    y_df =\\\n",
    "        valid_samples\\\n",
    "        ['token_id']\\\n",
    "        .tolist()\n",
    "\n",
    "    y_cp = cupy.asarray(y_df)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer][valid_samples['sample_ix'].tolist(), :].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer][valid_samples['sample_ix'].tolist(), :].to(torch.float16).detach().cpu())\n",
    "    \n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, y_cp)\n",
    "\n",
    "    tid_probe_accs.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(tid_probe_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "tid_export_df = pd.DataFrame(tid_probe_accs)\n",
    "display(tid_export_df)\n",
    "\n",
    "tid_export_df.to_csv(f'{svd_dir}/svd-probe-tid-{model_prefix}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict Position\n",
    "\"\"\"\n",
    "position_probe_accs = []\n",
    "\n",
    "for test_layer in tqdm(list(h_para_by_layer.keys())):\n",
    "\n",
    "    clear_all_cuda_memory(False)\n",
    "\n",
    "    valid_samples =\\\n",
    "        sample_df\\\n",
    "        .pipe(lambda df: df[df['token_ix'].isin(range(500))])\n",
    "\n",
    "    y_df = (valid_samples['token_ix'] // 100).tolist()\n",
    "\n",
    "    y_cp = cupy.asarray(y_df)\n",
    "    x_cp_para = cupy.asarray(h_para_by_layer[test_layer][valid_samples['sample_ix'].tolist(), :].to(torch.float16).detach().cpu())\n",
    "    x_cp_orth = cupy.asarray(h_orth_by_layer[test_layer][valid_samples['sample_ix'].tolist(), :].to(torch.float16).detach().cpu())\n",
    "    \n",
    "    para_acc, para_acc_lo, para_acc_hi, para_nmi, para_nmi_lo, para_nmi_hi = run_probe(x_cp_para, y_cp)\n",
    "    orth_acc, orth_acc_lo, orth_acc_hi, orth_nmi, orth_nmi_lo, orth_nmi_hi = run_probe(x_cp_orth, y_cp)\n",
    "    \n",
    "    position_probe_accs.append({\n",
    "        'test_layer_1': test_layer + model_n_dense_layers + 1,\n",
    "        'para_acc': para_acc, 'para_acc_lo': para_acc_lo, 'para_acc_hi': para_acc_hi,\n",
    "        'para_nmi': para_nmi, 'para_nmi_lo': para_nmi_lo, 'para_nmi_hi': para_nmi_hi,\n",
    "        'orth_acc': orth_acc, 'orth_acc_lo': orth_acc_lo, 'orth_acc_hi': orth_acc_hi,\n",
    "        'orth_nmi': orth_nmi, 'orth_nmi_lo': orth_nmi_lo, 'orth_nmi_hi': orth_nmi_hi,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(position_probe_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Export\n",
    "\"\"\"\n",
    "position_export_df = pd.DataFrame(position_probe_accs)\n",
    "display(position_export_df)\n",
    "\n",
    "position_export_df.to_csv(f'{svd_dir}/svd-probe-pos-{model_prefix}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
