{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This runs forward passes on samples and stores: (1) pre-MLP activations; (2) expert outputs; \n",
    " (3) top-k expert selections; (4) sample metadata.\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb960f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.loss.loss_utils import ForCausalLMLoss # Cross-entropy loss that handles label shifting\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import importlib\n",
    "import os\n",
    "import pickle\n",
    "import safetensors\n",
    "\n",
    "from utils.memory import check_memory, clear_all_cuda_memory\n",
    "from utils.loader import load_model_and_tokenizer, load_custom_forward_pass\n",
    "from utils.store_topk import convert_topk_to_df\n",
    "from utils.store_outputs import convert_outputs_to_df_fast\n",
    "\n",
    "main_device = 'cuda:0'\n",
    "seed = 123\n",
    "\n",
    "clear_all_cuda_memory()\n",
    "check_memory()\n",
    "\n",
    "# Below are for Mamba replicability - can remove if remove all SSMs\n",
    "# os.environ['MAMBA_DISABLE_CUDA_KERNELS'] = '1'\n",
    "# os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'\n",
    "# torch.use_deterministic_algorithms(True, warn_only = False)\n",
    "\n",
    "ws = '/workspace/interpretable-moes-analysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77bba1",
   "metadata": {},
   "source": [
    "## Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the base tokenizer/model\n",
    "\"\"\"\n",
    "model_prefix = 'gpt-oss-20b'\n",
    "tokenizer, model, model_architecture, model_n_moe_layers, _ = load_model_and_tokenizer(model_prefix, device = main_device)\n",
    "\n",
    "check_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fdfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load custom forward pass and verify equality to base model forward pass\n",
    "\"\"\"\n",
    "run_forward_return_metadata = load_custom_forward_pass(model_architecture, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90538f4f",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset - C4 + HLPT (en/zh/es)\n",
    "\"\"\"\n",
    "def load_raw_ds():\n",
    "    CACHE_FILE = '/workspace/data/pretrain.jsonl'\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        print('Loading cached dataset...')\n",
    "        return load_dataset('json', data_files = CACHE_FILE, split = 'train').to_list()\n",
    "\n",
    "    rng = np.random.default_rng(seed = seed)\n",
    "\n",
    "    def get_hlpt(lang): # eng_Latn/zho_Hans/spa_Latn\n",
    "        return load_dataset('HPLT/HPLT2.0_cleaned', lang, split = 'train', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "\n",
    "    def get_c4(lang): # en/zh/esp\n",
    "        return load_dataset('allenai/c4', lang, split = 'validation', streaming = True).shuffle(seed = seed, buffer_size = 50_000)\n",
    "\n",
    "    def get_data(ds, n_samples, data_source): # en/zh/es\n",
    "        raw_data = []\n",
    "        ds_iter = iter(ds)\n",
    "        for _ in range(n_samples):\n",
    "            sample = next(ds_iter, None)\n",
    "            if sample is None:\n",
    "                break\n",
    "            raw_data.append({'text': sample['text'], 'source': data_source})\n",
    "        return raw_data\n",
    "    \n",
    "    combined_ds =\\\n",
    "        get_data(get_c4('en'), 300, 'en') + get_data(get_hlpt('eng_Latn'), 600, 'en') +\\\n",
    "        get_data(get_c4('zh'), 100, 'zh') + get_data(get_hlpt('zho_Hans'), 200, 'zh') +\\\n",
    "        get_data(get_c4('es'), 100, 'es') + get_data(get_hlpt('spa_Latn'), 200, 'es')\n",
    "\n",
    "    combined_ds = [combined_ds[i] for i in rng.permutation(len(combined_ds))]\n",
    "\n",
    "    print('Caching dataset to disk â€¦')\n",
    "    Dataset.from_list(combined_ds).to_json(CACHE_FILE, orient = 'records', lines = True, force_ascii = False)\n",
    "\n",
    "    return combined_ds\n",
    "\n",
    "raw_data = load_raw_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff04271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create sample sequences\n",
    "\"\"\"\n",
    "input_df  = pd.DataFrame(raw_data).assign(prompt_ix = lambda df: list(range(len(df))))\n",
    "\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b4dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Load dataset into a dataloader which returns original tokens - important for BPE tokenizers to reconstruct the correct string later\n",
    "\"\"\"\n",
    "from utils.dataset import ReconstructableTextDataset, stack_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def chunk_list(input_list, max_length):\n",
    "    return [input_list[i:i + max_length] for i in range(0, len(input_list), max_length)]\n",
    "\n",
    "# Create and chunk into lists of size 250 each - these will be the export breaks\n",
    "test_dls = [\n",
    "    DataLoader(\n",
    "        ReconstructableTextDataset(chunk_df['text'].tolist(), tokenizer, max_length = 512, prompt_ix = chunk_df['prompt_ix'].tolist()),\n",
    "        batch_size = 32, shuffle = False, collate_fn = stack_collate\n",
    "    )\n",
    "    for chunk_df in tqdm(chunk_list(input_df, 250))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77569698",
   "metadata": {},
   "source": [
    "# Run exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Run forward passes + export data\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def run_and_export_topk(model, tokenizer, *, model_prefix: str, run_model_return_states, dls: list[ReconstructableTextDataset], layer_indices: list[int], return_expert_outputs: bool = False, retain_topk: int|None = None):\n",
    "    \"\"\"\n",
    "    Run forward passes on given model and store the intermediate hidden layers as well as topks\n",
    "\n",
    "    Params:\n",
    "        @model: The model to run.\n",
    "        @tokenizer: The tokenizer object corresponding to the model.\n",
    "        @model_prefix: (str) Model prefix used for file saving.\n",
    "        @run_model_return_states: (function) Fn returning dict with keys: `logits`, `all_topk_experts`, `all_topk_weights`, `all_pre_mlp_hidden_states`, `all_expert_outputs`.\n",
    "        @dls: List[ReconstructableTextDataset] returning `prompt_ix`, `input_ids`, `attention_mask`, and `original_tokens`.\n",
    "        @layer_indices: (list(int)) List of layer indices (0-indexed by MoE layers) for which to filter `all_pre_mlp_hidden_states`.\n",
    "        @return_expert_outputs: (bool) Whether to return expert outputs.\n",
    "        @retain_topk: (int|None) If expert_return_layers=True, the topk's for which to return experts outputs.\n",
    "\n",
    "    Returns:\n",
    "        A dict with keys:\n",
    "        - `sample_df`: A token-level df with input/output ids + text (removes masked tokens).\n",
    "        - `topk_df`: A sample token x layer_ix x topk_ix level df that giving the selected expert ID.\n",
    "        - `all_router_logits`: Tensor (n_samples, layer_indices, n_experts) with the pre-softmax router logits.\n",
    "        - `all_pre_mlp_hs`: Tensor (n_samples, layer_indices, D) with states. Each n_sample corresponds to a row of sample_df.\n",
    "        - `all_expert_outputs` Tensor (n_samples, retain_layers, retain_topk, D) of MLP outputs.\n",
    "    \"\"\"\n",
    "    cross_dl_batch_ix = 0\n",
    "\n",
    "    # Save metadata\n",
    "    output_dir = f'{ws}/experiments/geometry/activations/{model_prefix}'\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    with open(f'{output_dir}/metadata.pkl', 'wb') as f:\n",
    "        pickle.dump({'layer_mappings': layer_indices}, f)\n",
    "    input_df.to_feather(f'{output_dir}/prompts.feather')\n",
    "\n",
    "    # Iterate through dataloaders\n",
    "    for dl_ix, dl in enumerate(dls):\n",
    "        print(f\"Processing {str(dl_ix)} of {len(dls)}...\")   \n",
    "        dl_dir = f\"{output_dir}/{dl_ix:02d}\"\n",
    "        os.makedirs(dl_dir, exist_ok = True)\n",
    "\n",
    "        all_router_logits = []\n",
    "        all_pre_mlp_hs = []\n",
    "        all_expert_outputs = []\n",
    "        sample_dfs = []\n",
    "        topk_dfs = []\n",
    "\n",
    "        for _, batch in tqdm(enumerate(dl), total = len(dl)):\n",
    "            input_ids = batch['input_ids'].to(main_device)\n",
    "            attention_mask = batch['attention_mask'].to(main_device)\n",
    "            original_tokens = batch['original_tokens']\n",
    "            prompt_indices = batch['prompt_ix']\n",
    "\n",
    "            output = run_model_return_states(model, input_ids, attention_mask, return_hidden_states = True)\n",
    "\n",
    "            # Check no bugs by validating output/perplexity\n",
    "            if cross_dl_batch_ix == 0:\n",
    "                loss = ForCausalLMLoss(\n",
    "                    output['logits'],\n",
    "                    torch.where(input_ids == tokenizer.pad_token_id, torch.tensor(-100), input_ids),\n",
    "                    output['logits'].size(-1)\n",
    "                ).detach().cpu().item()\n",
    "                for i in range(min(20, input_ids.size(0))):\n",
    "                    decoded_input = tokenizer.decode(input_ids[i, :], skip_special_tokens = False)\n",
    "                    next_token_id = torch.argmax(output['logits'][i, -1, :]).item()\n",
    "                    print('---------\\n' + decoded_input + colored(tokenizer.decode([next_token_id], skip_special_tokens = False).replace('\\n', '<lb>'), 'green'))\n",
    "                print(f\"PPL:\", torch.exp(torch.tensor(loss)).item())\n",
    "\n",
    "            original_tokens_df = pd.DataFrame(\n",
    "                [(seq_i, tok_i, tok) for seq_i, tokens in enumerate(original_tokens) for tok_i, tok in enumerate(tokens)], \n",
    "                columns = ['sequence_ix', 'token_ix', 'token']\n",
    "            )         \n",
    "\n",
    "            prompt_indices_df = pd.DataFrame(\n",
    "                [(seq_i, seq_source) for seq_i, seq_source in enumerate(prompt_indices)], \n",
    "                columns = ['sequence_ix', 'prompt_ix']\n",
    "            )\n",
    "\n",
    "            # Create sample (token) level dataframe\n",
    "            sample_df =\\\n",
    "                convert_outputs_to_df_fast(input_ids, attention_mask, output['logits'])\\\n",
    "                .merge(original_tokens_df, how = 'inner', on = ['token_ix', 'sequence_ix'])\\\n",
    "                .merge(prompt_indices_df, how = 'inner', on = ['sequence_ix'])\n",
    "\n",
    "            # Create topk x layer_ix x sample level dataframe\n",
    "            topk_df =\\\n",
    "                convert_topk_to_df(input_ids, attention_mask, output['all_topk_experts'], output['all_topk_weights'])\\\n",
    "                .merge(sample_df[['sequence_ix', 'token_ix', 'prompt_ix']], how = 'inner', on = ['sequence_ix', 'token_ix'])\n",
    "            \n",
    "            sample_df = sample_df.drop(columns = ['sequence_ix']) \n",
    "            topk_df = topk_df.drop(columns = ['sequence_ix'])\n",
    "\n",
    "            sample_dfs.append(sample_df)\n",
    "            topk_dfs.append(topk_df)\n",
    "\n",
    "            valid_pos = torch.where(attention_mask.cpu().view(-1) == 1) # Valid (BN, ) positions\n",
    "            all_router_logits.append(torch.stack(output['all_router_logits'], dim = 1)[valid_pos][:, layer_indices, :])\n",
    "            # Store pre-MLP hs - the fwd pass as n_layers list as (BN, D), collapse to (BN, n_layers, D), with BN filtering out masked items\n",
    "            all_pre_mlp_hs.append(torch.stack(output['all_pre_mlp_hidden_states'], dim = 1)[valid_pos][:, layer_indices, :])\n",
    "            if return_expert_outputs and all(x is not None for x in output['all_expert_outputs']): # Some models never return this regardless\n",
    "                all_expert_outputs.append(torch.stack(output['all_expert_outputs'], dim = 1)[valid_pos][:, layer_indices, 0:retain_topk, :]) # (BN, n_layers, topk, D) - keep only top1 + top2\n",
    "\n",
    "            cross_dl_batch_ix += 1\n",
    "\n",
    "        sample_df = pd.concat(sample_dfs, ignore_index = True)\n",
    "        topk_df = pd.concat(topk_dfs, ignore_index = True)\n",
    "\n",
    "        sample_df.to_feather(f'{dl_dir}/samples.feather')\n",
    "        topk_df.to_feather(f'{dl_dir}/topks.feather')\n",
    "\n",
    "        tensors  = {\n",
    "            'all_router_logits': torch.cat(all_router_logits, dim = 0).contiguous(),\n",
    "            'all_pre_mlp_hs': torch.cat(all_pre_mlp_hs, dim = 0).contiguous()\n",
    "        }\n",
    "        if len(all_expert_outputs) > 0:\n",
    "            tensors['all_expert_outputs'] = torch.cat(all_expert_outputs, dim = 0).contiguous()\n",
    "\n",
    "        safetensors.torch.save_file(\n",
    "            tensors,\n",
    "            f\"{dl_dir}/activations.safetensors\",\n",
    "            metadata = {'layer_indices': ','.join(map(str, layer_indices))}\n",
    "        )\n",
    "\n",
    "    return True\n",
    "\n",
    "res = run_and_export_topk(\n",
    "    model,\n",
    "    tokenizer, \n",
    "    model_prefix = model_prefix,\n",
    "    run_model_return_states = run_forward_return_metadata,\n",
    "    dls = test_dls,\n",
    "    layer_indices = list(range(model_n_moe_layers)),\n",
    "    return_expert_outputs = False,\n",
    "    retain_topk = 1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
